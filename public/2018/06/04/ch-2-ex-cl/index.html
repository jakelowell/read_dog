<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.40.3" />


<title>ch 2 ex cl - A Hugo website</title>
<meta property="og:title" content="ch 2 ex cl - A Hugo website">



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/rstudio/blogdown">GitHub</a></li>
    
    <li><a href="https://twitter.com/rstudio">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">53 min read</span>
    

    <h1 class="article-title">ch 2 ex cl</h1>

    
    <span class="article-date">2018/06/04</span>
    

    <div class="article-content">
      <p>In this week we start dealing with the estimation of the PD of a counterparty in more details.</p>
<p>We begin with univariate models, where we consider a counterparty at a time. Next week, we will move towards the multivariate extensions, also known as portfolio models, where we have groups of counterparties, and we are interested in modeling the dependence among defaults. All the models we are going to deal with can be used under the IRB approaches of Basel II-III.</p>
<p>The models we start with in the next pages belong to the so-called family of structural models of default. Another important class, the mixture or reduced-form one, will be discussed next week.</p>
<p>In a structural model of default, we have two main ingredients:</p>
<p>A quantity (total assets, equity ratio, leverage, etc.) that we use to assess the financial solidity of our counterparty, and that we usually model with specific stochastic processes like the Geometric Brownian motion. A quantity (liabilities, distributional quantiles) that we use to set a threshold, conditionally on which we define the event “default”. In fact, in a structural model, a default is defined as the event “the quantity X falls under (or jumps over) the threshold L in/by time T”.</p>
<p>We start our journey with the first structural model of default in the literature: Merton’s model. We will show that this model is mostly a theoretical one, hardly used and usable in practice. Nevertheless, since many industry models derive from it, it is important to understand its functioning.</p>
<p>Among the industry models we will consider together, we find Moody’s KMV and JP Morgan’s CreditMetrics. These models (especially the first) are extremely popular, and they have also inspired other more recent constructions.</p>
<p>In this unit, we introduce Merton’s model. It first appeared in 1973-1974. The model inherits the name of its developer, Dr. Robert Merton, Professor at MIT Sloan School of Management. I strongly suggest you to visit his personal website.</p>
<p>In 1997, for his important contributions in risk modeling and continuous-time finance, Merton received the Nobel Memorial Prize in Economic Sciences, together with Prof. Scholes, the “co-inventor” of the Black-Scholes’ formula. Prof. Black died in 1995, hence he could not receive the prize.</p>
<p>As we will see, Merton’s model is for credit risk modeling what Black-Scholes is for pricing. Starting from different problems, the two models arrive to a very similar solution.</p>
<p>For those of you interested in reading the original paper by Merton, here a link.</p>
<p>The functioning of Merton’s model is rather sample. Or, to be more exact, the intuition of the default mechanism is rather simple. For what concerns the computation of the PD, stochastic calculus is needed (have a look at the extra materials of this week).</p>
<p>In what follows, we will consider many different aspects of Merton’s model, not really because this model is useful in practice. On the contrary, it is a quite theoretical model relying on many unrealistic assumptions!</p>
<p>Nevertheless, it is the starting point of many important industry models, and we can also use it to understand some non-trivial features of credit risk. For these reasons, it is worth spending some time on it.</p>
<p>In this week, and also in the next one, we will deal with the estimation of the PD, the probability of default, of a counterparty. We will see different approaches, and today, in this class, we introduce the first, the most important theoretical model, for the estimation of the PD, that is to say Merton’s model. We will see that Merton’s model relies on some strong assumptions that make the model not really useful in practice, but nevertheless Merton’s models is still a very important benchmark in the class of credit risk models; and for sure it is the starting point of a series of very important models that you can find in the literature, and that are also used in the industry. Models like Moody’s KMV and JP Morgan’s CreditMetrics. So, it is very important to understand the functioning, the mechanisms behind Merton’s model, because that will help us in understanding all the other models. Merton’s model was introduced in 1974 by Merton, who is a Nobel Prize winner for economics; and Merton’s model is to credit risk modeling what Black and Scholes is for pricing. And in fact we will see that, even if these two approaches obviously start from different starting points - one is pricing, one is credit risk, essentially they converge to a very similar solution. And that’s why, in financial mathematics, we typically speak of Black-Scholes-Merton’s formula. And not just Black and Scholes or Merton, because they really share something worth investigating. Merton’s model is the prototype of the class of models that go under the name of structural models of default. In this class, we typically see a stochastic process that mimics one of the quantities that can describe the financial solidity of our company, for example the total assets or the equity ratio. And we look at the probability that this quantity essentially touches, or overcomes a given threshold. In Merton’s model, the process that we consider is essentially a process mimicking the total assets of the company, and the threshold is defined by some sort of liability. Let’s start by considering the economy in which we perform our evaluations. As usual in financial mathematics, we make some simplifying assumptions, to make our computations easier. First we assume that the two fundamental theorems of asset pricing hold. This means that there is no arbitrage on the market, that is to say there exists at least one riskneutral measure equivalent to the physical one, to the market one if you prefer. Moreover, and this is the second theorem, the risk-neutral measure is actually unique and markets are complete, so that all financial products and risks can be assessed and evaluated. These are rather technical requirements for stochastic calculus purposes. More details on the course platform, if you wish. We then assume markets to be frictionless, meaning that there is no transaction nor hidden cost. Finally we assume the Modigliani-Miller theorem on the irrelevance of the financial structure. In other words, the value of a company is independent from the financing sources. A company totally financed by debt is as valuable as another one totally financed by equity, provided that their total value is the same. If we move to the company level, in Merton’s model we just deal with limited companies. The reason will be clear quite soon: in practice, we want shareholders to abandon the company in case of a default. The company can finance itself with equities, that is issuing shares, or by debt. Debt is represented by one single debt obligation, or zero-coupon bond, with face value B and maturity T. As we will see this is a strong assumption, and one of the points of weakness of Merton’s Model. Moody’s KMV, for example, tries to solve this problem. If S_t denotes the value of equity at time t, and B_t represents debt, we have that the value of the company at time t, V_t, is equal to S_t plus B_t. We then assume that the company cannot pay dividends or issue new debt before T. This is meant to make computations easier, once again. Regarding default, we define it as the situation in which the company is not able to pay debt holders, by missing the repayment on debt at time T. And only at time T. So, given all the assumptions together, we have that our counterparty, our company can only default at time T, not before, not after. That means that at time T we will essentially evaluate the event “default” for the counterparty. You can understand that this is clearly a point of weakness of Merton’s model, because we know that, even if we fix a time horizon, so our T, then default in reality may happen at any time before, not just at maturity. Given all the assumptions we have, at time T, we have two possible scenarios. If the value of the company’s assets exceeds the liabilities, that is V_T is larger than B, everyone is happy: the debt holders receive B, while the share holders receive the residual value, that is V_T minus B. Conversely, if V_T is smaller than or equal to B, we have a default as the company is not able to meet its obligations. Thanks to the Limited Company assumption shareholders do not have any interest in providing new capital, since it would go directly to debt holders. Instead they hand over control of the firm to debt holders by exercising the limited liability option. Debt holders thus liquidate the company and distribute the revenues among them. Debt holders get V_T and shareholders get 0. Notice that we assume that the company defaults also when V_T is equal to B, because we do not want a company without any equity. However, from a probabilistic point of view, this is irrelevant, given that we will play with continuous distributions. Now, have a look at the equations here. They should be familiar to you. If you have a look at the equations, you see that S_T, that is to say the value of equity at maturity T, corresponds to the payoff of a European call option on V_T, the total value of the company, with exercise price B, the liability level. And for what concerns the value of debt at maturity, this corresponds to the liability level B minus the pay-off of a European put on V_T, again with exercise price B. So you see here the connection between Merton’s model and Black and Scholes. In the next video lesson, and on the course platform, we will give more details, but it is already clear that Merton’s model is only a stylized description of a default. In reality, default can occur at many different times, and not only at “maturity”. And not all companies are limited companies. Nevertheless, despite its simplicity, Merton’s model is also able to give an effective view of the potential conflict of interest between shareholders and debt holders. The first have an interest in the company investing in risky projects that increase the volatility of the underlying “security” - the value of the company, while the second prefer a less volatile and risky asset value. Relying on market quantities like equity and debt as an input, in order to derive the probability of default, Merton’s model is considered a capital market model.</p>
<p>To derive the probability of default under Merton’s model we need some extra probabilistic assumptions on the behavior of the process V_t. As Black and Scholes do for the price of the underlying security, Merton assumes that the value of the company under evaluation follows a Geometric Brownian motion. On screen you can see the stochastic differential representation of a Geometric Brownian Motion. The quantities involved are: V_t, obviously, but also mu_V, sigma_V and W_t. mu_V represents the percent drift of assets (that is why it is multiplied by V_t), and tells us if assets have some deterministic growth trend. sigma_V is the percent volatility of the assets. It is a measure of their riskiness. Finally W_t is a standard Brownian motion. Now, if you are not familiar with the Brownian motion, it is sufficient to know that the Brownian motion is one of the most important stochastic processes in probability theory. From a heuristically point of view, a stochastic process can be seen as a sequence of random variables indexed by time. And in the particular case of a Brownian motion, these random variables are normally distributed, under the assumption that we can represent them in this way, where we have a standard normal z with mean 0 and variance or standard deviation 1 times the square root of dt, where dt is an infinitesimal time step. This construction tells us that the dW_t that you see in the slides is essentially a normally distributed random variable with mean 0 and variance dt. Solving the stochastic differential, we discover that V_T is a log normally distributed random variable. In fact it is the exponential of a Normal, represented by W_T. Notice that all the other quantities are just deterministic amounts, nothing more than numbers. On the course platform I am uploading my lecture notes in Stochastic Calculus. They go beyond the scope of the course, but if you are interested, you can find a lot of useful extra information and proofs. So, if you have a look at the formula here, you see that the logarithm of V_T follows a Normal distribution with a given mean and a given variance that you see in the formula. Now, how can we use this information to compute the probability of default of our counterparty? Now, the probability of default of our counterparty is the probability of the event V_T being smaller than/equal to B, the liability level. Ok, we can then exploit the relationship between the logarithm of V_T and V_T. We know that the logarithm of V_T follows a Normal distribution. This implies that V_T follows a lognormal distribution. And here it is: the probability of V_T being smaller than/equal to B corresponds to the probability of the same relationship in the logs, that we can express in terms of Phi, the cumulative distribution function of a normal distribution. Have a look at the probability of default according to Merton’s model. Now, this probability increases in the liability level B, and this totally makes sense; it decreases in V_0 and mu_V, that are two measures of the financial solidity of our counterparty, of the company we are considering; and again it increases in sigma_V, which is a measure of the volatility of the asset value of the company. So, I would say: not bad for a toy model. Now, we may also be interested in evaluating the company assets, including equity and debt, at any time t before T. Default can only happen in T, but the assets of the company do vary over time. We know that S_t and B_t are related to an EU call and an EU put respectively. Their values in t will be nothing more than the discounted values in t of the corresponding pay-offs. This is Black-Scholes-Merton theorem in disguise. In order to price, to evaluate, the quantities we are interested in, we assume that there exists a deterministic risk-free rate on the market. In the following formulas r will substitute mu_V. And this is in practice the difference between riskneutral and market pricing (ok, not really rigorous, but you get the idea). But you will find more details on the platform. Black-Scholes-Merton theorem tells us that, in perfect and complete markets, the fair value of an asset in t is simply the discounted expected value of the value of the asset at maturity, conditionally on the available information. This comes from the fact that we are playing with martingales and the quantity F_t that you see on your screen is the so-called filtration. That is the collection of sigmaalgebras (the information more prosaically) generated by the asset process V_t. As for h, it is a generic functional. Have a look at my lecture notes for more details, if you want to see them. Ok, then for equities we obtain the standard Black-Scholes formula for a European call. The only difference is that the underlying security are the company’s assets, and the strike price is B. Similarly for debt, where we also need to discount the liability level B, and then plug in the value in t of a European put on V_t. Ok, that’s all for this video lesson. On the course platform, you can find a lot of additional material, related to the mathematical computations involved by Merton’s model. And, most of all, you will find interesting relationships we can derive among the different quantities of interest for credit risk, from the PD to the recovery risk, and the Loss-Given-Default, under Merton’s model.</p>
<p>Let’s say something about the main computational issues of Merton’s Model. We know that, under this model, a default is defined as the event V_T lower than/equal to B. The probability is then the one we see on the screen, involving the normal distribution. Under risk-neutrality, we can substitute mu_V with the risk-free rate r, obtaining this new version. This probability depends on many different quantities. Some like B and T are easily known, but for some other variables it is not that simple. In particular V_0 and sigma_V are two quantities not directly observable on the market. They need to be somehow inferred from data. Regarding r, we can always approximate it with some government bond over a short time horizon. Why is it so difficult to assess V_0, the value of the company today? First, from an accounting point of view, the market value can strongly differ from the value as measured by accounting rules. Second, the market value of our company corresponds to the sum of the values of its equity and debt. For equity, if the company is traded on the market, there is no serious problem, but only a relatively small part of debts is really fully transparent and knowable. For example issued bonds. But what about loans or credit lines? If we assume to just deal with publicly traded companies, the solution can be found as follows. Using stochastic calculus (for the curious ones among you, we are using Ito’s lemma), we can obtain the relation in equation 1 for what concerns the value of equity in 0. S_0 is something that we can observe. The instantaneous volatility of equity, sigma_S, can be estimated via the volatility we observe on the market for the shares of the company under consideration. Now, if we combine this new equation with what we know about the European call behavior of equity, we can define two functions G and F. V_0 and sigma_V can then be obtained by minimizing the quadratic expression F squared + G squared, where the only unknown quantities are exactly V_0 and sigma_V. As usual, with numerical problems is always better to have a look at an example. Consider a company whose equity is 3 million euros. The volatility of equity is 0.80. The company’s debt is equal to 10 million euros and it must be paid in one year. The risk-free rate on the market is 5% per annum. What are the values of V_0 and sigma_V? What is the probability of default of the company in one year?</p>
<p>Risk-Neutral vs real-world probabilities The discussion on risk-neutral vs real-world (or physical) probabilities is one of the hot topics of the last years, both for academics and practitioners. The problem, despite its apparently technical nature, has important consequences in practice, given the risk of overestimating (sometimes underestimating) the probability of a given event. In our case, the default of our counterparty.</p>
<p>Consider the probability of default under Merton’s model. We have seen that we have two possibilities. The first, the real-world one, is</p>
<p>In this probability we use all the information we have about the asset process, including the drift (real return of rate) . The second probability, the risk-neutral one, is</p>
<p>Apparently these two probabilities simply differ for the use of , the risk-free rate, instead of . The reason of this substitution is related to important results of probability and financial mathematics (change of measure and martingale equivalence), which we cannot discuss here. Those of you interested in more details can have a look at my optional lecture notes (Chapter 4).</p>
<p>One could therefore believe that the actual difference, when performing all the computations, is just in the order of a few decimals. With no further implication.</p>
<p>In reality, probability theory tells us that the two probabilities are only equivalent. Prosaically, this means that they only agree on which events are possibles, and which are not, but nothing more. In other words, the two probabilities could be extremely different when dealing with possible events (like a default for us).</p>
<p>Why do we prefer to use ? Because the risk-free rate is easier to approximate than the real rate of return , which is hardly observable from data (and, in case, highly sensitive to historical biases). Moreover, risk-neutrality allows us to exploit a lot of useful mathematical results, which could not hold (or could require much more work) under real-world probabilities. In particular, we do not have to adjust our evaluations every time we take into consideration a slightly different risk profile/preference (which would imply the existence of different risk premia).</p>
<p>Ok, what happens at the end? We can easily assume that . In fact, there would be no reason to invest in something risky, like the assets of a company, if its real rate of return is smaller than/equal to the risk-free. But then the Normal distribution tells us that , that is to say risk-neutral probabilities tend to be higher than real-world probabilities. Therefore, a risk-neutral Merton’s model tends to overstate the real probability of default.</p>
<p>PD and Recovery Rates in Merton’s model Despite its simplicity and almost purely theoretical nature, we can use Merton’s model to say something about the relationship between the PD and the recovery rates (from now on RR). A simple model like this is already sufficient to show that assuming the independence between PD and LGD (more details here) is not a clever choice.</p>
<p>We have seen than the probability of default of a counterparty (under real-world probabilities) is</p>
<p>In order to simplify computations, let’s assume that there is no liquidation cost. In case of default in , the recovery rate RR will correspond to the ratio of the asset value to debt, that is . This implies that the expected recovery rate is . It goes without saying that this expectation is actually defined if , otherwise no recovery is given, view that no default is observed. A graphical representation in the picture here below. In formal terms, we therefore have</p>
<p>which is the conditional mean (a sort of expected shortfall) of a truncated lognormal random variable (remember that is normal, hence is lognormal).</p>
<p>This quantity can be easily computed using the properties of lognormal random variables (see for example Kleiber and Kotz, 2003; but also Wikipedia), and the result is where</p>
<p>Merton’s model therefore tells us that:</p>
<p>An increase in the liability level increases the probability of default, while reducing the recovery rate on the defaulted exposure. An increase in reduces the PD and increases RR. An increase in the volatility increases the likelihood of a default and decreases the associated recovery rate. PD and RR move therefore in opposite directions.</p>
<p>The previous points are true if we change the initial conditions of the model, but also if we stress the model as it goes. A sudden increase in , for example, can be the result of unexpected new liabilities due to some legal claims, or large structural damages to a production plant. can be revised upwards because of a new innovative product. And for volatility? Just think about what happened during the dot-com bubble.</p>
<p>Default risk and recovery risk (in general) In credit risk modeling, PD and LGD are often assumed to be two independent random variables, as if they were functions of diverse non-overlapping factors. As we have just seen, Merton’s model, in its simple theoretical construction, is somehow an exception*.</p>
<p>The large majority of credit pricing and VaR models relies on the strong assumption of independence between PD and LGD. Recovery rates are considered constant or, if random, they are taken as if independent from default rates.</p>
<p>Recovery risk thus becomes an idiosyncratic risk that we can control and even delete thanks to diversification.</p>
<p>If, on the contrary, we assume recoveries to be even simply correlated (i.e. linearly dependent) with defaults, then recovery risk assumes a systematic flavor, and a specific risk premium should be taken into consideration. This would lead to significant changes in expected and unexpected losses.</p>
<p>In most models, the probability of default is usually assumed to depend on the economic, financial and idiosyncratic characteristics of the counterparty. We can easily list company-related factors like the leverage, the profitability, the management structure, and so on. And we can define industry-level factors like industrial prospects, regulations or entry-barriers. Finally we can take into consideration different macroeconomic factors. The loss given default, conversely, is mainly associated to the type of the exposure or the seniority. This surely simplifies the analysis, but we know it is not representative of reality. It is sufficient to come back to what we said in Week 1.</p>
<p>We do not need rocket science to understand that many factors may affect both the PD and the LGD. These factors can be systemic, at the macro level (economic cycle, volatility on the market, exchange rates), but also firm-specific, and related to the expectations about the asset value for instance.</p>
<p>Empirical studies (for example this, I strongly suggest this reading) show that there is often a negative relation between default rates and recovery rates. Hence a positive relation between PD and LGD. A sort of alternative wrong-way risk.</p>
<p>How can we explain such a phenomenon?</p>
<p>We can identify many “contributory causes”. For example:</p>
<p>Cycle effects: if defaults increase because of a economic downturn, and the assets of the defaulting companies are partly represented by claims or credits to other companies, then a decrease in recovery rates necessarily follows. Industry effects: if some industry-specific event, like the introduction of a new product, decreases sales or turnover for most of the companies, increasing the number of defaults, then a decrease in the inventories’ values is also plausible, thus leading to a decrease in the recovery rates. Real estate effects: if some collaterals are linked to real estate assets, an increase in the number of defaults could be followed by a decrease in real estate prices (due to the increased supply, as during the last subprime crisis in the US), and therefore in recovery rates. Other effects can naturally be isolated, depending on the type of exposure.</p>
<p>We will be back on this discussion later on, during the course, when we have all the ingredients to enter into more details.</p>
<p>For the moment, I think it is important to start thinking about these topics.</p>
<p>Assuming independence makes always things easier, but the price of simplicity is generally very high. And model risk is just behind the corner.</p>
<p>We cannot model everything, that’s for sure. We already have a perfect model for reality: it is reality itself. Unfortunately we do not know how to use it. Understanding the impact and the limitations of our modeling choices is fundamental to avoid bad risk management.</p>
<ul>
<li>Let me be rude: some people use models without understanding their implications. If you use Merton’s model (or some of its more advanced extensions), you cannot assume that PD and LGD are independent. Unfortunately, it happens.</li>
</ul>
<p>A word on spreads We have seen (slides of Video Lesson 2, page 10) that the value of , the market value of the zero-coupon bond representing liabilities (the loan granted by the debt holders), can be computed as (set to simplify notation):</p>
<p>Now, let’s consider the equilibrium yield on this loan, that is the rate such that the present value of the final repayment is equal to the current market value of the zero-coupon. In a formula:</p>
<p>This tells us that Once we have , we can compute the equilibrium spread of the loan, that is , where is the risk-free rate, obtaining The value of quantifies the risk premium that debt holders ask to grant a loan to the company, in the form of a zero-coupon bond.</p>
<p>It is not difficult to see that - ceteris paribus - the spread (or the risk premium, as you prefer) increases in the leverage and in the asset volatility. It is sufficient to substitute the complete expressions of and . This behavior makes sense, given all the things we have said until now regarding the PD, the recovery rates, and the conflict of interest between shareholders and debt holders.</p>
<p>In this lecture we introduce Moody’s KMV™, another model we can use to estimate the PD of a company under the IRB class.</p>
<p>The original model was introduced in the late 80’s by KMV (the V is the V of Vasicek, one of the founders), a research-driven company that soon became a leading provider of quantitative credit analysis tools. The model, and the data set on which it relies, are now maintained and developed by Moody’s Analytics, which acquired KMV (now Moody’s KMV) in 2002.</p>
<p>Moody’s KMV is one of the most used models for credit risk. It can be used both as a F-IRB and an A-IRB model. In this class we just focus on the F-IRB case, for the computation of the PD (or the EDF as we will soon discover).</p>
<p>This model is a derivation of Merton’s one, trying to overcome some of its flaws. We will start from Merton’s model to describe how Moody’s KMV actually works.</p>
<p>Being a proprietary model, not all the quantities of interest are publicly known. For example the survival function. If I can be more direct: they cannot be disclosed, because of the risk of infringing copyright. Having worked for different companies providing consulting about credit risk, I actually know some more details than what I can say on this platform. Sorry. Nevertheless, at the end of this class, you will have a rather complete understanding of the mechanisms behind the model. I guarantee.</p>
<p>Moody’s KMV. Moody’s KMV is one of the most important industry models out there for the estimation of the probability of default of a counterparty. Or, if we want to use Moody’s terminology, instead of the PD, we will deal with the EDF, the Expected Default Frequency. Now, the Expected Default Frequency is nothing more than the probability of default of our counterparty over a 1-year time horizon. Moody’s KMV is a structural model of default that originates from Merton’s model. In a sense, Moody’s KMV tries to overcome many of the weaknesses of Merton’s model. For example, we substitute the Normal distribution, that you know is the distribution according to which we compute the probability of default of a counterparty under Merton’s model, with another distribution, which is empirically computed. This new distribution allows for fatter tails, so for more extreme events, and you know that this can be much more plausible than the thin tails of a Normal distribution. Then, for what concerns the liability level B, under Moody’s KMV this is substituted with a more realistic liability structure that takes into account intermediate payments, and not only the zerocoupon bond with maturity T and face value B. And this also allows for the possibility of default before maturity. Not only at maturity as in Merton’s model. And finally, in Moody’s KMV, we introduce a quantity, called Distance to Default, that tries to simplify the relationship between the market quantities, that we use as input to compute the probability of default of a counterparty, and the probability of default. We can use Merton’s model in order to understand the most important characteristics of Moody’s KMV. First of all, let’s define the EDF according to Merton’s model. This is the probability of default within 1 year. In order to obtain this we can start from the quantity we know. We set T equal to 1, so you see T disappears from the equation, and then we use the symmetry property of the Normal distribution, so that we can express our probability in terms of survival function. We then substitute B, that is the liability level according to Merton’s model, with a B tilde, which is a more representative quantity of the complexity of the liability structure of a company. We are here considering all the liabilities that are payable within one year. So, also considering all the intermediate payments. Then we substitute the entire argument of the survival function with a quantity, the DD, the Distance to Default, that we will define in a few minutes. And finally, we substitute the Normal survival function 1-Phi with an empirical survival function. This is essentially the way in which we can move from Merton’s model towards Moody’s KMV. As in Merton’s model, also in Moody’s KMV the quantities V_0 and sigma_V are not directly observable and they need to be inferred from data. The starting point is more or less the same: we exploit the European call behavior of equity, of S_t. To be more exact, Moody’s KMV does not exactly rely on the standard formula for a European call, the one we have used so far under Merton’s model, but they rather use a proprietary function that includes the formula of the European call but also adds extra arguments, like for example the quantity d, that is the leverage ratio of the company under scrutiny, and the quantity c, that is the average coupon paid by long-term debt of the company, if this information is available, or of a homogenous group of companies similar to the one we are interested in. Then, thanks to an iterative procedure, an iterative algorithm, we can compute the quantities sigma_V and V_0, that are the two quantities we still miss, that we need, in order to compute the distance to default, a quantity that we are going to introduce in a minute, which is the basis, the fundamental quantity for the estimation of the EDF. As said, Moody’s KMV tries to overcome some of the weaknesses of Merton’s model. For example the idea that default can only happen at maturity. This is a quite strong assumption. In Moody’s KMV this is not true: we are considering the possibility of intermediate default. And for what concerns the asset values, we know that asset values are not necessarily lognormal, as it is assumed by Merton’s model. In fact, the empirical literature shows that very often asset values have heavy tails, meaning that large deviations are much more probable than what we would expect under a lognormal distribution, if we consider asset values, or a normal distribution, if we consider the logs. Starting from these points of criticism, Moody’s KMV introduces a quantity called Distance to Default, DD as an acronym, which is probably the most important quantity under this approach. The DD may appear as a simple ratio, the one you see on your screen, but in reality is the result of a careful analysis of the default phenomenon. In this quantity, for example, you see the B tilde we were speaking about before, that is the new threshold we define, and it often represents all the liabilities that are payable within one year. In Moody’s KMV, the Distance to Default, this guy, is used to approximate the argument of the survival function. In Merton’s model we use the normal distribution, in Moody’s KMV we use something else, something that we are going to see in a minute. If you are asking yourself how this substitution is possible, just notice that the difference of log V_0 and log B tilde can be approximated by the expression you see on the screen, V_0 minus B tilde over V_0. For what concerns the difference between mu_V and half of the variance of the asset, Sigma_V squared, the empirical evidence show that this difference is negligible, very very close to zero. As we have already said, Moody’s KMV does not rely on the Normal survival function, but rather on an empirical survival function, which is estimated on a huge historical data set. This data set collects the proportion of companies defaulting for the different values of DD, of the distance to default, and for different time horizons. So, here we are. In front of us we can collect all the ingredients we have been talking about in the last minutes. And we discover that the Expected Default Frequency according to KMV (remember: the probability of default within one year) is nothing more than the probability we obtain by applying the empirical survival function we can estimate from the data to a specific value of DD, that will be the measure on the basis of which we can make our evaluations. Two very different companies that share the same Distance to Default will essentially have the same EDF, the same probability of default over one year. Since I like to repeat stuff in order to make you understand, in front of you, you see a graphical, another graphical representation of how we can move from Merton’s model to Moody’s KMV. As you see, we have the different ingredients: the liability B is now substitute by the liability threshold B tilde, the probability of default is now what we call the EDF (ok, but this is just a minor change), but most of all the lognormal distribution, the normal distribution in the logs, is substituted by an empirical counterpart, by an empirical survival function that we can estimate from data. And while in Merton’s model the assets follow a geometric Brownian motion, in Moody’s KMV this is not really relevant. Actually you can still prove that this is true, but it is not really relevant, because the PD depends on the DD, the Distance to Default. As Merton’s model, Moody’s KMV is a capital market model that uses information from the market to compute the probability of default of a counterparty. In fact, the DD, the Distance to Default, incorporates the information about the equities, about their value on the market. This makes Moody’s KMV react quickly to changes in the economic prospects of the counterparty. And the DD also incorporates information about the macroeconomic scenario, in which we are making our evaluations about the probability of default, or the EDF in this case. Because you can imagine that the prices on the market incorporate the expectations about the economic situation in which we are making all our evaluations. Being a capital market model, one of the limitations of Moody’s KMV is that it is typically available for traded companies, companies that are listed on the market. So for the small company behind the corner, it’s quite difficult to use Moody’s KMV, at least in this version. Moreover, since this model reacts quickly to changes in the economic prospects on the market, essentially it can be affected by the problem of procyclicality. That you know, for example when we deal with Value-at-Risk and all the discussion about the procyclicality of Value-at-Risk, it’s quite a relevant problem when we deal with credit risk.</p>
<p>In the video lesson, we have said that in Moody’s KMV the Merton’s threshold is replaced by .</p>
<p>This new threshold generally accounts for all the payable liabilities within one year.</p>
<p>A slightly different version of the model (actually there are different versions available, including one that replaces with , thus relying more on the stochastic modeling of ) substitutes with a quantity called , default point. In other words, we have The default point takes into account a combination of both short-term and long-term debt. The actual formula is rather simple, being where and are short- and long-term debt respectively.</p>
<p>Consider a simple company with 10 million euros of short-term debt and 4 million of long-term debt. The default point is simply given by Now, assume that and that . The distance to default is Notice how depends on the volatility of assets. The riskier a company, the lower the , if all the other quantities remain fixed. This will lead to a higher EDF, as the company is “less distant, but rather closer to default”.</p>
<p>Now, I believe you can understand why the is often defined as “the number of standard deviations from default”.</p>
<p>In this class we deal with CreditMetric (in what follows I drop), an interesting structural model of default, which can be seen as a further derivation of Merton’s one.</p>
<p>Introduced in 1997 by JP Morgan, CreditMetrics, whose name is reminiscent of RiskMetrics (the model that popularized Value-at-Risk), has some interesting features:</p>
<p>The default threshold is defined through credit ratings, and not using liabilities. These ratings can be external or internal. Since thresholds are defined through credit ratings, CreditMetrics allows for the assessment of both default and migration risks. The computation of the thresholds is rather simple, provided that we accept the quite strong Normality assumption. In this week, we will just consider the univariate version of CreditMetrics; the multivariate case will come later, together with portfolio models.</p>
<p>CreditMetrics. CreditMetrics was introduced in 1997, a very special year, because most of the models related to credit risk appeared in 1997. It was introduced by JP Morgan, and it’s a model that comes from the family of structural models of default. Actually it derives from Merton’s model, but, differently from Merton’s model and Moody’s KMV, in CreditMetrics the liability is not the quantity we use as a threshold. Rather the threshold is defined in terms of ratings, credit ratings. These ratings can be internal or external. That is to say, they can be computed internally by the bank, or they can be acquired externally from a rating agency. We consider a company that has been assigned to some credit rating class (say B) at the beginning of period [0,T]. Thanks to the transition matrix, in CreditMetrics we can not only study the probability of default of the company, but also migration risk. This is a nice novelty, with respect to the other models we have seen so far. With p(j), for j from 0 to n, we indicate the probability that our company will be in rating class j at time T. We assume to have n+1 classes of risk. n classes plus the default class. We order the j’s, so that j=0 means “default” and j=n means that our company is in the best rating class, say AAA. With p(0), we therefore indicate the probability of default of the company. The essential part of Merton’s model remains valid. That is to say, we still assume that assets are following a geometric Brownian motion. But now we will not really split the assets into equity and debt. And from a certain point of view, this is even more consistent with the Modigliani-Miller theorem about the irrelevance of the financial structure. From Merton’s model we know that the log-assets follow a normal distribution. Now, let’s use the probabilities p(j) to define a sequence of thresholds, from d_0 tilde to d_n+1 tilde. I will omit the tilde in the rest of my talk. These thresholds are defined such that the probability that V_T is between d_j and d_{j+1} is exactly p(j). They are quantiles. d_1 is clearly the default threshold. If V_T is smaller than/equal to d_1, the company defaults in T. Thanks to the thresholds we can identify in which risk or rating class our company will be in T. It is worth noticing that the migration probabilities are invariant under simultaneous strictly increasing transformations of both V_T and the thresholds d. We know that log V_T follows a Normal distribution with the mean and the variance we have already seen. We can create a new variable X_T by standardising log V_T, that is by removing the mean and dividing by the standard deviation. Naturally, we can do the same for the thresholds. Notice that the standardised quantity X_T is now following a normal distribution, but a very special one, a standard normal distribution, with mean zero and variance 1. For what concerns the thresholds, we can now see them as quantiles of a standard normal. Ok, this is CreditMetrics in a slide. Again, we can use X_T and the new thresholds to see in which rating class is our counterparty in T. Let’s consider a simple example to better understand. We have a B-rated company and we have the following 1-year transition matrix. Let X_B be the value of the rescaled asset value, according to CreditMetrics, of our B-rated company at the end of the year. We know that X_B follows a standard Normal. How can we compute the default threshold d_1? We simply apply the quantile function of a standard normal to the probability 5.2% that we can read in the transition matrix. We get -1.6258. If X_B is smaller than this threshold, we have a default. We can use R or any other program to compute the quantiles. Even the old-fashioned but reliable standard Normal tables. Similarly, we can study the thresholds associated to the other migration probabilities for our company. We simply need to cumulate the right probabilities and to invert via the quantile function of the standard Normal. You understand that one limitation of CreditMetrics is the actual possibility of having reliable ratings. This can be a problem for small and medium companies. If ratings are available, reliable thresholds are easier to define, if we compare it with the other models we have seen. The CreditMetrics model can be extended to deal with a portfolio of counterparties, which is actually the most interesting case for us. But, in order to perform this extension, we need to introduce copulas, and we will do that next week. In reality, in this week, on the course platform, you find another video lesson, whose aim is really to introduce you to the basics of copulas. So, for the moment, we just stop here with CreditMetrics</p>
<p>we have introduced the first three models to estimate the PD of a counterparty.</p>
<p>All these models belong to the structural class, where default is defined in terms of some stochastic process mimicking assets (or another measure of financial solidity ) touching or overcoming a predetermined threshold level.</p>
<p>In probabilistic terms we are dealing with randomly stopped processes.</p>
<p>Do these models work? What are their main flaws? In the next pages we are going to analyze the pros and the cons of each model.</p>
<p>Merton’s Model Let’s start by analyzing the strengths and the weaknesses of Merton’s model, always keeping in mind that we are dealing with a theoretical model, not really fit for practical purposes. In simple words: we cannot be extremely picky.</p>
<p>Strengths</p>
<p>In its simplicity, Merton’s model is able to identify the conflict of interest between shareholders and debt holders. It allows for a clear identification of the main variables driving the PD of a counterparty: the ratio debt to assets (a well-known measure of leverage) and the volatility of assets. The first appears in the form , while the second gives us a measure of the riskiness of the company, and of its business decisions. Using Market inputs (remember: Merton’s model is a capital market model), the model allows for a clear and elegant computation of the PD and of spreads. Many interesting relations can be derived, for example between PD and RR, or PD and spreads. Weaknesses</p>
<p>A first flaw is the simplistic assumption about the zero-coupon bond nature of the liabilities. I do not think I have to elaborate more on this. You know better than me how complex the liability structure of a company can be. This is also why most of the models deriving from Merton’s one have changed this part of the construction. The assumption of Normality for the distribution of log assets may be acceptable in extremely good market conditions, and short time horizons, but its is absolutely not realistic in most situations. The empirical literature and the business practice show that extreme events are more probable than what a Gaussian may tell us. We will be back on this soon, when we play with Gaussian copulas. Some of the parameters of the model, namely and , are not observable, and must be inferred indirectly. The choice of the procedure (we have just considered the simplest one via the quadratic function) introduces degrees of freedom in the reliability of the estimates. From a theoretical point of view, these problems could be solved by assuming that a company only finances itself through zero-coupon bonds, and that the company is also listed on the market, so that both and can be easily assessed. Regarding volatility we could assume full information and perfect markets, but you will agree with me that this is not reality. Given the common assumptions and the similar construction, Merton’s model shares a well-known problem of the Black and Scholes’ approach: the assumptions that no-arbitrage can take place on the market thanks to trading. But the assets of a company are definitely not entirely traded on financial markets. Merton’s model assumes the constancy of the risk-free rate. Depending on the time horizon, this is not plausible. Alternative models are available in the literature, and they mainly introduce stochastic interest rates. However, since we are more interested in practical issues and actually usable models, and not in stochastic calculus, we do not enter in the details here. Merton’s model only considers default risk. Nothing is said about migration risk, for example.</p>
<p>Moody’s KMV Moody’s KMV nicely overcomes some of the weaknesses of Merton’s model, but still, as in all models, we can find flaws that it is important to know, in order not to be overconfident about the estimates we get (one of the biggest risks for a risk manager). Remember that the one big manifestations of model risk is to excessively believe in the goodness of a model.</p>
<p>Strenghts</p>
<p>As said, Moody’s KMV quickly reacts to changing financial conditions. If compared to agency ratings, the EDF of a company increases weakly if there are expectations about the worsening in its creditworthiness. As in Merton’s models, it is clear what are the driving forces of the probability of default of a counterparty. The DD represents a simple yet ingenious measure to assess the riskiness of a counterparty Relying on a large amount of empirical data*, which are used to obtain the empirical survival function, Moody’s KMV can get rid of the implausible normality assumption. Weaknesses</p>
<p>The model can only be applied to listed companies. This can be a problem for a bank dealing with small and medium-sized businesses with no access to the capital markets. Moody’s Analytics has proposed different solutions. For example to use the market data of listed companies that are similar, in terms of industrial sector, leverage and size, to the counterparty under assessment. The main idea is that the assets of an unlisted company will by highly correlated with those of its listed peers. A sort of instrumental approach. Another possibility is to use Generalized Linear Models that include the DD among the covariates. But in that case the modeling changes. An implicit assumption of Moody’s KMV model (but also of Merton’s one) is that markets are informationally efficient, so that every quantity we take into consideration, say , provides the best information about the phenomenon we are considering, say volatility. Empirical evidence (but also common sense…) tells us that this is not true. The risk is therefore to have non-optimal estimates that, especially in periods of turbulence, may excessively rely on market sentiment and inefficiencies, and not on the actual financial solidity of our counterparty. Relying on market inputs, Moody’s KMV reacts quickly to changes in the economic scenarios. This is surely a plus, but it can become a problem during a strong crisis, when the volatility on the markets is excessive and, often, irrationally driven.</p>
<ul>
<li>For the sake of completeness (and I will do that again later in the course), I would like to stress that relying excessively on data can lead to some serious historical bias, especially in risk management. Historical bias occurs because of the overconfidence we put in past observations as optimal predictors of future events. The fact that something did not happen before does not guarantee it will not happen in the future. And the future could be tomorrow. Do not underestimate historical bias. Extreme value theory is a branch of statistics that takes into serious account these problems. Just think in terms of sports: the next record is never in the past data.</li>
</ul>
<p>Regarding CreditMetrics, we will now analyze its pros and cons for the univariate version we have just introduced. In the next weeks, when dealing with portfolio extensions, we will add extra points on both sides.</p>
<p>Strengths</p>
<p>First of all, CreditMetrics not only deals with default risk, but it also gives us the possibility of modeling migration risk. It combines company-specific information (the asset value) with higher-order pieces of information like the credit ratings. Since ratings are typically less sensitive to short-term turbulence on the markets, CreditMetrics is less subject to market irrationalities and sentiment. Weaknesses</p>
<p>The probability of default is not really estimated, but rather read from ratings. This is particularly evident in the univariate case. In the multivariate case, where correlations are introduced, this is no longer true. The quality of ratings has an evident impact on the quality of the model’s results. The Normality assumption is back and, we know, this is not desirable.</p>
<p>A copula is a way of describe the dependence among random variables and to represent their joint multivariate distribution. In formal terms, given a random vector , a copula for is defined as the multivariate distribution of the new marginally uniform vector , which we can obtain via the probability transformation of .</p>
<p>Copulas are extensively used in many portfolio models for credit risk, that is to say those models in which we consider a portfolio of exposures, and we are interested in modeling not only the single default of a counterparty, but also groups of defaults, including their dependence structure and the possible cascading effects.</p>
<p>In the next video lesson, I want to give a basic introduction to copulas, in case you are not familiar with them. From next week, we will start dealing with copulas, therefore I think it is essential to give you the necessary tools to follow, or to refresh some of your previous knowledge.</p>
<p>Many copulas can be defined and studied, but our aim is not taxonomy. Therefore, we will just focus on some relevant copulas for us. Other will be introduced later, when and if needed.</p>
<p>If you want to know more, my favorite book on the subject is this one.</p>
<p>In order to “digest” the condensed contents of this lesson, I strongly suggest you to download the slides and to try to re-perform all the steps in the proofs and in the exercises.</p>
<p>For extra questions, please use the course forum.</p>
<p>The most important take-home messages about copulas are the following:</p>
<p>A copula is an alternative way of representing multivariate distributions. Every multivariate distribution has a copula representation. Sometimes this representation is unique. In the copula approach, the copula function accounts for the dependence structure among the random variables, while the marginal distributions account for the marginal, idiosyncratic behavior of each single random variable. In estimation, the copula function and the marginals can be conveniently treated separately, given that the first accounts for the joint dependence, and the second ones for the idiosyncratic behaviors. I personally do not really agree, not completely at least. But let’s say that for most basic applications this is true.</p>
<p>Copulas are a very important tool, a statistical or a probabilistic tool, that we can use to approach multivariate distributions. Now, I cannot assume that everyone in this course is aware of what a copula is, so in this video lesson, I want to share with you the basic knowledge that we need in order to move together to the multivariate framework, to the portfolio framework. So, a copula is a function used to describe the dependence between two or more random variables. With copulas we aim to represent the joint distribution of a random vector. In the copula approach, the copula describes the dependence structure among the components of the random vector, while their marginal behavior is contained in their marginal distributions. Models like JP Morgan’s CreditMetrics rely extensively on copulas, when we consider their portfolio extension, that is to say when we have a portfolio of exposures that may default. Survival models like the Li’s model, a model that we will consider together next week, again rely on specific copula assumptions. And many of the formulas you know, under the Basel framework, for example in the internal rating-based approach, actually derive form the assumption that we are using copulas to model the joint distribution of default. But, in order to efficiently introduce copulas and their properties, and their basic theorems, we need first to introduce the concept of generalized inverse. The generalized inverse is a special function that you probably already know in its basic definition. And we need two important quantities of this function in order to proceed. The generalized inverse is an extension of the concept of inverse. It is a generalized quantile function. If a distribution function is strictly increasing and continuous, the generalized inverse coincides with the standard quantile function you are probably used to. For us, the most important properties of the generalized inverse are the quantile transformation and the probability transformation. The quantile transformation tells us that the quantiles of a uniform random variable with support on the interval [0,1] can be linked to a distribution function G. In fact, the probability that the generalized inverse of U is smaller than/equal to a value y corresponds to G(y), where G is the distribution function on which we have defined the generalized inverse. The probability transform tells us that whatever continuous distribution function G is distributed according to a uniform distribution, if we treat it as a new random variable Z=G(Y). We know that the distribution function takes values in the interval [0,1]. To prove it is uniformly distributed, we can play with the generalized inverse, as you can see on the screen. I do not want you to learn the proof by heart. Just remember the possibility of moving from a given distribution function to the uniform one. Consider a random vector X_1,…,X_d with continuos marginals F_1,…,F_d. If we apply the probability transformation to each component, we obtain a new vector with uniform marginals. A copula for X_1,…,X_d is defined as the joint distribution of the new uniform vector U_1,…,U_d. The most interesting aspect of copulas is that we can show that the function C contains all the information on the dependence structure among the components of the random vector, while the F_i’s necessarily contain all the information on the marginal distributions. Sklar’s theorem is the most important theorem for copulas, because it is the theorem that tells us that we can always give a copula representation of whatever joint distribution. So, it’s a very important result. And always Sklar’s theorem tells us what are the conditions we need to fulfil in order to have a unique representation of our joint distribution. On your screen you can read the text of Sklar’s theorem. We do not prove it. The summary of the theorem is: every joint distribution has a copula representation. If the marginal distributions are continuous, then this representation is unique. In other words, there exist only one copula function that can represent the dependence structure. If margins are not continuous, then we can still find copulas, but they are not uniquely defined. For those of you that like to play a little bit with mathematics, here an exercise. It is a simple way of proving that with discrete marginals we cannot have a unique copula. An interesting property of copulas is the existence of bounds. These are called Fréchét bounds. Every copula C is contained between W and M. The upper bound M is known as the comonotonicity copula. It represents the case of perfect multivariate positive dependence, when all the random variables can be expressed as linear combinations of one of them. Remember that under comonotonicity the Value-at-Risk is coherent. It is one of the few cases. The lower bound W is not a copula, not in general. It is a copula only in the bivariate case, and - in that case - it is called counter-comonotonicity copula. It represents the case of perfect negative linear dependence. There are different examples of copulas we can take into consideration. For instance, the independence copula, that is the copula characterizing the joint distribution of independent random variables. As expected it is the product of the probability transforms. We then have the Gaussian copula. This is the most important copula for us. It is the basis of some IRB formulas and important models like the Li’s one. We will discuss pros and cons of this copula. For the moment, just remember that under the Gaussian copula, all the information we need about the dependence is in the correlation matrix. We do not need anything else. And dependence is only of the linear type. Please notice that a multivariate Gaussian, or a multivariate Normal, and a Gaussian copula are not the same thing in general. In a Gaussian copula, the marginals we take into consideration can be of whatever type. There is no limitation in practice. For what concerns the multivariate normal, on the contrary, the marginal are necessarily normally distributed. Ok, so, this is a quite important difference. Necessarily if we start from Normal marginals, and we apply a Gaussian copula, what we obtain is a multivariate Normal, but this is a special case. As you can imagine, we can define a plethora of possible copulas. This is very nice in terms of flexibility, when fitting data, but it also opens the non-trivial problem of model selection. Ok, I think it is enough for the moment. We will introduce extra material about copulas when we need it. My suggestion is to try to solve this exercise. The solution will appear in a couple of weeks on the course platform. Obviously, this is just an optional thing for you, in order to make you a little bit think about copulas.</p>
<p>Structural Models of Default In a structural model of default, the event default is defined as the situation in which a stochastic process Xt, mimicking the behavior of some measure of financial solidity of our counterparty (assets, equity ratio, etc.), overcomes or touches a given threshold level (typically represented by liabilities or indirectly defined via credit ratings). Capital market model Every model in which market quantities (asset values, spreads, etc.) are used as the inputs for the assessment of the PD.</p>
<p>Merton’s Model The prototype of structural models of default. Introduced in 1973 by Robert Merton. Main assumptions: • The two fundamental theorems of asset pricing hold, so that we can have risk-neutral measures. • Frictionless markets. • The financial structure is irrelevant, i.e. Modigliani-Miller theorem holds. • The counterparty is a limited company. • Debt is represented by a zero-coupon bond with face value B and maturity T. • The asset value of the company is given by the sum of equity and debt. • The asset value Vt at time t follows a geometric Brownian motion, i.e. Then, the probability of default, which can only happen in T, is equal to This probability increases in the level of debt B and in the volatility of assets sV, while it decreases in V0. We can show that, in Merton’s model, recovery rates and PD move in two opposite directions. Merton’s model is not really used in practice, but it is the starting point of many industry models. Moreover, despite its simplicity, it is able to capture interesting phenomena, as the conflict of interest between shareholders and debt holders, and the negative relationship between recovery rates and PD. From a computational point of view, it is not difficult to use Merton’s model. The biggest issue is the computation of V0 and sV that cannot be directly observed from data. Moody’s KMV Moody’s KMV derives from Merton’s model and tries to solve some of its flaws. In particular, after defining the Expected Default Frequency (EDF) as the probability of default of our counterparty within 1 year, Moody’s KMV replaces the zero-coupon threshold B with a more 1 of 2 ACRM - P.Cirillo Structural Models of Default In a structural model of default, the event default is defined as the situation in which a stochastic process Xt, mimicking the behavior of some measure of financial solidity of our counterparty (assets, equity ratio, etc.), overcomes or touches a given threshold level (typically represented by liabilities or indirectly defined via credit ratings). Capital market model Every model in which market quantities (asset values, spreads, etc.) are used as the inputs for the assessment of the PD.</p>
<p>reliable threshold. This new threshold can include all payable liabilities within 1 year, or it can also take into account longer time horizons, if we introduce the so called Default Point, or DP. Relying on a huge data set of defaults, Moody’s KMV substitute the normality assumption of Merton’s model with an empirically estimated decreasing function, that plays the role of a survival function. The argument of this function is not the usual argument of Merton’s model, but rather an approximation, called Distance to Default, of the form or In a simple formula, we have Moody’s KMV is a model that efficiently incorporates the information of markets, which are assumed to be rational and transparent (!), and that quickly reacts to changes in the economic prospects of the counterparty. Given its nature, the models is essentially available for listed companies. CreditMetrics Introduced by JP Morgan in 1997, this model is a special type of structural model of default. From a mathematical point of view, it is very similar to Merton’s model. The main difference is that, instead of log-assets, we deal with standardized log-assets, so that their distribution is a standard normal. The main novelty, however, is the use of credit ratings (internal or external) for the definition of thresholds. We no longer use liabilities, or other quantities from the balance sheet. Using a rating transition matrix, under the hypothesis of normality, thresholds can be computed via the quantile function of a standard normal. Differently from the other models, CreditMetrics takes into account both default risk and migration risk. The most relevant version of CreditMetrics is its portfolio representation, which introduces an interesting dependence structure. Copulas In this week we have also introduced copulas, as statistical tools to approach multivariate distributions. In the copula approach, we can separate the marginals from the dependence structure. Copulas are particularly useful in credit risk management, when dealing with portfolios of exposures and defaults’ dependence. More details on the course platform.</p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    

    
  </body>
</html>

