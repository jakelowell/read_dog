<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.40.3" />


<title>cl dog 3 - A Hugo website</title>
<meta property="og:title" content="cl dog 3 - A Hugo website">



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/rstudio/blogdown">GitHub</a></li>
    
    <li><a href="https://twitter.com/rstudio">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">64 min read</span>
    

    <h1 class="article-title">cl dog 3</h1>

    
    <span class="article-date">2018/05/16</span>
    

    <div class="article-content">
      <p>Lesson 3: Preparing the Data for Credit Scoring • Lesson Overview</p>
<p>Introduction Data is the key ingredient to any analytical credit risk model, be it a PD, LGD, or EAD model. So, it is of critical importance that you prepare the data properly. Worth mentioning here is the garbage-in-garbage-out (GIGO) principle, which essentially states that messy data will yield messy analytical models. Hence, it is of utmost importance that every data preparation step is carefully justified, carried out, validated, and documented before proceeding with further analysis. Even the slightest mistake can make the data totally useless for further analysis. In what follows, we will elaborate on the most important data preparation steps that should be considered during an analytical credit-scoring exercise. Note that many of the ideas, which will be presented here within the context of credit scoring for PD modeling, are equally applicable to LGD and EAD modeling as we will discuss later.</p>
<p>Preparing the data is step 0 of the credit-scoring process. Data preparation includes data collection, sampling, exploring, and preprocessing. With respect to collection, it is important to thoroughly consider and list all data sources potentially of interest before starting the analysis. You should hereby think of both internally gathered, as well as externally collected, data. The more data, the better is the rule here. Once all data sources have been collected and brought together, a sample will be taken, which is representative of the future population. This sample will first be explored and then preprocessed by applying various data-filtering mechanisms to clean up and reduce the data to a manageable and relevant size. In this lesson, you learn to do the following: • describe the two main types of variables: continuous and categorical • identify the main considerations for sampling • identify the main types of charts that can be used for visual data exploration • describe the main strategies for handling missing values • describe the main strategies for detecting and treating outliers • describe the main strategies for standardizing data • describe the main strategies for transforming data • describe the main strategies for performing coarse classification • describe the main strategies for recoding categorical variables • describe the main strategies for performing segmentation</p>
<p>Sampling and Exploring the Data</p>
<p>Text Version Collapse Print Sampling and Exploring the Data</p>
<p>Classifying the Variables When you prepare your data for analysis, it is very important to distinguish between the two main types of variables: continuous and categorical. This categorization can be made by carefully inspecting all metadata and data dictionaries available in order to fully grasp the meaning of every variable. If documentation would be lacking, it is important to discuss the meaning of the variable with one or more credit-risk business experts. Continuous variables are variables, which can take on values within a continuous range. Categorical variables are variables, which can only take on a limited set of values.</p>
<p>In SAS Enterprise Miner, continuous variables are referred to as interval variables because continuous variables are always defined on a particular interval. Note that I will use the terms interval and continuous interchangeably in this course. Let’s take a closer look at these two types of variables.</p>
<p>An interval or continuous variable can take on any value in a specified interval. This interval can be limited (for example, between 0 and 1, between 100 and 1000), or can even be unlimited (for example, between minus infinity and plus infinity). Popular examples are income, savings balance, FICO score, age, etc. For these variables, it makes sense to calculate summary statistics such as the average, median, standard deviation, and confidence intervals.</p>
<p>As said, categorical variables can only assume values from a predefined set. A further distinction can be made between nominal, ordinal, and binary categorical variables. Nominal variables are categorical variables whereby you do not have an ordering between the values. Examples are purpose of loan, which can be car, house, cash, etc., and marital status, which can be married, single, widowed, and divorced. In both these cases, there is no meaningful ordering between the different values of the variable. Ordinal variables, on the other hand, are variables that are categorical, and whereby you do have an ordering between the values. Think about a credit rating, for example. A credit rating can be AAA, which is better than AA, or AA, which is in turn better than A, and so on. Being aware of this ordering will be very useful for both the modeling and the performance measurement, as we will be discussing later. Binary variables are a special case of categorical variables, since they can only have two values. An example is gender, which can either be female or male, or employment status, which can either be employed or unemployed.</p>
<p>Note that, for categorical variables, it doesn’t make any sense to compute statistics such as standard deviation or confidence intervals. It does make more sense to look at statistics such as the mode, which is the most frequently occurring value.</p>
<p>To conclude, it is very important to make a distinction between interval and categorical variables at the outset of your credit scoring project. This will allow the analytics software to properly treat each variable and make sure that the right summary statistics will be reported. Sampling A second important activity in data preparation is sampling. The aim of sampling is to take a subset of past customer data and use that to build your credit scoring model. A first obvious question concerns the need for sampling. Why shouldn’t we build our credit scoring models on all data available, rather than take a subset?</p>
<p>A first reason concerns the computational resources available. Many firms have only limited resources to do the analysis, and in many cases, those resources are being simultaneously used by other applications.</p>
<p>Furthermore, credit-scoring data sets are getting bigger and bigger these days, both horizontally, in terms of the number of variables, as well as vertically in terms of the number of observations. You may have heard of the term “Big data,” whereby people refer to very big data sets, huge data sets—sometimes gigabytes, terabytes, or even larger in size. So in order to make it computationally feasible to build a credit scoring model, it is really important to work with a sample or subset of the observations. That will allow us to quickly build credit scoring models, measure their performance, and bring them into production.</p>
<p>However, there is also another reason why you should consider sampling. Remember what you are doing when you are building an analytical model: you are assuming that the future resembles the past. That’s why it makes sense to take a set of historical observations, analyze their default behavior by building a credit scoring model, and use that to score the future customers. So when you sample, it is really important to stick as close as possible to your future population. Observations of today are more similar to observations of tomorrow than observations of one year ago. You want to stick as close as possible to your future population and that is really very important. It relates to the timing of the sample. How far do you go back to get your sample? On the one hand, you want to go back as far as possible in time, because that will give you more observations and thus more robust models. On the other hand, you want to stay as close as possible to your future population to make sure that the models built capture, as much as possible, the behavior of that future population. Hence, choosing the optimal time window of the sample involves a trade-off between lots of data versus recent data. As an example of a sampling window, application and behavioral scorecards are typically built using a two to three-year time period.</p>
<p>You must also carefully inspect the distribution of Bads versus Goods in the sample. In case of very skewed samples with only a few Bads, undersampling or oversampling procedures might be considered, as we will discuss later. Furthermore, the sample must also be taken from a normal business period to make sure that we get as accurate a picture as possible of the target population. Also the performance window should be long enough to make sure that the Bad rate has stabilized.</p>
<p>Based upon these considerations, you can see that drawing a good, unbiased, and thus representative sample is not that obvious. In fact, it’s quite difficult. In application scoring, for example, we have the reject inference sampling problem, whereby the behavior of the previously rejected applicants is typically unobserved, hereby creating a strong bias in the sample. In behavioral scoring, we will have a seasonality issue when we only pick one observation point to observe customer behavior. Both sampling issues will be discussed later in the course.</p>
<p>To conclude, it is important to start from an unbiased and representative sample to build your credit scoring models. In case assumptions have been made or sample limitations are present, it is important to clearly list and document those at the outset of the analysis. Visual Data Exploration Once a sample has been obtained, you can start exploring it using visual data exploration. The aim here is to get a feeling for your data. It also allows you to get insight into the data characteristics and even detect some initial patterns in the data. Visual data exploration is a very important part of getting to know your data in an informal way.</p>
<p>Different plots or graphs can be useful here. A first popular example are pie charts. A pie chart represents a variable’s distribution as a pie, whereby each section represents the portion of the total percent taken by each value of the variable. The figure in the upper left corner represents a pie chart for a housing variable whose status can be own, rent, or for free. Bar charts represent the frequency of each of the values of a variable as bars. The upper right corner shows a bar chart for the purpose variable. Other handy visual tools are histograms and scatter plots. A histogram provides an easy way to visualize the central tendency and to determine the variability or spread of the data. It also allows to contrast the observed data with standard known distributions such as the normal distribution. The figure in the lower left corner shows a histogram for age, which is as expected rather skewed. Scatter plots allow to visualize one variable against another one to see whether there are any correlation patterns in the data. The figure in the lower right corner shows a scatter plot between duration and age where a weak linear relationship can be seen.</p>
<p>To get further visual insight, you could consider to do any of the plots discussed previously for the Goods and Bads separately. Here you see a pie chart for a housing variable, but now split up for the Goods and Bads separately. You can clearly see that more of the Bads are renting and living for free (for example, with their parents or friends) than the Goods, which typically have their own housing facilities. Insights that result from this preliminary visual analysis can then be discussed with the business expert. Using SAS Programming to Explore Your Data In this demonstration, we will illustrate how to do visual data exploration in SAS.</p>
<p>First, we start by creating a SAS library mydata, which connects to the physical directory where the data is stored. On your machine, this may be a different directory depending upon where your data is stored. We now run this LIBNAME statement and have a quick look at the applicants data set, which will be used in the remainder of this demonstration. This data set is a publicly available application scoring data set. As you will see, it has many application characteristics such as duration, amount, employed, marital status, and a target good_bad indicator. To see the data dictionary for the applicants data set, click the Information button.</p>
<p>We will start by using PROC UNIVARIATE to report some summary statistics. Here we run it on our applicants data set and analyze the continuous age variable. In the output, you can see statistics such as the mean, standard deviation, skewness, median, mode, and other related statistics. It also includes a statistical test to see whether the average of the variable is significantly different from zero or not. Obviously, in our case, we can see a low p-value, implying that the variable’s mean is indeed significantly different from zero. Further down in the output, you can also see information about the quantiles and some extreme observations.</p>
<p>Let’s now clear these results and move on.</p>
<p>By adding the HISTOGRAM statement to PROC UNIVARIATE, we can also get a frequency distribution. Here you can see the frequency distribution of age, which is as anticipated, rather skewed.</p>
<p>We clear the results and proceed.</p>
<p>By including a CLASS statement, we can do a class-by-class analysis. This is very handy to analyze the Goods and Bads separately during the exploratory data analysis step. Here you can see that the average age for a bad payer is about 34 years, whereas for a good payer, it is about 36 years. You can also see both histograms contrasted.</p>
<p>Again we clear the results.</p>
<p>Bar and pie charts can be constructed in SAS using PROC GCHART. We illustrate this for the purpose variable. Here you can see that purpose value 3 is the most popular, whereas value 6 is the least popular. We clear the results.</p>
<p>Scatter plots can be made using PROC GPLOT. Here we create a scatter plot between the age and amount variables. Let’s also run this statement. The scatter plot indicates a weak positive relationship between both.</p>
<p>Preprocessing the Data</p>
<p>Text Version Collapse Print Sampling and Exploring the Data</p>
<p>Classifying the Variables When you prepare your data for analysis, it is very important to distinguish between the two main types of variables: continuous and categorical. This categorization can be made by carefully inspecting all metadata and data dictionaries available in order to fully grasp the meaning of every variable. If documentation would be lacking, it is important to discuss the meaning of the variable with one or more credit-risk business experts. Continuous variables are variables, which can take on values within a continuous range. Categorical variables are variables, which can only take on a limited set of values.</p>
<p>In SAS Enterprise Miner, continuous variables are referred to as interval variables because continuous variables are always defined on a particular interval. Note that I will use the terms interval and continuous interchangeably in this course. Let’s take a closer look at these two types of variables.</p>
<p>An interval or continuous variable can take on any value in a specified interval. This interval can be limited (for example, between 0 and 1, between 100 and 1000), or can even be unlimited (for example, between minus infinity and plus infinity). Popular examples are income, savings balance, FICO score, age, etc. For these variables, it makes sense to calculate summary statistics such as the average, median, standard deviation, and confidence intervals.</p>
<p>As said, categorical variables can only assume values from a predefined set. A further distinction can be made between nominal, ordinal, and binary categorical variables. Nominal variables are categorical variables whereby you do not have an ordering between the values. Examples are purpose of loan, which can be car, house, cash, etc., and marital status, which can be married, single, widowed, and divorced. In both these cases, there is no meaningful ordering between the different values of the variable. Ordinal variables, on the other hand, are variables that are categorical, and whereby you do have an ordering between the values. Think about a credit rating, for example. A credit rating can be AAA, which is better than AA, or AA, which is in turn better than A, and so on. Being aware of this ordering will be very useful for both the modeling and the performance measurement, as we will be discussing later. Binary variables are a special case of categorical variables, since they can only have two values. An example is gender, which can either be female or male, or employment status, which can either be employed or unemployed.</p>
<p>Note that, for categorical variables, it doesn’t make any sense to compute statistics such as standard deviation or confidence intervals. It does make more sense to look at statistics such as the mode, which is the most frequently occurring value.</p>
<p>To conclude, it is very important to make a distinction between interval and categorical variables at the outset of your credit scoring project. This will allow the analytics software to properly treat each variable and make sure that the right summary statistics will be reported. Sampling A second important activity in data preparation is sampling. The aim of sampling is to take a subset of past customer data and use that to build your credit scoring model. A first obvious question concerns the need for sampling. Why shouldn’t we build our credit scoring models on all data available, rather than take a subset?</p>
<p>A first reason concerns the computational resources available. Many firms have only limited resources to do the analysis, and in many cases, those resources are being simultaneously used by other applications.</p>
<p>Furthermore, credit-scoring data sets are getting bigger and bigger these days, both horizontally, in terms of the number of variables, as well as vertically in terms of the number of observations. You may have heard of the term “Big data,” whereby people refer to very big data sets, huge data sets—sometimes gigabytes, terabytes, or even larger in size. So in order to make it computationally feasible to build a credit scoring model, it is really important to work with a sample or subset of the observations. That will allow us to quickly build credit scoring models, measure their performance, and bring them into production.</p>
<p>However, there is also another reason why you should consider sampling. Remember what you are doing when you are building an analytical model: you are assuming that the future resembles the past. That’s why it makes sense to take a set of historical observations, analyze their default behavior by building a credit scoring model, and use that to score the future customers. So when you sample, it is really important to stick as close as possible to your future population. Observations of today are more similar to observations of tomorrow than observations of one year ago. You want to stick as close as possible to your future population and that is really very important. It relates to the timing of the sample. How far do you go back to get your sample? On the one hand, you want to go back as far as possible in time, because that will give you more observations and thus more robust models. On the other hand, you want to stay as close as possible to your future population to make sure that the models built capture, as much as possible, the behavior of that future population. Hence, choosing the optimal time window of the sample involves a trade-off between lots of data versus recent data. As an example of a sampling window, application and behavioral scorecards are typically built using a two to three-year time period.</p>
<p>You must also carefully inspect the distribution of Bads versus Goods in the sample. In case of very skewed samples with only a few Bads, undersampling or oversampling procedures might be considered, as we will discuss later. Furthermore, the sample must also be taken from a normal business period to make sure that we get as accurate a picture as possible of the target population. Also the performance window should be long enough to make sure that the Bad rate has stabilized.</p>
<p>Based upon these considerations, you can see that drawing a good, unbiased, and thus representative sample is not that obvious. In fact, it’s quite difficult. In application scoring, for example, we have the reject inference sampling problem, whereby the behavior of the previously rejected applicants is typically unobserved, hereby creating a strong bias in the sample. In behavioral scoring, we will have a seasonality issue when we only pick one observation point to observe customer behavior. Both sampling issues will be discussed later in the course.</p>
<p>To conclude, it is important to start from an unbiased and representative sample to build your credit scoring models. In case assumptions have been made or sample limitations are present, it is important to clearly list and document those at the outset of the analysis. Visual Data Exploration Once a sample has been obtained, you can start exploring it using visual data exploration. The aim here is to get a feeling for your data. It also allows you to get insight into the data characteristics and even detect some initial patterns in the data. Visual data exploration is a very important part of getting to know your data in an informal way.</p>
<p>Different plots or graphs can be useful here. A first popular example are pie charts. A pie chart represents a variable’s distribution as a pie, whereby each section represents the portion of the total percent taken by each value of the variable. The figure in the upper left corner represents a pie chart for a housing variable whose status can be own, rent, or for free. Bar charts represent the frequency of each of the values of a variable as bars. The upper right corner shows a bar chart for the purpose variable. Other handy visual tools are histograms and scatter plots. A histogram provides an easy way to visualize the central tendency and to determine the variability or spread of the data. It also allows to contrast the observed data with standard known distributions such as the normal distribution. The figure in the lower left corner shows a histogram for age, which is as expected rather skewed. Scatter plots allow to visualize one variable against another one to see whether there are any correlation patterns in the data. The figure in the lower right corner shows a scatter plot between duration and age where a weak linear relationship can be seen.</p>
<p>To get further visual insight, you could consider to do any of the plots discussed previously for the Goods and Bads separately. Here you see a pie chart for a housing variable, but now split up for the Goods and Bads separately. You can clearly see that more of the Bads are renting and living for free (for example, with their parents or friends) than the Goods, which typically have their own housing facilities. Insights that result from this preliminary visual analysis can then be discussed with the business expert. Using SAS Programming to Explore Your Data In this demonstration, we will illustrate how to do visual data exploration in SAS.</p>
<p>First, we start by creating a SAS library mydata, which connects to the physical directory where the data is stored. On your machine, this may be a different directory depending upon where your data is stored. We now run this LIBNAME statement and have a quick look at the applicants data set, which will be used in the remainder of this demonstration. This data set is a publicly available application scoring data set. As you will see, it has many application characteristics such as duration, amount, employed, marital status, and a target good_bad indicator. To see the data dictionary for the applicants data set, click the Information button.</p>
<p>We will start by using PROC UNIVARIATE to report some summary statistics. Here we run it on our applicants data set and analyze the continuous age variable. In the output, you can see statistics such as the mean, standard deviation, skewness, median, mode, and other related statistics. It also includes a statistical test to see whether the average of the variable is significantly different from zero or not. Obviously, in our case, we can see a low p-value, implying that the variable’s mean is indeed significantly different from zero. Further down in the output, you can also see information about the quantiles and some extreme observations.</p>
<p>Let’s now clear these results and move on.</p>
<p>By adding the HISTOGRAM statement to PROC UNIVARIATE, we can also get a frequency distribution. Here you can see the frequency distribution of age, which is as anticipated, rather skewed.</p>
<p>We clear the results and proceed.</p>
<p>By including a CLASS statement, we can do a class-by-class analysis. This is very handy to analyze the Goods and Bads separately during the exploratory data analysis step. Here you can see that the average age for a bad payer is about 34 years, whereas for a good payer, it is about 36 years. You can also see both histograms contrasted.</p>
<p>Again we clear the results.</p>
<p>Bar and pie charts can be constructed in SAS using PROC GCHART. We illustrate this for the purpose variable. Here you can see that purpose value 3 is the most popular, whereas value 6 is the least popular. We clear the results.</p>
<p>Scatter plots can be made using PROC GPLOT. Here we create a scatter plot between the age and amount variables. Let’s also run this statement. The scatter plot indicates a weak positive relationship between both.</p>
<p>Preprocessing the Data</p>
<p>Common Problems with Data There are various reasons for preprocessing data. Real-life data can be very dirty or noisy. It can contain lots of inconsistencies, and possibly, weird values. Consider the example data set depicted here. John’s value for Age is -2003. This value is definitely incorrect and can be the result of somebody entering the wrong birthdate. Sarah’s value for Income is 0. This could mean that either she is unemployed or maybe someone forgot to enter her income. Sophie has a missing value for Gender. It could be that the data has not been entered or that the customer is not willing to give the data because of privacy concerns. Although Sophie’s gender is obvious based upon her name, this is not that evident if you have big data sets with lots of missing values for different variables. Finally, also note that David has been included twice in the data set, creating a data duplication problem.</p>
<p>Dirty data can originate from data-integration and data-merging problems. For example, you can have one data source in which amounts are expressed in euros and another data source in which amounts are expressed in dollars. You cannot simply merge both data sources, as you can see here, for the income variable with values expressed both in dollars and euros. It is important that data is always coded in a consistent format.</p>
<p>Adequately preprocessing data is an important activity during the development of a credit scoring model. This can be motivated by the GIGO principle already introduced earlier. Remember, GIGO refers to garbage in, garbage out, which means that bad data will lead to bad models. Hence, it is important to carefully consider all the necessary preprocessing activities and take sufficient time to execute them. It is commonly known that preprocessing data takes about 80% of the total credit scoring model development effort. In what follows, we will discuss various preprocessing activities such as handling missing values, detecting and treating outliers, standardizing data, transforming data, coarse classification and grouping of variables, recoding categorical variables, segmentation, and definition of the target variable. Handling Missing Values A first important data preprocessing activity relates to missing values. Observations can have missing values for various reasons. It could be that information is simply not applicable. The default date, for example, is not known for non-defaulters. So if you’re interested in analyzing when people default, then you only have default dates available for the defaulters. Non-defaulters never default and thus have a missing value for time of default.</p>
<p>Another reason that information could be missing is because it has not been disclosed. For example, some customers did not disclose information about their income because they considered this to be part of their privacy. There could also be errors when merging data. For example, typos in a customer’s name or ID when merging observations, hereby creating missing values.</p>
<p>As we will discuss later, some analytical techniques such as decision trees can directly deal with missing values and thus do not require special preprocessing. Other techniques, such as regression, do require special preprocessing. In what follows, we will discuss the various options available.</p>
<p>Three important strategies to deal with missing values are keep, delete, and replace. Let’s look at each one into more detail.</p>
<p>The first strategy is keep. This means that you’re going to keep the fact that the variable is missing as separate information in your data. The reason why you want to keep missing information in the data is because missing information could be related to the target concept. For example, I did not tell you my income because I’m unemployed. Now, being unemployed could be related to whether I am a good or bad payer, so it is definitely something that you want to include in the data, as it may turn out to be significant in the analytical model. We can then choose to assign a special category to the missing value, or even a special variable. In fact, you can add a missing indicator variable, either one per variable or one for the entire observation.</p>
<p>Another, easier approach to deal with missing values is to simply remove them from the data. In doing so, you could opt to either remove the observation or remove the variable, depending on how the missing values are distributed. To give an example, if more than 90% of the values for a variable are missing, you could consider to delete it. In the example data set depicted here, you could opt to remove the Credit Bureau Score variable from the data set, as it has too many missing values.</p>
<p>A final option to deal with missing values is to replace or impute them with a known value. Various procedures exist to do this. Since missing values can occur both during model development as well as during model usage, it is important to be consistent when dealing with them in both cases.</p>
<p>To impute missing values, you can choose between simple procedures for both continuous and ordinal or nominal variables, or more advanced procedures based upon regression or decision tree models. We will discuss these in what follows.</p>
<p>For continuous variables, missing values can be replaced by the mean. For the example data set depicted, this would mean that the two missing values for Income would be replaced by the mean of the known values. Note, however, that the mean is sensitive to outliers, and since we haven’t treated those yet, a wiser option could be to use the median instead. If missing values can only occur during model development, you can also replace the missing value by the mean or median of all values within the given target class. For example, since observation 6 is a good payer, you can replace the missing value of Income with the average income for all good payers.</p>
<p>For ordinal or nominal variables, it does not make sense to look at the mean or median. Instead, you could use the mode or modal value, which is the most frequently occurring value for imputation. In our example data set, this would mean that the two missing values for Marital Status would be replaced with the status single, which is the most frequently occurring marital status. Again, if missing values can only occur in the model development data set, you can replace the missing value based upon the mode within the considered target class, Good or Bad.</p>
<p>Regression or tree-based imputation procedures are another way to deal with missing values. Here, a regression or decision tree is built to estimate the missing value based upon all the information available. For example, you could build a linear regression or decision tree predicting income based upon age, marital status, and credit bureau score. This model can then be used to find the values for the missing Income observations. Although this technique seems powerful, empirical evidence has shown that it often does not substantially add to the performance of the resulting model. Hence, it is perfectly fine to continue with the simpler imputation approaches, which we discussed earlier. Using SAS Programming to Impute Missing Values In this demonstration, we will illustrate how to impute missing values in SAS.</p>
<p>We start by creating a small data set, credit, with two inputs, income and savings. Note the black dots, which represent the missing values. Let’s now run this data set and quickly inspect it in the work library. We see that the data set has been successfully created. We now run PROC STANDARD with the REPLACE option. This will replace the missing values by the mean. The result of the PROC will be stored in a new data set called creditnomissing. Let’s now also inspect the creditnomissing data set. As you see, the missing values have been replaced by the mean, which is about 1743 for the income variable and 700 for the savings variable.</p>
<p>Note that PROC STANDARD can only do imputation for continuous variables, whereby missing values are always replaced by the mean. For more advanced imputation procedures, it is recommended to use SAS Enterprise Miner as we will discuss later.</p>
<p>Overview of Handling Outliers A next important preprocessing activity concerns the detection and treatment of outliers. Outliers are extreme or unusual observations. In the example data set depicted, you can see that John’s age is 300, which is clearly an outlier. Also, the income of Tobias is 1000000, which is again an outlier. Possible causes for outliers are recording errors, data entry errors, or just noise. Different types of outliers exist, so let’s discuss them into more detail.</p>
<p>When looking back at the outliers depicted in the earlier example, you can clearly see a conceptual difference. An age of 300 years is definitely an incorrect value. However, an income of 1000000 is not necessarily wrong. It could be a valid observation, but with an unusually high income. It is very important to distinguish between valid and invalid outlying observations because it will have an impact on the treatment, as we will discuss later.</p>
<p>Outliers can also be hidden in one-dimensional views of the data. Here you can see the values for the variables Age and Income. When looking at both variables univariately, we cannot see any unusual patterns or outliers. However, when combining both Age and Income into a scatter plot, you can see two multivariate outlying observations. One of them is a young person who earns a lot, whereas the other one is an older person who doesn’t earn that much. Both are outlying when considering Age and Income simultaneously, rather than separately. Note that we here only combine two variables. You can easily generalize the idea of multivariate outliers to three, four, or more variables.</p>
<p>A first important step in dealing with outliers is to detect or find them. Different methods exist to do this for both univariate and multivariate outliers. Once they have been found, you need to apply treatment methods to take care of them. We will discuss both in more detail in what follows. As always, it is again important to document all the outlier detection and treatment steps in a comprehensive way. Detecting Outliers We will start by discussing methods for detecting outliers. First, we will cover univariate outlier detection methods. These include basic statistics such as the maximum and minimum values of a variable, histograms, z-scores, and box plots.</p>
<p>A straightforward approach to find outliers is to simply calculate the minimum and maximum of a variable. In the data set depicted, you can see that the minimum value of Age is -20 and the maximum value is 300. Both clearly represent outliers. For the Income variable, we can see that the maximum value equals 1000000, which is again an outlier.</p>
<p>Histograms can also be used to find univariate outliers. Remember, a histogram provides a visual display of the distribution of a variable. Here you can see an example histogram for the variable Age. The histogram clearly shows two outlying categories being the category 0 to 5 years and the category 150 to 200 years.</p>
<p>Z-scores calculate how many standard deviations away from the mean an observation lies, and are also handy to detect outliers. They are calculated by subtracting the mean of the variable from the observation’s value and dividing by the standard deviation. For the sake of simplicity, in the example data set provided, the Age values are all multiples of 10. Let’s assume that the average age equals 40 and the standard deviation equals 10. You can then see that the z-score for the value 30 is -1, since it is minus one standard deviation away from the mean 40. For 50, the z-score then becomes +1 and so on. Note that, by definition, the mean of all z-scores equals 0, and the standard deviation equals 1. The higher the z-score in absolute value, the further an observation is away from the mean, and thus the more likely it is an outlier. A practical rule of thumb then defines outliers when the absolute value of the z-score is bigger than 3. The z-scores can be easily calculated in SAS using PROC STANDARD.</p>
<p>Box plots are another popular visual aid for finding outliers. A box plot represents five numbers. First, it represents the three key quartiles of the data. The first quartile, Q1, is the value such that 25% of the observations have a lower value. The median or the middle value, M, is the value such that 50% of the observations have a lower value, and the third quartile, Q3, is the value such that 75% of the observations have a lower value. All three quartiles are represented as a box. The minimum and maximum values are then also added unless they are too far away from the edges of the box. Too far away is then quantified as more than 1.5 times the interquartile range, which is the difference between Q3 and Q1. In the example figure given, you can see that there are three outlying observations to the right of the plot.</p>
<p>Various methods can be used to find multivariate outliers. A first one is to calculate the Mahalanobis distance between each observation and the mean of the population. Note that, as opposed to the Euclidean distance, the Mahalanobis distance does take into account the variability or spread of the data. Another method would be to cluster the observations and to look for sparse clusters containing only a few observations, which are then most likely the multivariate outliers. You can also estimate regression models and look for observations with large errors by inspecting the residual plots. Note, however, that practical experience has shown that it’s best to focus on the univariate outliers, since dealing with multivariate outliers requires a lot more effort, and the added value thereof is usually not very significant. Treating Outliers Once we have found the outliers, we need to treat them. Concerning the treatment, a distinction needs to be made between methods for invalid outlying observations and methods for valid outlying observations. Let’s get into more detail. Remember, in the example data set, John’s age is 300, which is clearly an invalid value. In order to treat this, you can basically consider it as a missing value and use any of the methods discussed in that section. In other words, we could keep all invalid observations into a separate category, delete the observation or variable in case the latter would have too many invalid values, or opt to replace or impute the invalid value with a valid value such as the mean or median.</p>
<p>For valid outlying observations, other methods need to be adopted. Consider the example of the income of Tobias, which is 1000000. Assuming this is a valid value, it makes no sense to delete the observation or replace it by the mean or median, since you are then messing up a good value. You could, however, consider to keep the value as is and put it into a special category with separate analysis. Another option would be to use a truncation scheme, which is also referred to as winsorizing or capping. The idea here is to make an extreme observation less extreme, so as to reduce its impact on the analytical model. This is especially relevant for regression-based models, which are highly sensitive to outliers, whereas other analytical techniques, such as decision trees, are fairly robust with respect to outliers and do not require special treatment.</p>
<p>Here you can see an example of a truncation-based scheme based upon the z-scores. The idea is to compute the limits of a variable’s range based upon the mean plus or minus three times the standard deviation. Any values lying outside these limits are then brought back to these limits to reduce their influence. As an example, this could imply that the income of Tobias of 1000000 is replaced by 10000, which is still very big, but less than it originally was. As you will understand, this scheme does not apply for the invalid outlying values. It makes no sense to replace an age of 300 with an age of 90, for example.</p>
<p>Another way to calculate the limits is by using robust estimates based upon the interquartile range, as you can see illustrated in the first bullet here. Note that, for further information about these values, I refer to the article mentioned by Van Gestel, myself, et al. Also a sigmoid transform can be used to smooth the impact of valid outlying observations. Finally, you can also rely on limits set by business experts in case sufficient business knowledge is available. Using a Box Plot to Detect and Treat Outliers In this demonstration, we will illustrate how to create box plots in SAS.</p>
<p>PROC UNIVARIATE allows to create box plots by including the PLOT option. Let’s now create a box plot for the age variable by running this statement. We can clearly see some outliers for this variable right here.</p>
<p>Demo Code: proc univariate data=mydata.applicants plot; var age; run; Standardizing Data Standardizing data is a preprocessing activity which aims at scaling variables to the same range. Consider, for example, the variables Salary, Age, and Employment Status. All three variables have a different range (Salary between 0 and 100,000 let’s say, Age between 20 and 70, and Employment Status is either 0 or 1). If you use those variables as such in a logistic regression model, whereby the output is bounded between 0 and 1, as we will discuss later, it may well be that the coefficient for Salary will become very small. Small coefficients are not nice to interpret and can also lead to optimization issues. Also, variables with a larger range can easily overpower other variables. Various methods exist to do standardization. Let’s discuss those into more detail.</p>
<p>First, we have min/max standardization. This method scales variables to a new range by applying a linear transformation, whereby newmin represents the new minimum and newmax represents the new maximum. Both newmin and newmax can be set by the analyst. For example, one could set newmin to 0 and newmax to 1 to scale the variable to the 0/1 range.</p>
<p>Z-score standardization scales variables by representing them in terms of the z-scores. Remember, as discussed in the section on outliers, z-scores measure how many standard deviations away from the mean an observation is situated. For the Salary and Age variables, the standardized range will thus depend upon the values of the average and standard deviation.</p>
<p>Decimal scaling is another method, which simply divides by a power of 10. In the example given, Salary could be divided by 10 to the power 5, such that the new range becomes 0 to 1. Age could be divided by 10 to the power 2, such that the new range becomes 0.2 to 0.7.</p>
<p>Standardization is an activity to rescale variables to a similar range. Whether it is needed depends upon the analytical technique, which will be used to develop the credit scoring model. Regression-based approaches could definitely benefit from it, whereas for decision trees, it is not needed. Using SAS Programming to Standardize Data In this demonstration, we will illustrate how to standardize data in SAS.</p>
<p>We start by creating a small data set, credit, with two inputs, income and savings. Let’s now run this data set and inspect it in the work library. Here we can see that the data has been successfully created.</p>
<p>We now run PROC STANDARD on this data set with the option mean=0 and std=1. This will calculate the z-scores or number of standard deviations each observation is away from the mean. Remember the mean of the z-scores equals 0 and the standard deviation 1. The results of this PROC will be stored in a new data set called creditstand. Let’s now also inspect this creditstand data set. Here, you can see that the z-scores have been successfully calculated. As a verification, we now run PROC MEANS on the creditstand data. In the output, we can clearly see that the mean is zero (or very close to it due to precision issues) and the standard deviation is indeed 1.</p>
<p>Transforming Data In the data transformation preprocessing step, additional transformations are applied to the data. These transformations can be considered for various reasons, such as to obtain a more symmetric or normal distribution or remove the correlation between variables. The logarithmic transform is a popular choice, although more complex transformations could also be considered. Let’s get into more detail. Here you can see the Amount variable, which has a very skewed distribution, as is typically the case for any size-related variable. Let’s now apply a logarithmic transformation and see what happens.</p>
<p>By taking the logarithm of the Amount values, you can clearly see that the obtained distribution is a lot more symmetric and also closer to a normal distribution. Logarithmic transformations are often applied to size variables such as asset size, loan amounts, and gross domestic product, because typically the corresponding distributions are very skewed.</p>
<p>Besides logarithmic transformations, you could also consider more complex transformations. Principal component analysis transforms a set of variables to a set of uncorrelated principal components, which are linear combinations of the variables. The Box-Cox family of transformations can also be used to make variable distributions more symmetric and normal.</p>
<p>To wrap up, various transformations can be used to further modify the data. However, you should be very careful when applying these transformations, since it’s typically going to make your analytical models more complex and thus more difficult to understand. In credit scoring, the logarithmic transformation is used occasionally. Other transformations are very uncommon. Upon doubt whether it is useful to apply a transformation or not, you can always estimate a credit scoring model both with and without the transformed data and gauge the impact on the model performance. When the performance benefit is non-existing or marginal, you can proceed with the untransformed data. Otherwise, the transformation can be applied, but again, don’t forget to also properly document it. Overview of Coarse Classification Coarse classification is a very important data preprocessing activity, which aims at grouping variable values. It needs to be done for both the categorical as well as the continuous variables. Various synonyms exist for this activity with the most popular ones being categorization, classing, binning, and as already mentioned, grouping. In what follows, we will first discuss why you should consider doing coarse classification.</p>
<p>We start by considering coarse classification for categorical variables. Here you can see a credit scoring data set with a categorical Purpose variable. The Purpose variable specifies the purpose of the loan, and let’s say it can have 100 different values corresponding to 100 different loan purposes. Let’s also assume that we are building a linear regression model to relate the binary good-bad target to the Age and Purpose variable. Remember, in linear regression, all variables need to have numerical values, and the regression parameters are then optimized using the idea of minimizing the sum of squared error terms. How can we now include the categorical and textually encoded Purpose variable into our linear regression model?</p>
<p>We can use two methods to include the Purpose variable into our linear regression model. The first method is code it numerically and the second one is to use 0/1 dummy variables. Let’s discuss both into more detail. We could start by coding Purpose in a numeric way. We assign the value of 1 to Purpose is car, 2 to Purpose is cash, 3 to Purpose is travel, and so on. We now have a numerical Purpose variable, which can be directly used in the linear regression model. However, by adopting this method, we have transformed a nominal variable to an ordinal variable. Indeed, we are now assuming that cash equals two times car, travel equals three times car, and so on. Clearly, this is not a good approach to work with.</p>
<p>Another option is to use 0/1 dummy, or indicator variables. In this method, every individual Purpose value will now get its own regression coefficient. For example, the regression coefficient β2 now indicates the impact of having a car loan on the Good-Bad risk, the coefficient β3 indicates the impact of having a cash loan on the Good-Bad risk, and so on. Although this method is a lot more appealing, it creates other problems. If we have 100 different Purpose values, we will end up with 100 different dummy variables, and thus, 100 different coefficients for the purpose characteristic. This creates two problems. First, it creates a correlation problem between the purpose dummy variables, since they always sum to 1 for a particular observation. This can be easily remedied by leaving out one purpose dummy as the reference category and work with 99 dummy variables instead. Another, bigger problem is that our analytical model now becomes very big, since we have to estimate 99 coefficients for the Purpose variable.</p>
<p>The goal of using coarse classification for the categorical Purpose variable is to group the 99 values into a few categories, say three or four, to reduce the size of the model and number of coefficients to be estimated. This will make the estimates more robust, since we now need to estimate less parameters with the same amount of data.</p>
<p>Now that we have discussed coarse classification for categorical variables, an obvious question that arises is why do we need coarse classification for continuous variables. Well, let’s find out.</p>
<p>Here you can see a graph plotting the relationship between Default Risk and Age of a customer. This pattern was found by my colleague and friend professor Lyn Thomas from the University of Southampton in the citation mentioned below. As you can see, the graph is non-monotonically decreasing. It decreases until around the age of 26, then increases back again until around 32, followed by a more or less monotonic decrease. Many reasons can be thought of for the risk increase starting at the age of 26. Examples are kids, marriage, or maybe even an early mid-life crisis. Now, if we could approach this nonlinear pattern with a linear regression model, as is typically done in credit scoring, the best fit would be a linear decreasing line. This would imply that we miss out on the local non-monotonic behavior in the age range between 26 and 32. By coarse-classifying the Age variable into three categories (less than 26, 26 to 32, and 32 plus), we can have separate regression coefficients for each of these categories and thus approximate the nonlinear behavior in a piece-wise linear way.</p>
<p>Hence, coarse classification of continuous variables can be beneficial to introduce non-linear or non-monotonic effects into linear models. Note, however, that if you were using nonlinear models such as neural networks, then coarse classification would not be needed, as these models can directly cope with the non-linear patterns in the data. Coarse Classification Methods Now that we have discussed why coarse classification could be interesting for both categorical as well as continuous variables, let’s zoom into some coarse-classification methods. In what follows, we will discuss binning methods, chi-square methods, and briefly zoom into some other methods as well.</p>
<p>A first, very basic approach towards coarse classification is binning. Let’s assume we have an Income variable, which has only six values as depicted. Equal interval binning creates groups by splitting the variable’s range into equal-sized parts. Since the range is from 1000 to 2000, and assuming we want two bins, the first bin would then be from 1000 to 1500, and the second one from 1500 to 2000. Note that the first bin has four values and the second bin has two values.</p>
<p>Equal frequency binning, or histogram equalization, creates bins based upon the frequency. In other words, it ensures that every bin has the same number of values. In our case, assuming we create two bins, the first bin contains the lowest three values and the second bin the highest three values. It is clear that both these binning approaches are very basic in the sense that they do not take into account the good-bad target variable or default risk during the binning process.</p>
<p>A more sophisticated method for doing coarse classification is based on chi-square analysis. Let’s illustrate how this works with an example taken from the book “Credit Scoring and Its Applications” written by Lyn Thomas et al., and published in 2002. Here you can see a variable Residential Status, which has values Owner, Rent Unfurnished, Rent Furnished, With Parents, Other, and No Answer. The table depicts the distribution of Goods and Bads across these values. Let’s say we are considering two options for coarse classification. Option 1 is Owners, Renters, and Others, whereas option 2 is Owners, With Parents, and Others. We will now analyze which is the best coarse classification option using chi-square analysis.</p>
<p>Let’s first zoom into option 1. We start by building a table with the empirical frequencies for option 1. The number of good and bad owners can be directly copied from the previous table. The number of good renters is the sum of the number of good unfurnished renters, which was 1600, and the number of good furnished renters, which was 350, thus equaling 1950 in total. The other numbers in this table are computed in a similar way. We will now contrast these empirical frequencies with the independence frequencies. The independence frequencies are the frequencies that are obtained by assuming that the Good-Bad status is independent from the residential status. In other words, assuming independence between both variables, the number of good owners becomes 6300/10000 * 9000/10000 * 10000 or 5670, as you see depicted in the table below. Following this reasoning, how would you calculate the independence frequency for the bad renters? Well, this equals 1000/10000 * 2490/10000 * 10000 or 249.</p>
<p>Once we have all independence frequencies calculated, we can compare them with the empirical frequencies. Now, what do we want to have a good coarse classification? Do we want both tables to be as similar as possible or as dissimilar as possible? Well, let’s think about it. Assume we would have a perfect match of the numbers in both tables. This would imply that what we observe empirically is independence, or the good-bad variable is independent from the residential status variable. This is clearly not what we want since we want both to be related as much as possible. In order to have a good variable, we want dependence. In other words, we want to have the empirical frequencies as different as possible from the independence frequencies. So, we need a measure to quantify the dissimilarity.</p>
<p>The chi-square statistic is a measure which quantifies the difference between the empirical and independence frequencies. It is calculated by summing the ratio of the squared difference between the empirical and independence frequencies and the independence frequencies across all cells of the table. In our case, the value is 583. The bigger this value, the bigger the dissimilarity between both tables, and thus, the more dependence there is between the good-bad variable and the Residential Status variable.</p>
<p>In order to formally judge upon its significance, the obtained chi-square statistic should follow a chi-square distribution with k minus 1 degrees of freedom, with k the number of classes of the characteristic, which is 3 in our case. This can then be summarized by a p-value to see whether there is a statistically significant dependence or not. We can now also compute the chi-square value for option 2. Following a similar computation, we can see that the value is 662. Since both options assume three categories, we can directly compare the value of 662 to 583, and since the former is bigger, conclude that option 2 is the better coarse classification.</p>
<p>Other methods can also be used for performing coarse classification. Examples are decision-tree-based methods and methods based upon the information value statistic. Both will be discussed later. A very handy and simple method is based upon the use of pivot tables. Let’s discuss this briefly with an example.</p>
<p>Here you can see an example of a credit-scoring data set with a categorical Purpose variable. A pivot table can now be created to compute the number of Goods and Bads for each of the Purpose values. This will then in turn allow to calculate the odds, which is simply the number of Goods divided by the number of Bads. For example, for car, the number of Goods equals 1000; the number of Bads equals 500; resulting in an odds of 2. We can now perform coarse classification by grouping Purpose values with similar odds. Let’s say we want three groups for our example. Group 1 can then consist of car and study, since both have the lowest odds. Group 2 is house, and group 3 is cash and travel since both have the highest odds.</p>
<p>As a closing remark, note that also the business expert can be actively involved in the coarse-classification process and adjust or even override any of the coarse classifications found by the statistical procedures. Performing Coarse Classification In this demonstration, we will illustrate how to do coarse classification in SAS.</p>
<p>We hereby make use of the example discussed in the course. Remember we had an attribute residential status, which has values: owner, rent unfurnished, rent furnished, with parents, other, and no answer. Here you can see the data set residence, which represents the number of Goods and Bads for each of the residential status values. Remember that the dollar sign indicates a textual input. Let’s now run this data set.</p>
<p>We now consider two categorizations. Option 1 is owners, renters, and others, whereas option 2 is owners, with parents, and others. For both these options, we created two separate SAS data sets, called coarse1 and coarse2. Note that we manually added the numbers, so 1950 good renters is just the sum of the 1600 good unfurnished renters, and the 350 good furnished renters. Let’s now run these data sets.</p>
<p>We can now run PROC FREQ on both these SAS data sets. Note that the PROC FREQ statement explicitly asks for the chi-square statistic, which will allow us to decide upon the best coarse classification. Let’s run the first PROC FREQ. You can see that the chi-square value equals about 584. Let’s now also run the second PROC FREQ. Here, the chi-square value equals 662, which is higher than the previous one. Hence, option 2 owners, with parents, and others is the better coarse classification.</p>
<p>Recoding Categorical Variables Let’s assume that we have coarse-classified the variable Age into four categories. Remember, we did this to take into account nonlinear effects. We also coarse-classified the Purpose variable into five categories, so as to avoid having to use too many dummy variables. Taking into account the fact that one of the dummy variables always serves as a reference category, with a zero-valued coefficient, this implies we need to estimate eight parameters in total (one for the intercept variable, three for the Age variable, and four for the Purpose variable). This is quite a lot given the fact that we only use two variables, Age and Purpose.</p>
<p>Although the coarse classification reduces the number of parameters for the categorical variables, it introduces new additional parameters for the continuous variables. A practical question now is, can we not make the model more parsimonious by estimating only one parameter for Age and one parameter for Purpose, respectively? In other words, is there a monotonic transformation f(), which we could define, which is monotonically related to the target variable Y, representing the Good-Bad status? This transformation can either be monotonically increasing or decreasing resulting in either a positive or negative value of the beta parameter, respectively.</p>
<p>A popular transformation to do this is the weights-of-evidence transformation, abbreviated as WOE. Let’s have a look at the following example of a coarse-classified Age variable. Assume we have a data set of 2000 observations, which is rather small, but enough for explaining how the weights-of-evidence coding works. Fifty customers have a missing value for Age, 200 customers have an Age between 18 and 22, and so on. Fifty out of 2000 equals 2.50%, such that 2.50% of the customers have a missing value for Age. We will now look at the Goods and Bads separately. There are a total number of 1806 Goods, 42 of which have a missing value for Age. This equals 2.33%. In other words, 2.33% of the good customers have a missing value for Age. There are a total number of 194 Bads, eight of which have a missing value for Age. This equals 4.12%. In other words, 4.12% of the bad customers have a missing value for Age. The Weights of Evidence, which is the final column, can then be calculated as the logarithm of the blue number, representing the distribution of Goods, divided by the green number, representing the distribution of Bads. The result can then be multiplied by 100 to represent it as a percentage.</p>
<p>How is the weights of evidence now monotonically related to the Good-Bad risk? Well, remember that a logarithmic curve crosses the X axis at the value of 1, since the logarithm of 1 is 0, or in other words, every number you raise to the power 0 equals 1. So if the weights of evidence is bigger than 0, it means that the input to the logarithmic transformation is bigger than 1, or that there are more Goods than Bads within the category. Indeed, you can see this for the age categories 30 to 35, 35 to 44, and 44+, where the distribution of the Goods is higher than the distribution of the Bads, resulting into a positive weights of evidence; vice versa if the weights of evidence is negative. This means that the distribution of Bads is higher than the distribution of Goods. This can be seen for the age categories missing, 18-22, 23-26, and 27-29.</p>
<p>Based upon the weights of evidence, we can now define a new measure called the information value, which will allow us to determine the predictive power of a variable. Let’s reconsider our earlier example for the Age variable. Looking at the table, how could you determine whether Age is an important predictor for the target Good-Bad risk?</p>
<p>Well, if Age would be important in discriminating between good and bad payers, you would expect certain age categories where you have a concentration of good payers, and other age categories where you have a concentration of bad payers. This very basic intuition can now be quantified by means of the information value. The information value is the sum across all categories of the product of the difference between the distribution of Goods and Bads, multiplied by the weights of evidence. If, for a particular category, the distribution of Goods is bigger than the distribution of Bads, then the weights of evidence will also be positive and the product will thus be positive. Vice versa, if for a particular category, the distribution of Goods is smaller than the distribution of Bads, then the weights of evidence will be negative but the product of both will remain positive. In other words, the product tells us something about the absolute difference between the distribution of Goods and Bads in a particular category. Summed across all categories, it gives us an aggregate measure for the difference. Hence, the higher the value of the information value, the more predictive the variable is, since it gives us more information about the target. The information value measure can now be used in various ways. It can first be used to assess the appropriateness of the coarse classification. In fact, it can help to adjust the coarse classification so as to increase the information value of the coarse-classified variable. It can also be used for variable screening, since a higher information value represents a more predictive variable.</p>
<p>Various rules of thumb are available to gauge the predictive power of a variable using the information value. If the information value is less than 0.02, then the variable is not predictive and can be left out for further analysis. If the information value is between 0.02 and 0.1, then the predictive power is considered to be weak. Above 0.1, the variable is assumed to be medium predictive, and above 0.3 strongly predictive. Instead of using those rules of thumb, you can also calculate the information value of all your variables and simply proceed the analysis with the top 10 or 20% variables having the highest information value.</p>
<p>Once the weights of evidence have been calculated, they can be contrasted with prior domain experience provided by the business experts. Here you can see a graphical display of the weights-of-evidence values for the Age variable. The graph confirms that younger people tend to represent a higher risk than the older population, which is clearly in line with the business experience. Segmentation Segmentation is another preprocessing activity. The aim is to create segments or clusters in the data for which separate credit scoring models will be built. This can be motivated because the data can sometimes be too heterogeneous in order to be analyzed by one credit scoring model. This heterogeneity can have various causes. A first one is strategic. Banks may want to adopt special strategies to specific segments of customers. Another reason is operational. New customers must have a separate model because the characteristics in the standard model do not make sense for them operationally. Finally, there could also be significant variable interactions, which cannot be properly captured into one credit scoring model.</p>
<p>Various approaches are available to do segmentation. A first one is experience-based. Here, it is the credit business expert who will make the segmentation based upon experience and business knowledge. Also statistical methods can be used to do segmentation. The most popular ones are k-means clustering or decision trees. You can also adopt a hybrid approach by combining both methods. If the segmentation is successful, then the predictive performance of the combined credit scoring models will exceed the performance of using only a single credit scoring model.</p>
<p>Here you can see an example of using a decision tree for segmentation. The decision tree is only two levels deep, hereby creating four segments. For each of these segments, you can then build separate credit scoring models.</p>
<p>In this example, you can see an expert-based segmentation scheme whereby the entire credit data set is first segmented based upon duration, then based upon new or existing customer, and so on.</p>
<p>To conclude, segmentation is an important preprocessing activity to take care of the heterogeneity in the data. Although it could definitely be beneficial from a predictive performance perspective, you should be aware of the drawbacks that come with it. More segments means more credit scoring models, which automatically implies bigger efforts for model maintenance, monitoring, and backtesting. These issues should also be considered when deciding upon the optimal segmentation. Defining the Target Variable To conclude data preprocessing, let’s say a few words about the definition of the target variable. Remember, when building a credit scoring model, the target variable is a good or bad payer, or in other words, defaulter or not. With the introduction of compliance guidelines such as Basel II and Basel III, the definition of a defaulter has been set to 90 days in payment arrears. Some local regulators have changed this definition. For example, in the United States, for residential mortgages and revolving exposures, the default definition is 180 days, whereas for other exposures, it has been set to 120 days. Some banks use proprietary definitions of default or bad payer based upon charge off, profit, net present value, or even fraud.</p>
<p>To measure the stability of the default definition, roll-rate analysis can be used. The purpose here is to see how customers migrate from one delinquency status to another one during a specific period of time. The graph shows four delinquency states: more than 90 days in arrears, which is indicated in red; 60 to 90 days in arrears, which is indicated in orange; 30 to 60 days, which is indicated in yellow; and less than 30 days, indicated in green. From the graph, it can be seen that once the 90+ delinquency status has been reached, about 60% of the customers will remain in this delinquency status and only a few will recover. Hence, this indicates that this is a good default definition. Using SAS Enterprise Miner to Explore and Preprocess Your Data In this demo, we will illustrate how to work with SAS Enterprise Miner version 13.1. Enterprise Miner is the data mining product shipped by SAS to build analytical models. It can be started using the Start button in Windows, or a shortcut icon on the desktop. This brings us to the following starting screen. As you will see, this screen asks for a SAS environment for which we chose the default SAS environment. It also requests a user ID and password, which has already been entered for us. We now click the Logon button to log on to the software.</p>
<p>After having clicked the Logon button, we get the following Welcome screen. In this Welcome screen, we can browse Help topics, create a new project, open an already existing project, open a recent project, view metadata, or exit the software. We will continue the demo by creating a new project. This will start up a four-step wizard. In step 1 of this wizard, we specify the SAS server for the project. This is the server, which will handle all the processing. We accept the default setting and click Next. In step 2, we can specify the project name and the SAS Server directory where the project files will be stored. We will name the project MyCreditProject1 and set the directory to c:. We now click Next. Step 3 allows us to select the SAS Folders location for this project. Again we accept the default setting and click Next. Step 4 then summarizes all the settings specified thus far and allows us to finish the wizard.</p>
<p>This bring us to the SAS Enterprise Miner environment. As you will see, the interface is divided into components: • This is toolbar 1. This toolbar provides a set of common utilities to assist in building your projects. • This is toolbar 2. This toolbar is a graphical set of node icons and tools that you use to build process flow diagrams in the Diagram workspace. • This is the Project panel. You can use the Project panel to manage and view data sources, diagrams, model packages, and list users. • This is the Properties panel. You can use the Properties panel to view and edit the settings of any object that you select, including data sources, diagrams, nodes, results, and users. • This is the Diagram workspace. You can use the diagram workspace to build, edit, run, and save process flow diagrams. This is where you graphically build, order, and sequence the nodes that you use to mine your data and generate reports. • This is the Help panel. The Help panel displays a short description of the property that you select in the Properties panel. Extended Help can be found in the Help Topics selection from the Help main menu. We will now continue our demo by importing our data into SAS Enterprise Miner. First, we have to create a SAS library. This can be done by means of the Project Start Code property. We click the button next to this property.</p>
<p>We are now ready to import our data into SAS Enterprise Miner. We right-click on the Data Sources folder in the Project panel to open the Data Source Wizard. We accept the SAS Table option in Step 1. In Step 2 of this wizard, we select the applicants data set from the earlier defined mydata library. Step 3 then presents some general information about the data. We prefer to manually set the metadata information and leave the Metadata Advisor option to basic in Step 4. Step 5 allows us to set the measurement role and measurement level of all our variables. We will set the measurement role of the good_bad variable to Target. We leave all the other variables to Input since they will serve as predictors in our model. We will now also set the measurement levels as follows: We will set checking and savings to Ordinal; good_bad to Binary; age, amount, depends, duration, installp to Interval; and all the other ones to Nominal. Note that this should typically be done using a data dictionary, which, for our data set, is also provided in the course material.</p>
<p>We can now also briefly explore the distribution of some of the variables. We select the age variable and click the Explore button. This shows the expected skew distribution of age, together with some of its summary statistics like minimum, maximum, and mean. We close the window and do the same for the purpose attribute. Here we can see that purpose number 3 is the mode, or most popular purpose value. We again close this screen. We now complete the wizard by clicking Next, Next, Next, Next, and Finish. The data has now been imported into SAS Enterprise Miner and is ready to be analyzed.</p>
<p>We will now create our first diagram by right-clicking Diagrams, Create Diagram, and enter Mydiag1, followed by OK. We now see that toolbar number 2 becomes activated and provides us with a whole range of nodes to do data analysis. When moving with the mouse pointer above each node, an accompanying text specifying the purpose of the node is being displayed.</p>
<p>We now drag and drop the applicants data set to the diagram workspace. From the Explore tab of toolbar 2, we add a Multiplot node to the diagram workspace and connect it to the applicants data set. We can now inspect the properties of this node. We can choose the graph orientation to be vertical or horizontal. We can choose to include missing values Yes or No and so on. We will keep the properties to their default values and run the node by right-clicking it and selecting Run. Now, we click Results and inspect the results of this node. This plot basically displays the distribution of each variable colored according to the number of Goods and Bads. The buttons below allow us to navigate between the different variables. This Multiplot node is a very handy node for exploratory data analysis. Let’s now close the node and continue with the demo.</p>
<p>We will add a Sample node from the Sample tab. We connect the Sample node to the applicants data set and set the Sample method to stratify. This will create a random stratified sample whereby the stratification is done by using the target variable good_bad. The percent property is set to 10%, which means that a sample of 10% of the observations will be drawn. Given that we have 1000 observations, the sample will thus have 100 observations. We run the node by right-clicking it and choosing Run. We now select Results. We can see that the sample indeed has 100 observations and we can see that the Good-Bad distribution in the sample is the same as in the original data. We have 30% Bads in the sample and we had 30% Bads in the original data. To explore the sample into more detail, we close the Results screen and we click on the button next to Exported Data. We can here select the sample data and click the Explore button to inspect the data itself and some accompanying statistics. Here you can see the data itself and here you can see some statistics like minimum, maximum, mean for the different variables. We now close all the screens back again and continue with the demo.</p>
<p>In a next step, we will now filter some of the outliers. From the Sample tab, we add a Filter node to the diagram workspace and connect it to the applicants data set. Let’s now inspect the Class variables section of the Properties panel. This section right here. For class variables that have less than 25 values (which is the Maximum number of levels cutoff property), values that occur in less than 1% of the cases (which is the Minimum frequency cutoff property) will be removed. So to give an example, marital status has only four values: married, single, widow/widower, and divorced. So this is clearly less than 25, and if one of those values would occur in less than 1% of the cases, then the observation will be removed. In the interval variables section of the Properties panel, we can see the default filtering method for interval variables is standard deviations from the mean. When we click the button next to Tuning Parameters, we can now set the cutoff for standard deviation to 2.5 instead of 3. We are now ready to run the node and inspect the results. We can now see that purpose value number 8 has been deleted since it occurred in only nine observations, which is less than 1%. We can also see that existing credit value number 4 has been deleted, since it occurred in only six observations, which is less than 1%. By maximizing the output screen, we can see the filter limits for the interval variables. Remember that these have been determined using the 2.5 standard deviation limits from the mean. We can also see that out of the 1000 observations, 87 have been filtered out because of the specified filter settings. Let’s now close these results and add another node to the diagram workspace.</p>
<p>From the Explore tab, we add a StatExplore node to the diagram workspace. We connect it to the applicants data set, accept the default settings, and we run it. This node gives some exploratory statistics about our data. The graph in the upper left corner depicts the chi-square statistic for each of the categorical variables. It can also show the Cramer’s V statistic as discussed in the course. From this graph, it can be seen that the checking account variable is the most important categorical variable. Let’s now maximize the output screen. This output first shows some general information about the data set, and then zooms in separately into the class and interval variables. For the class variables, we can see the number of levels, the number of missing, the mode, the percentage of the mode (Remember the mode is the most frequently occurring value.), mode2, and the percentage of mode 2. Mode 2 is the second most frequently occurring value. This is followed by the summary statistics for the interval variables such as mean, standard deviation, and so on. A second portion of this output is then going to do a class-by-class analysis. So, it is going to separately analyze the Bads and the Goods. For the checking account variable, we can see that in the overall population, the mode is 4. However, when we analyze the Bads and the Goods separately, we can see that the mode is different. For the Bads, the mode is 1; for the Goods, the mode is 4. This clearly confirms our earlier finding that checking account is an important variable. We can continue to the age variable and look at the first continuous variable. Here we can see, when we inspect the age variable for the Goods and the Bads separately, we can see that on average bad payers are younger than good payers. Let’s now close these results.</p>
<p>To finish this demo, we will add the Transform node from the Modify tab to the diagram workspace and connect it to the Applicants node. We right-click the Transform node and select Edit variables. For the age and amount variable, we specify the method property to standardize. This will create standardized variables for both variables by calculating their z-scores. Remember the z-scores can be calculated by subtracting the mean and dividing by the standard deviation. We click OK, run the node, and inspect its results. Here we can see the two standardized variables, STD_age and STD_amount. We can see the formula that comes with each of these variables together with the mean of these variables, which is zero, or close to zero, and the standard deviation, which is one.</p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    

    
  </body>
</html>

