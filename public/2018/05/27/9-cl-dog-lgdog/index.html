<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.40.3" />


<title>9 cl dog lgdog - A Hugo website</title>
<meta property="og:title" content="9 cl dog lgdog - A Hugo website">



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/rstudio/blogdown">GitHub</a></li>
    
    <li><a href="https://twitter.com/rstudio">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">57 min read</span>
    

    <h1 class="article-title">9 cl dog lgdog</h1>

    
    <span class="article-date">2018/05/27</span>
    

    <div class="article-content">
      <p>Introduction In this lesson, we will discuss modeling loss given default or LGD. Remember, LGD is a very important risk parameter, since it has a linear impact on the capital requirements. To facilitate the discussion of LGD modeling, we will adopt the same multilevel architecture as we used for PD modeling. At Level 0, we will first review the data preparation. This will be followed by a discussion of how to create an LGD model at Level 1. At Level 2, we will present various methods to define loss ratings and calibrate the accompanying LGD.</p>
<p>Objectives</p>
<p>In this lesson, you learn to do the following: • define loss given default (LGD) • describe the Basel requirements for LGD models • identify the data preparation issues for LGD models • describe the following methods of building LGD models: segmentation, regression, and two-stage models • describe the main performance measures for LGD models • describe the main approaches to defining LGD ratings and calibrating LGD models • Text Version Collapse Print Level 0: Constructing an LGD Data Set</p>
<p>Introduction to Level 0: Constructing an LGD Data Set A crucial step in modeling LGD is preparing the data. Next to the typical data complexities already covered in PD modeling, various additional issues arise here, such as the definition of a default, the definition and calculation of LGD, handling issues in constructing an LGD data set such as the fact that it should cover at least one business cycle, the measurement of LGD, the definition of the workout period, the handling of incomplete workouts, the definition of the discount rate, the handling of LGDs outside the normal range, the inclusion of indirect costs, the identification of the drivers for LGD modeling, and finally also the data preprocessing. In what follows, we will discuss each of these into more detail. Defining a Default for LGD Modeling In order to quantify the loss given default, you first need to have a well-framed definition of default. For non-retail exposures, rating agencies such as Moody’s, Standard &amp; Poor’s, and Fitch use definitions of default, which, although to a large extent overlapping, are not identically the same. Hence, if you use different definitions of default, then of course, you cannot compare the resulting default and loss rates.</p>
<p>More specifically, there is a direct correlation between the default definition, the default rates, and the loss or LGD rates. Hence, when you see LGD rates reported, it is always important to ask for the default definition adopted to make sure you can correctly interpret and benchmark them.</p>
<p>Usually, a bank will distinguish between different types of defaults. An operational default is due to technical issues at the obligor side. For example, an obligor is accidentally late when making the payment. A technical default is a default due to an internal information system issue. For example, the payment was made on time, but on the wrong account. A real default is a default due to financial problems or insolvency. These are the defaults we are interested in when modeling LGD.</p>
<p>In case of default, various actions can take place. First, there can be a cure. This means a defaulter will pay back all outstanding debt and return to a performing, or thus non-defaulter status with no accompanying loss. There could also be a restructuring or settlement, whereby the bank and the defaulter work out a recovery or repayment plan. This could, for example, result into an extension of the loan maturity to reduce the monthly installment amount. This usually comes with a medium loss. Finally, there could also be a liquidation, repossession, or foreclosure, which implies that the bank takes full possession of the collateral asset, if available, and sells it by starting up a bankruptcy procedure. Depending upon the value of the collateral, this may come with a high loss.</p>
<p>When modeling LGD, it is of key importance that the default definition used is the same as for PD. Remember, PD and LGD will be combined to calculate both expected and unexpected loss. Hence, it is important that both definitions are consistent. Note that changing the default definition simultaneously impacts both the PD and LGD. If you would, for example, relax the default definition from 90 days to 60 days in payment arrears, then the default rates and PD will increase, but the loss rates and LGD will decrease. Hence, the combined effect in terms of expected loss stays relatively constant.</p>
<p>Cures are those defaulters that become non-defaulters or back-performing by repaying all outstanding debt. The corresponding LGD will thus be 0 or close to 0. As already mentioned, note that this depends upon the default definition. Relaxing the definition of a default (for example, from 90 to 60 days) will typically increase the number of cures. In case of multiple defaults, you could opt to only include the last default event and also relate the PD and EAD to this. Defining and Calculating LGD The loss given default can now be defined as the ratio of the loss on an exposure due to the default of an obligor to the amount outstanding at default. As such it is the complement of the recovery rate, or in other words, LGD equals 1 minus the recovery rate. Important to note here is that LGD focuses on economic loss, rather than accounting loss. Hence, all costs, but potentially also benefits, need to be properly taken into account when defining the LGD. Example costs are the costs for realizing the collateral value, administrative costs incurred (for example, by sending collection letters or making telephone calls with the defaulted obligor), legal costs, time delays in what is recovered, etc. Also benefits such as interests on arrears, penalties for delays, or other commissions can, however, be considered.</p>
<p>LGD can be measured using various methods such as the workout method used for both corporate and retail exposures, the market approach used for corporate exposures, the implied historical LGD approach used for retail exposures, and the implied market approach used for corporate exposures. In what follows, we will discuss each of these into more detail.</p>
<p>The most popular method for defining LGD is the workout method, which is frequently adopted for both corporate and retail exposures. The idea here is to work out the collection process of a defaulted exposure and carefully inspect the incoming and outgoing cash flows. Both direct and indirect cash flows should be considered. Example indirect costs could be the operating costs of the workout department. These cash flows should then be discounted to the moment of default to calculate the loss.</p>
<p>Here you can see a simplified example. Let’s assume an exposure goes into default with an exposure at default of $100. Soon after default, the collection department will contact the defaulted obligor either by telephone or by sending a collection letter. Let’s assume the cost for this equals $5. This is followed by the obligor paying back $20, which is clearly not enough to cover all outstanding debt. So, the collection department contacts the obligor again at a cost of $5. Let’s say that the obligor does not react, so the bank decides to materialize the collateral and receives $70 for it. We can now discount all these cash flows back to the moment of default using a discount factor, which we leave unspecified for the moment. Let’s say that the discounted amount equals 70. Note that this is smaller than the sum of the four numbers, which equals 80, because of the discounting that has been applied. In other words, this means that $70 has been recovered from the $100 EAD, hereby giving a recovery rate of 70% and an LGD of 30%.</p>
<p>Another way to measure the LGD is by using the market approach. The idea here is to look at firms that went bankrupt and have debt securities, such as bonds or loans, trading in the market. Once the bankruptcy event has occurred, the bonds will become junk bonds and investors will start trading them based upon what they think they will recover from the bankrupt firm. To allow for some time for the market to stabilize and absorb all information, this approach looks at the market price one month after the bankruptcy or default event. This market price is then used as a proxy for the recovery rate, which will then also allow to calculate the LGD as 1 minus the recovery rate. Note that this approach only works for debt securities that trade in the market, and thus not for retail exposures. This was the approach, which was followed by Moody’s in their LossCalc tool.</p>
<p>The implied historical LGD method works by using the PD estimates and the observed losses to derive the implied LGD. In other words, it calculates the expected loss first and then makes use of the expression expected loss = PD*LGD. The LGD can then be backed out as the expected loss divided by the PD. Note, however, that according to the EU regulation, this approach is only allowed for the retail exposure class.</p>
<p>Finally, the implied market LGD approach is a very theoretical approach and I have never seen it being used in the industry. It analyzes the market price of risky, but not defaulted, bonds using asset pricing models such as structural or reduced form models. It then finds the spread above the risk-free rate, which reflects the expected loss, and backs out the LGD from there. One of the key concerns of this approach is that the market price is only partially determined by the credit risk.</p>
<p>Before we continue the discussion on LGD modeling, let’s see what the Basel Accord itself has to say about it. According to Basel, “The definition of loss used in estimating LGD is economic loss.” As already said, this means that every cash flow or cost related to the default should be properly taken into account. In the foundation IRB approach, estimates of the LGD are prescribed in the Accord. For corporates, sovereigns, and banks, the following applies: “Senior claims on corporates, sovereigns, and banks not secured by recognized collateral will be assigned a 45% LGD. All subordinated claims on corporates, sovereigns, and banks will be assigned a 75% LGD.” Issues in Constructing an LGD Data Set When constructing an LGD data set, various issues pop up, such as: The data set should cover at least a complete business cycle. We should decide how we are going to measure LGD. The workout period needs to be defined. Incomplete workouts need to be handled. The discount rate needs to be defined. LGDs outside the normal range should be handled. Indirect costs should be included. The drivers of LGD need to be identified. In what follows, we will elaborate on each of these. Covering at Least One Business Cycle The data set used for LGD modeling should cover at least a complete business cycle. The obvious question that follows is: “What is a business cycle?” Well, for the retail portfolio, at least five years of data are needed. For wholesale, or corporates, sovereigns, and banks, at least 7 years of data are needed. Preferably, the data should include one or two downturn periods. This will be handy for the LGD calibration, as we will discuss later. Note that you don’t need to attach equal importance to each year of data. Hence, if you think data of five or seven years ago is less relevant today, you can attach a lower weight to it. Measuring LGD Two approaches can be used to compose the data set for LGD modeling when using the workout method. They differ based upon the event they consider during a particular period. The cohort approach considers the time of default, whereas the vintage approach looks at the loan origination date. Let’s discuss both in more detail.</p>
<p>Both approaches can be easily understood by using a Lexis chart. This chart is a very handy visualization tool to analyze the behavior of populations as they mature in a particular process. The X axis represents the calendar Time, and the Y axis the Age. In an LGD context, every defaulted loan is represented as a line. The line begins on the X axis at loan origination.</p>
<p>For example, during both 2005 and 2006, two loans were started. Every line continues diagonally upward as the loan ages. The default date of the loan is marked by a cross. After default, the workout period starts, which is represented as a dashed line. The end of the workout period is represented by a circle. The cohort approach now looks at all defaults that occur within a given year to build the LGD data set for that particular year. In our chart, the January 2006 cohort then contains three defaults for which the workout period has ended. The January 2011 cohort also contains three defaults, whereby, for two defaults, the workout period hasn’t ended yet. These are what we refer to as incomplete workouts.</p>
<p>The vintage approach looks at loan origination date instead of default date. It is quite often used for retail exposures. In our example, the 2005 vintage contains two defaults for which the workout period has finished. The 2008 vintage contains one default for which the workout period has also finished. Defining the Workout Period The length of the workout period can vary depending upon the type of credit, the workout policy of the financial institution, and the local regulation. Some regulators, such as the Bank of International Settlements (BIS) or the Hong Kong Monetary Authority (HKMA), have provided further input on this. Here you can see some of their guidelines. The workout period can finish when the non-recovered value is less than 5% of the EAD, one year after default, at the time of repossession of the collateral, or at the time of selling off the debt to a collection agency. On average, many financial institutions have workout periods of two to three years. Handling Incomplete Workouts We already briefly touched upon the issue of incomplete workouts. Incomplete workouts represent obligors that went into default status, and for which the workout process is still ongoing. Some regulatory authorities have provided some further input about incomplete workouts. The Committee of European Banking Supervisors (CEBS), which is the predecessor of the European Banking Authority (EBA), initially mentioned in their regulation:</p>
<p>“Institutions should incorporate the results of incomplete workouts as data/information into their LGD estimates, unless they can demonstrate that the incomplete workouts are not relevant.”</p>
<p>Part of this was copied by the Prudential Regulation Authority of the United Kingdom as follows:</p>
<p>“In order to ensure that estimates of LGDs take into account the most up-to-date experience, we would expect firms to take account of data in respect of relevant incomplete workouts ([in other words,] defaulted exposures for which the recovery process is still in progress, with the result that the final realized losses in respect of those exposures are not yet certain).”</p>
<p>The most recent EU regulation does not mention anything further about incomplete workouts. Incomplete workouts can be treated in various ways. A first treatment option is to calculate the current LGD of an incomplete workout and use this as the final LGD in the data set. This is a very conservative approach giving an upward bias to the LGDs. In other words, using this approach, the LGDs will be overestimated since additional future recoveries are likely. Note, however, that I have seen some banks systematically disregard recoveries after three or five years into their LGD calculations.</p>
<p>Another option is by using expert or predictive models, which estimate the final LGD of an incomplete workout based upon various characteristics, such as date of default, percentage already collected, time of collection, etc. The most easy method is simply to ignore incomplete workouts and only include complete workouts in the LGD modeling data set. This approach is quite commonly used in practice. Finally, you can also use survival analysis, whereby the loss amount is considered as a censored variable. Note, however, that this is very theoretical and not commonly applied in industry. For more information about this, we refer to the paper of Stoyanov, of which the citation is mentioned. Defining the Discount Rate We have already mentioned that LGD represents economic loss. Consequently, when quantifying the LGD, you should also take into account the time value of the money. One dollar today is worth more than one dollar tomorrow. Hence, as you may remember from your courses on investment theory, we should apply discounting. A key problem when applying discounting is setting the discount rate. Also, here within the context of LGD modeling, this is not that easy. Various choices are available. Let’s briefly discuss some of them. A first option is the contract rate at time of default. Another option is to use the risk-free rate with a risk premium added. The contract rate is quite commonly used, although it has been criticized. An expert group from the FSA (the Financial Services Authority), which was the predecessor for the PRA in the UK previously mentioned:</p>
<p>“The expert group agrees that the use of the contract rate as the discount rate is conceptually inappropriate. The group proposes that the discount rate for this asset class should be close to the risk-free rate, so long as firms can evidence and justify sufficient conservatism in their estimation of the downturn. One potential approach to a discount rate for this asset class could be the risk-free rate plus an appropriate premium.”</p>
<p>Building upon this, the Prudential Regulation Authority of the United Kingdom stated the following:</p>
<p>“The PRA expects firms to ensure that no discount rate used to estimate LGD is less than 9%.”</p>
<p>I have seen some banks adopting overly theoretical approaches to set the discount factor, based upon, for example, the capital asset pricing model. My practical advice would be to keep it more simple as follows: Use the contract rate to start off with and perform a sensitivity analysis of the LGD with respect to the discount factor. In case the LGD would be highly sensitive to the discount factor, an extra margin can be added based upon expert input, for example. Note that the issue of setting the discount factor is the subject of ongoing debate between supervisors and firms. Handling LGDs Outside the Normal Range Typically, any real-life LGD data set will contain negative LGDs and LGDs exceeding 100%. An obvious question is: “Where do these extreme values come from and how should they be treated?” A negative LGD is the same as a recovery rate exceeding 100%. There could be various reasons for this. One example is that the EAD was measured at the time of default, and the claim on the borrower increased after that because of fines or fees, and everything was recovered. In other words, the amount recovered was higher than the EAD, hereby giving a recovery rate of bigger than 100% or a negative LGD. Another reason for a negative LGD could be a gain in collateral sales. Negative LGDs should be capped at zero as also illustrated by this PRA article:</p>
<p>“The PRA expects firms to ensure that no LGD estimate is less than zero.”</p>
<p>Vice versa, LGDs exceeding 100% correspond to negative recovery rates. Also, here there could be various reasons for this. A first one is that additional recovery costs were incurred, whereas nothing was recovered. Alternatively, it could have been that additional drawings after default were considered as LGD, hereby seriously increasing the costs. Also, here it is recommended to cap LGDs exceeding 100% at 100%. Including Indirect Costs Remember, we already mentioned that LGD represents economic loss. Hence, also indirect costs should be properly taken into account. Here you can see some quotes from regulatory documents to illustrate this:</p>
<p>“The definition of loss used in estimating LGD is economic loss…This must include material discount effects and material direct and indirect costs associated with collecting on the exposure.</p>
<p>“Work-out and collection costs should include the costs of running the institution’s collection and work-out department, the costs of outsourced services, and an appropriate percentage of other ongoing costs, such as corporate overhead.</p>
<p>“Cost data comprise the material direct and indirect costs associated with workouts and collections.</p>
<p>“Material indirect costs, costs of running the collection and workout department, costs of outsourced services, appropriate percentage of overhead, must be included.”</p>
<p>The question now is how to take into account these indirect costs. Obviously, indirect costs are not tracked on a defaulter-by-defaulter basis, so need to be calculated on an aggregated level. Most banks conduct a small accounting exercise to calculate the indirect cost rate. Here you can see an example of this. Suppose we have four years of data from 2010 to 2013. The second column represents the total exposure at default of files in workout measured at the end of the year. Note that, because the workout period usually lasts longer than a year, most of these numbers include double counts. In other words, in the number 1500 measured in 2011, there are some observations, which were also already included in the 1000 measured in 2010. The third column represents the amount recovered in each year. There are no double counts here. Finally, the last column represents the aggregated internal workout costs per year. This includes the costs of the workout department, the salaries of the people working in it, the electricity, the computer hardware and software, etc.</p>
<p>We can now calculate two cost rates. The first one uses the exposure at default as the denominator. The assumption here is that higher workout costs are incurred for higher exposures at default. The cost rate can now be calculated in a time-weighted or pooled way. The time-weighted cost rate is just the average for all years of the workout costs divided by the exposure at default. For our example, this becomes 1.8%. The pooled cost rate divides the sum of all workout costs by the sum of all exposure values. In our case, this becomes 1.91%. A disadvantage when using this cost rate is that it has to be multiplied by the number of years the workout lasted.</p>
<p>Another way of calculating the cost rate is by using the amount recovered as the denominator. The assumption here is that higher workout costs are incurred for higher recoveries. Again, the cost rate can be calculated in a time-weighted or pooled way. The time-weighted cost rate is the average for all years of the workout costs divided by the recovery amounts. For our example, this becomes 6.5%. The pooled cost rate divides the sum of all workout costs by the sum of all recovered amounts. In our case, this becomes 6.49%. The advantage of this approach is that it is independent of the length of the workout period because each amount was recovered during one year only. Hence, this is simpler to implement.</p>
<p>Here you can see both cost rates applied to the calculation of LGD. Let’s first define the recovery rate as the net actualized cash flow (NCF), divided by the EAD. The NCF can then be calculated by discounting all the cash flows using a discount factor i. In the first option, the cost rate is multiplied by the length of the workout period and the EAD, as you can see. In the second option, the cost rate can be directly used to correct the collected cash flows. Identifying the Drivers of LGD Let’s now have a look at the drivers of LGD. In other words, the variables that can be used to model LGD. Four types of characteristics to be included are obligor characteristics, loan characteristics, country-specific regulations, and macroeconomic factors.</p>
<p>First of all, obligor characteristics can be considered. Given the correlation often observed between default and loss rates, it is advised to include a measure of creditworthiness in the LGD data set. Examples are a PD, a credit rating, an application or behavioral score, a bureau score, trends in the creditworthiness, rating changes, delinquency history, etc. In case of retail obligors, you can also include socio-demographic variables, such as marital status, gender (if allowed by the regulation), salary, time at address, time at job, etc. Also, variables measuring the intensity of the bank relationship such as number of years client, number of products with the bank, can be considered. In case of corporates, you can include industry sector, sector indicators, size of the company, legal form of the company, age of the company, balance sheet information such as revenues, total assets, solvency, profitability, liquidity ratios, etc.</p>
<p>A next important set of characteristics for LGD modeling are loan or facility characteristics. Usually, collateralized loans have a lower LGD than non-collateralized loans. So, it also important to include variables such as type of the collateral. Popular examples here are real estate, cash, inventories, guarantees, etc. It is highly advised to qualify the collateral as precisely as possible. So, in case of real estate, you can further categorize the collateral into, for example, flat, apartment, villa, detached/semi-detached house. It speaks for itself that also the value of the collateral needs to be properly taken into account.</p>
<p>An important variable here is the loan-to-value (LTV) ratio, which is the ratio of the value of the loan, or outstanding exposure, to the value of the underlying collateral asset. To calculate this, the current market value of the collateral is determined. In case of real estate collateral, this can be computed as follows: the current house price index divided by the house price index at the start of the loan times the value of the collateral at the start of the loan. A haircut can then be applied to the obtained market value to represent any additional costs and the forced sale. Historical haircuts can be calculated by dividing the forced sales price by the computed market value. Obviously, the loan-to-value ratio will vary through time and needs to be tracked as such.</p>
<p>The EAD itself can also be used as a predictor for LGD. In case of bonds, the debt seniority structure can be included. Bonds can be qualified as senior secured, senior unsecured, senior subordinated, subordinated, and junior subordinated. The seniority refers to the order of repayment in case a bankruptcy occurs. Upon bankruptcy, the first ones to get their money back are the regular loan investors, followed by the senior secured bondholders, then the senior unsecured bondholders, and so on, as depicted in the figure on the right. The last ones in line are the shareholders. Hence, the LGD goes up as you go down this seniority scale.</p>
<p>You can opt to either measure the seniority in absolute sense or in relative sense. The relative debt seniority of a bond can be calculated by the ratio of the debt amount above the seniority of the bond divided by the total debt, or the debt amount below the seniority of the bond divided by the total debt. Finally, also the remaining maturity can be considered as a variable to be included.</p>
<p>Country-specific regulations are also important to take into account. You can here think of local regulation pertaining to the management and handling of defaulters, the bankruptcy procedures that are available, the guidelines for materializing the collateral, and selling off bad debt to collection agencies, for example. Also, the creditor friendliness of the country-specific bankruptcy regulation might have an influence. All of these can be captured by adding variables to the LGD data set.</p>
<p>Macroeconomic factors will obviously also have an impact on LGD. Here you can think of gross domestic product (GDP), which reflects economic growth, aggregated default rates, inflation, unemployment rate, and the interest rates, which obviously will have an impact on the discount factor. Data Preprocessing for LDG Modeling We have now finished the construction of our LGD data set. Just as with PD, our LGD data set needs to be preprocessed to take care of any inconsistencies. There could be missing values, for which you may adopt any of the procedures discussed before (keep, delete, or impute). There could also be outliers, which need to be detected first using, for example, z-scores, histograms, box plots, and then treated using, for example, truncation procedures. Coarse classification or categorization could be useful for categorical variables with many values such as industry sector. Both decision trees and chi-square analysis can be used for this purpose.</p>
<p>Text Version Collapse Print Level 1: Creating an LGD Model</p>
<p>Introduction to Level 1: Creating an LGD Model Once our LGD data set has been preprocessed, we can start creating LGD models. A first key distinction with PD is that our dependent variable is not categorical, but continuous between 0% and 100%. The purpose of our LGD model at Level 1 of the credit risk model architecture is to come up with a model, which can provide a good ordinal ranking of the exposures in terms of their loss risk. In other words, the models should assign high scores to the high-loss exposures and low scores to the low-loss exposures.</p>
<p>In what follows, we will discuss various methods that have been used for LGD modeling in the industry. We will start by covering one-stage models. A first example of these are segmentation-based models, such as expert-based segmentation based on business experience and statistical segmentation based on regression trees. Also, regression methods, such as linear regression, linear regression with a beta transformation, logistic regression, and cumulative logistic regression, can be used for LGD modeling. Finally, given the intrinsic difficulties of LGD modeling, as we will discuss later, two-stage models have been introduced to decompose the LGD modeling into various subcomponents. Expert-Based Segmentation for LGD Modeling A first approach toward segmentation is expert-based segmentation. This means that one or more credit business experts decides upon the segmentation criteria to be used. Popular examples are debt type, seniority class (such as senior versus subordinated), collateral type (such as secured versus unsecured), loan purpose, or business segment. Once these segmentation criteria have been defined, historical averages will be computed to estimate the LGD. Simple table lookup procedures can then be adopted to find the LGD for an exposure in a particular segment.</p>
<p>Here you can see an example of an expert-based segmentation scheme. The segmentation criteria are collateral type such as real estate, cash, inventories, and other, and the loan-to-value ratio, which is categorized as high when it’s bigger than 80%, medium when it’s between 50% and 80%, and low when it’s smaller than 50%. The cells then contain the average historically observed LGDs. Note that you can easily add a standard deviation and confidence interval if desired. Although expert-based segmentation schemes are very user-friendly and intuitive, you will typically observe a wide variability of loss rates per segment. Hence, to explain the LGD variability in a more optimal way, statistical estimation techniques should be adopted. Statistical Segmentation for LGD Modeling: Regression Trees Let’s take a look at statistical segmentation for LGD modeling using regression trees. Here you can see an example of a regression tree. It starts from the root node, which determines the type of collateral. If the collateral is None, then the loss or LGD is estimated to be 72%. If the collateral is Cash, then it needs to be verified whether the client is known to the bank or not. If Yes, then the loss becomes 43%; if No, the loss becomes 55%. This is very similar to classification trees we discussed earlier for PD modeling. The only difference is that the terminal or leaf nodes of the tree do not contain Good-Bad classes, but continuous loss rates instead. Just as with classification trees, three decisions need to be answered when building a regression tree: the splitting, the stopping, and the assignment decision.</p>
<p>The splitting decision aims at minimizing the impurity. However, since the target variable is continuous, the impurity needs to be measured in another way. Two popular impurity measures are the mean squared error (MSE) and analysis of variance (ANOVA) with a corresponding F test.</p>
<p>A first way of quantifying impurity is by looking at the mean squared error or variance as it is referred to in SAS E [Enterprise] Miner. This is the average squared deviation from the mean. A pure node is a node having a low variance; in other words, with all values close to the mean. Vice versa, an impure node will have a high mean squared error or variance.</p>
<p>Another way of deciding upon the split is by using an analysis of variance (ANOVA) with accompanying F test. A good split splits a node into subnodes, whereby the variability within each subnode is as small as possible, whereas the variability between subnodes is as high as possible. This is perfectly embodied into the F statistic, which you can see here. The sum of squared between, SSbetween, calculates the variability between the subnodes. The sum of squared within, SSwithin, calculates the variability within the subnodes. The F statistic then also takes into account the number of observations, N, and the number of branches, B, of the split such that it is distributed according to an F distribution with degrees of freedom equal to N−B and B−1, respectively. You can then also compute the p-value. A good split will thus have a high F statistic and low corresponding p-value.</p>
<p>The stopping decision can be made in a similar way as for classification trees. You split the data into a training and validation set. The training set is used to make the splitting decision. The validation set is used to make the stopping decision. The latter can be done by making a two-dimensional plot of the error on both the training and validation set as the tree is grown. This error needs to be a regression-based error, such as mean squared error, for example. The stopping decision can then be made where the validation set error starts to increase.</p>
<p>Just as with classification trees, also for regression trees, the assignment decision is easy to make. You can just calculate the average of the loss rates within a leaf node and assign that value to the leaf node. Also, a standard deviation and confidence interval can be calculated, if desired.</p>
<p>Here you can see an example regression tree from a paper by Zhang and Thomas, published in 2012. First, the tree splits on loan amount, then on mortgage (yes or no), and then on residential status. The tree reports the average recovery rate and number of observations in each node. The tree splits the population into four segments. It can be seen that large amount loans have lower recovery rates than small amount loans. Also, as expected, obligors with a mortgage have higher recovery rates than those without a mortgage. Finally, house owners or those living with parents have higher recovery rates than tenants or those with some other residential status. Linear Regression for LGD Modeling Also linear regression can be used to model LGD. The idea here is to estimate a linear relationship between LGD and a set of variables using ordinary least squares (OLS) regression. You can see an example here of an LGD model reported by Loterman et al. in 2014. The linear model can be interpreted as follows: The baseline LGD is 74% and decreases with 15% when the loan is senior unsecured, or increases with 18% when the loan is junior subordinated. Additionally, the LGD increases with the US default rate from the previous year with a speed of 2% per unit. You can see that this is a very transparent and easy-to-understand model.</p>
<p>When using linear regression for LGD modeling, various problems arise. First, the linear regression output is not bounded between 0 and 1. Just as with logistic regression, this can be easily remedied by using a sigmoid transformation as illustrated. Another, more complex problem is that the recovery rate does not follow a normal distribution, but usually has a skewed or bimodal distribution. This is a problem, since regression assumes the errors follow a normal distribution. This can be solved by fitting a beta distribution, as we will discuss in what follows.</p>
<p>Here you can see the characteristics of six LGD data sets as they were used in a benchmarking study by Loterman et al. in 2012. The data sets cover personal loans, mortgages, revolving credits, and corporate loans. They are all quite big in terms of observations, ranging from 3351 to 119211 observations. Let’s have a look at the LGD distributions for each of these data sets.</p>
<p>Here you can see the distributions for each of the six data sets discussed earlier. None of the distributions looks normal or even symmetric. In fact, most of them are either single or double peaked. This presents some serious challenges for building good linear regression models.</p>
<p>To model a unimodal or bimodal LGD distribution, Moody’s introduced the idea of fitting a beta distribution on the LGD variable. Beta distributions are very flexible and versatile, and can fit a whole range of different empirical distributions, as you can see displayed here: U-shaped, skewed right, bell-shaped, and skewed left. If the LGD distribution is unimodal, one beta distribution usually suffices. For a bimodal LGD distribution, two or more beta distributions may need to be fitted in a mixture model.</p>
<p>Here you can see the beta distribution defined. It is bounded between 0 and 1. Just as a normal distribution has parameters mu and sigma, a beta distribution has parameters alpha and beta. Both parameters allow to model a wide variety of distributions. They can be estimated using a maximum likelihood procedure or by using the method of moments. The latter works as follows: The mean, μ, of a beta distribution equals α divided by α + β. The standard deviation is then equal to the square root of the ratio of α times β and (α + β squared) times (1 + α + β). From both these expressions, you can now back out α and β using some straightforward mathematical operations. This will result into α equal to (μ squared times (1−μ) divided by σ squared) −μ, and β equal to α times (1 divided by μ−1).</p>
<p>Here you can see the method of moments for estimating α and β illustrated. Suppose we have a data set with defaulted exposures, predictor variables such as collateral and LTV, and the target LGD variable. You can then calculate the average of the LGD, which is 44% in our case, and its standard deviation, which is 8% in our case. Both can then be plugged into the earlier derived formulas, hereby yielding α equal to 16.94 and β equal to 21.56. The beta distribution has now been parameterized.</p>
<p>Suppose now that we fitted a beta distribution on our LGD target variable. We can then use this beta distribution to transform our LGD variable to a new variable, which more closely follows a normal distribution. This new variable can then be used as the target in an ordinary least squares regression. This idea was first introduced by Moody’s in its Analytics LossCalc tool. It works as follows: We start from the cumulative beta distribution, displayed on the left, and the cumulative standard normal distribution, displayed on the right. Remember, a standard normal distribution has a mean of 0 and a standard deviation of 1. We put our LGD value into the cumulative beta distribution. We then put the value obtained into the inverse cumulative standard normal distribution. This will then give us the transformed LGD value, which we can use as target in our linear regression. As already mentioned, this transformed LGD variable will now be more closely normally distributed, and as such, may give us a better performing linear regression model.</p>
<p>Here you can see a step-by-step outline of the method discussed. We start by estimating the α and β parameters of the beta distribution using the method of moments. This will give us the beta distribution together with its cumulative variant. The original LGDs can then be transformed using this cumulative beta distribution. The numbers obtained are then put into the inverse cumulative standard normal distribution, which will give us the values to be used in the linear regression. Once the linear regression has been estimated, it can be used for prediction. To get the original LGD predictions based on the regression, you need to perform all steps backwards. In other words, the predictions of the linear regression need to be put into the cumulative standard normal distribution first, followed by the inverse cumulative beta distribution. Note that this mapping procedure is very easy to implement in SAS, and requires only a few lines of code. Performing Beta Linear Regression In this demonstration, we will illustrate how the beta distribution can be used for LGD modeling.</p>
<p>We start by importing the tab-delimited data set LGD.txt into SAS using PROC IMPORT. Let’s run this statement and inspect the LGDdata in the work library. Note that this is an anonymized data set with various accounting ratio variables and a continuous LGD target variable. We start by inspecting the distribution of the LGD target variable using PROC UNIVARIATE. Here we can see that the distribution is rather skewed.</p>
<p>We now split the data into a training set for model development and a holdout set for model evaluation. Since we can assume that the data has already been randomized, we use the first 337 observations for training and the remaining 169 observations for testing. We run this statement and then look in the work library to confirm that both data sets have been successfully created.</p>
<p>We will now estimate a linear regression model on the training set using PROC REG. We run this statement and quickly inspect the output where we can see that the model is significant. Next, we calculate the LGD predictions on the holdout set using PROC SCORE. The predictions are stored in the preds data set. Let’s inspect the preds data set in the work library. Here you can see the predictions from the linear regression.</p>
<p>Using PROC CORR, we can calculate the correlation between the predictions and the target. We can see that it equals 0.75 and is significantly different from zero. Let’s now also calculate the mean squared error. We first create the data set MSEtemp to store the intermediate results needed to calculate the MSE. The MSE itself can then be calculated using PROC MEANS. Let’s run this statement. We can see that the MSE is equal to about 0.015.</p>
<p>We will now fit a beta distribution to transform the target variable. First, we start by calculating the alpha and beta parameters using the method of moments. We use PROC MEANS to calculate the mean and variance of the LGD. Here you can see the mean and standard deviation of the LGD target variable. The data set betaparams then calculates the alpha and beta using the formula discussed in the course. Let’s also inspect this data set. Here you can see the alpha and beta values.</p>
<p>The next step is then to transform the LGD to a standard normally distributed variable. This is done in the transformedtraining data set using the CDF and probit function. Let’s run this statement and inspect the data set. Here you can see the transformed LGD target variable.</p>
<p>Let’s now also visualize its distribution using PROC UNIVARIATE. We can see that this distribution is more symmetric and less skewed than the original LGD distribution.</p>
<p>We now estimate a linear regression model on the transformed training data using PROC REG. A quick inspection of the output reveals that the model is significant. We also score the holdout observations with this new model and store the predictions in the transpreds data set. Let’s run this statement and inspect the transpreds data set in the work library. Here you can see the predictions.</p>
<p>As discussed in the course, we will now transform the predictions back to the original space using the CDF and Betainv function. Let’s now also calculate the performance. We start by calculating the correlation using PROC CORR. We can see that the correlation is about 80% and thus higher than in the standard linear regression setting, which we discussed in the first part of this demonstration.</p>
<p>Let’s also calculate the mean squared error. We run these two statements. We can see that also the mean squared error decreased to about 0.013. Hence, for this LGD data set, it was beneficial to transform the target variable using a beta distribution.</p>
<p>Demo Code: proc import out= work.lgddata datafile= “N:4C13_items.txt&quot; dbms=tab replace; getnames=yes; datarow=2; run;</p>
<p>proc univariate data=LGDdata; var LGD; histogram; run;</p>
<p>data training holdout; set lgddata; if 1 &lt; = <em>N</em> &lt; = 337 then output training; else output holdout; run;</p>
<p>proc reg data=training outest=outests; LGDOLS: model LGD= ratio1-ratio13; run;</p>
<p>proc score data=holdout score=outests out=preds type=parms; var ratio1-ratio13; run;</p>
<p>proc corr data=preds; var LGD LGDOLS; run;</p>
<p>data MSEtemp; set preds; MSEterm=(LGD-LGDOLS)**2; run;</p>
<p>proc means data=MSEtemp; var MSEterm; run;</p>
<p>proc means data=training; var LGD; output out=meanstats mean=mu var=sigmasq; run;</p>
<p>data betaparams; set meanstats; alpha=(mu<em>mu</em>(1-mu)/sigmasq)-mu; beta=alpha*(1/mu -1); run;</p>
<p>data transformedtraining; if <em>N</em>=1 then set betaparams; set training; newLGD=probit(cdf(‘BETA’,LGD,alpha,beta)); run;</p>
<p>proc univariate data=transformedtraining; var newLGD; histogram; run;</p>
<p>proc reg data=transformedtraining outest=outests2; LGDBETA: model newLGD= ratio1-ratio13; run;</p>
<p>proc score data=holdout score=outests2 out=transpreds type=parms; var ratio1-ratio13; run;</p>
<p>data preds2; if <em>N</em>=1 then set betaparams; set transpreds; LGDpred=betainv(cdf(‘NORMAL’,LGDBETA),alpha,beta); run;</p>
<p>proc corr data=preds2; var LGD LGDpred; run;</p>
<p>data MSEtemp2; set preds2; MSEterm=(LGD-LGDpred)**2; run;</p>
<p>proc means data=MSEtemp2; var MSEterm; run; Handling Problems with Beta Regression for LGD Modeling A problem with using the beta distribution is that it cannot cope with LGD values equal to 0 or 1. As already mentioned, these cases typically occur in real-life LGD data sets. A first option is to use a scaling factor ε and make sure the LGD ranges between 0.01 and 0.99, for example. An alternative is to use a zero-one-inflated beta regression. The idea here is to decompose the LGD model into three parts: two probabilities, π0 and π1, representing the probability that LGD equals 0 or 1 respectively, and a beta distribution for the intermediate values. Both probabilities π0 and π1 can then be regressed on a set of predictor variables using a logistic regression. The mean and standard deviation of the beta distribution can then also be regressed on a set of predictor variables.</p>
<p>The regression parameters can then be determined using maximum likelihood optimization. The model can also include interaction and nonlinear effects using splines, for example. A key advantage is that different variables can pop up as significant in the various models, hereby giving maximum flexibility and also model performance. A disadvantage, however, is that four models are obtained, which increases the complexity of the approach. This model is more extensively discussed by Swearing, Castro, and Bursac, and Tong, Mues, and Thomas, in the references cited. Logistic Regression for LGD Modeling Another way to model LGD is by mapping it into a classification problem and build a logistic regression model. This is the approach followed by Van Berkel and Siddiqi in the reference cited. In the first step, the continuous LGD variable is transformed into a binary variable with weights. In our example, the observation with LGD equals 25% is duplicated into a bad observation with weight 25% and a good observation with weight 75%. Either the classification algorithm can directly cope with weights, or the observations can be replicated to account for the weights.</p>
<p>Just as with an application or behavioral scorecard, you can then do a weights-of-evidence analysis and select variables using the information value. The remaining variables can then be input into a logistic regression model, which can be represented in a scorecard format, as we discussed earlier in the part on PD modeling.</p>
<p>Here, you can see an example of this. You can see that every variable has been coarse-classified into various categories. Every category has a score assigned to it. The Bad rate then represents the LGD. A key advantage of this method is that it results into a very user-friendly, easy-to-understand LGD scorecard. Besides binary logistic regression, you can also use cumulative logistic regression to model LGD using ordinal classification. The idea here is to categorize the LGD into ranges (for example, 0% to 5%, 5% to 10%, 10% to 15%, etc.). Alternatively, similar to the shadow-rating approach for PD, you can also make use of recovery ratings as they have been provided by the rating agencies. A next step is then to build a cumulative logistic regression model, as we discussed earlier for PD modeling. Remember that a cumulative logistic regression model estimates cumulative probabilities and makes use of a category-specific intercept and common set of predictors. Two-Stage LGD Models Two-stage models have also been very popular for LGD modeling. A first approach could be to categorize the LGD distribution and then estimate a follow-up model for all or some of the categories identified. Here you see that the LGD has been categorized into three classes: LGD ≤ 0 (for which the prediction becomes 0), LGD between 0 and 0.4 (for which the prediction is set to 0.2), and LGD above 0.4 (for which a second LGD model is estimated). The categorization can be obtained by either expert-based input or by building a regression tree.</p>
<p>Another commonly used approach is to build two models: a classification model predicting cures and a regression model predicting the LGD, given that an account has not cured. In other words, the LGD is calculated as follows: the probability of cure times a fixed handling cost plus one minus the probability of cure multiplied by the LGD of the workout procedure. The probability of cure can then be obtained using a classification model such as a logistic regression or a decision tree, for example. The LGD workout, sometimes also referred to as loss given loss, can then be the result of a regression model, such as regression trees, or linear regression. Once this model has been estimated, it can also easily be used to do stress testing. One simple stress scenario could correspond to stressing the cure rates, for example.</p>
<p>Here you can see another example of a two-stage model for mortgages as it was developed by Leow and Mues in the reference cited. Once an obligor has gone into default (180 consecutive days of missed payments in our case), a first model will be a classification model estimating the probability of repossessing the collateral. In case of repossession, a second model is estimated to determine the price at which the collateral can be sold. Both models can then be combined to estimate the final loss. Advanced LGD Models Besides the techniques we already discussed, also more complex techniques, such as neural networks or support vector machines, can be used for LGD modeling. Although these techniques benefit from a universal approximation property and are thus very powerful, they usually suffer from a loss of interpretability. The resulting models are very complex to understand by the human decision maker. Hence, it is not recommended to use these techniques for LGD modeling. Performance Measures for LGD Models Once the LGD model has been obtained, its performance needs to be quantified. Various performance measures can be used for this purpose. Let’s define yi as the actual or target LGD, y hati as the estimated LGD, and Y bar as the average LGD.</p>
<p>A first, commonly used performance diagnostic in regression is the R square. This represents the variability explained by the model versus the variability observed. It typically ranges between 0% and 100%. Obviously, the R square should be as high as possible. Based upon the benchmarking study by Loterman et al, as we already referred to earlier, we found that the R square usually ranges between 20% and 30%. This is relatively low and a clear indication that LGD is hard to predict. Two alternative measures are the mean squared error and the mean absolute deviation. They are defined as follows: Mean squared error is the sum of the squared deviations divided by n, with n the number of observations. The mean absolute deviation is the sum of the absolute deviations divided by n. Also, a scatter plot can be used to visualize the performance. The idea here is to contrast the predicted LGD versus the actual LGD. Ideally, all observations should be close to the diagonal. In a real-life LGD modeling exercise, the graph will unfortunately not look as nice as it does right here. The strength of the relationship can then be further quantified into a Pearson correlation as illustrated.</p>
<p>Also, a CAP curve can be used to measure the performance of an LGD model. Here you can see an example of a CAP curve for application or behavioral scoring as we discussed it before for PD modeling. Let’s briefly refresh the idea. On the X axis, the obligors are sorted from low score to high score. On the Y axis, the fraction of defaulters is depicted. The perfect model increases linearly up to the portfolio default rate and remains at 100% after that. The random model is then represented by the diagonal. The current model lies in between the random and perfect models, preferably as close as possible to the perfect model. The CAP curve can be summarized into an accuracy ratio or Gini coefficient. This can be calculated as follows: the area below the CAP curve for the current model minus the area below the CAP curve for the random model divided by the area below the CAP curve for the perfect model minus the area below the CAP curve for the random model. A perfect model will thus have an accuracy ratio of 1, and a random model an accuracy ratio of 0.</p>
<p>Suppose that we now want to use the CAP curve for LGD modeling. The problem here is that our target variable LGD is not dichotomous as was the case for PD. Hence, we need to find another way of coming up with a binary variable for the Y axis.</p>
<p>A first way of doing this is by categorizing the defaulted exposures according to whether their actual LGD is higher than the long-term average LGD. You can then think of the Goods as the ones who have an actual LGD lower than the historical average, and the Bads as the ones who have an actual LGD higher than the historical average. The X axis then represents the defaulted exposures sorted from high to low LGD score as it comes out of the model. The Y axis then represents the fraction of losses higher than the average, or in other words, the Bads.</p>
<p>If the binary outcome on the Y axis represents whether the actual LGD is higher than the long-term average LGD, then the CAP curve and corresponding accuracy ratio indicate how much better the LGD model predicts than a long-term average. In other words, it illustrates the performance in the center of the LGD distribution. Also other ways of creating a binary variable on the Y axis can be considered. A first example is a binary outcome representing whether the actual LGD is higher than the long-term 75 percentile of the LGD distribution. The corresponding CAP curve and accuracy ratio will then indicate how much better the LGD model allows to predict high LGDs. It thus illustrates the performance on the right tail of the LGD distribution. Finally, the binary outcome can also represent whether the actual LGD is higher than the long-term 25 percentile of the LGD distribution. The corresponding CAP curve and accuracy ratio will now indicate how much better the model allows to predict low LGDs, hereby illustrating the performance on the left tail of the distribution.</p>
<p>The performance of the estimated LGD models can also be evaluated by looking at some efficiency measures. Interesting to consider are confidence intervals for the predicted LGDs. A confidence interval is a measure of reliability. It is an interval which frequently includes the parameter of interest (in this case, LGD), when considering multiple samples. How frequently it includes the LGD parameter is determined by the confidence level. Popular values here are 90, 95, and 99%. The width of a confidence interval provides information about the precision and efficiency of the estimate. Narrower confidence intervals are preferred, because that gives more certainty about the LGD estimates and thus also about the required capital to protect against losses. The reliability can be measured as the number of times the actual losses fall outside the confidence interval on an independent test set.</p>
<p>Text Version Collapse Print Level 2: Defining Ratings and Calibrating an LGD Model</p>
<p>Introduction to Level 2: Defining Ratings and Calibrating an LGD Model Once the LGD model has been estimated at Level 1, we can move on to Level 2 of the credit risk model architecture. This is a crucial level as it will output the LGD numbers that will be directly used in the Basel Capital requirement formula. Remember, the purpose here is to define the LGD ratings and do the calibration. Various issues need to be considered here, and will be covered in what follows. Defining LGD Ratings A first activity, which needs to be undertaken here at Level 2, is to map the outcome of the segmentation, regression, or two-stage LGD model to a set of LGD facility ratings. An LGD rating groups defaulted exposures in terms of similar loss risk. The reason why LGD ratings are needed is similar as to what we discussed for PD ratings. When working directly with the LGD scores of Level 1, a slight change in exposure characteristics might give a different LGD score and hence different capital. This would make the capital very volatile, which is clearly undesirable. Ratings, on the other hand, provide an ordinal and more stable measure of loss risk, hereby also giving a more stable capital. Every LGD rating should be accompanied by a cardinal measure of loss risk. Hence, based upon historical data, an LGD number needs to be calibrated. Various requirements need to be satisfied when calibrating the LGD, as we will discuss in what follows. Before we continue the discussion on defining the ratings and calibrating the LGD, let’s see what the Basel Accord has to say about it. For corporate, sovereign, and bank exposures, it says:</p>
<p>“Estimates of LGD must be based on a minimum data observation period that should ideally cover at least one complete economic cycle but must in any case be no shorter than a period of seven years for at least one source.”</p>
<p>Hence, we need at least seven years of data for at least one source. Note that, as mentioned earlier, this does not imply that we should attach equal weight to each of those years. So, if we can prove that older data is less relevant, then we can assign a lower weight to it. Another paragraph reads:</p>
<p>“There is no specific number of facility grades for banks using the advanced approach for estimating LGD.”</p>
<p>This is contrary to PD, where for corporate, sovereign, and bank exposures, a minimum of seven ratings was needed for the non-defaulters and one for the defaulters. For retail exposures, the Accord says:</p>
<p>“The minimum data observation period for LGD estimates for retail exposures is five years.”</p>
<p>Again, a lower weight can be assigned to older data, if deemed necessary.</p>
<p>Traditionally, credit risk models only included a default dimension. By defining LGD ratings, a second dimension can be added to the picture. By visualizing exposures in this bivariate plot, you can easily check whether there is any correlation between a PD and an LGD rating. It will then also become possible to trace the PD and LGD rating migrations over time. The methods to define LGD ratings are actually quite similar to the methods to define PD ratings: map onto an agency rating scale, regression trees, and directly optimizing an objective function. Let’s discuss each of these into more detail.</p>
<p>Next to default ratings, rating agencies also provide LGD or loss ratings. Here you can see the rating scales adopted. Each rating agency defines six LGD ratings. Note the similarity between the Moody’s and Fitch scales. These rating scales can now be used as a reference to define the internal rating scale. Although a perfect match might be hard to obtain, a close enough approximation can be satisfactory. Note that the LGD rating scales provided by the rating agencies can also be useful for benchmarking purposes, as we will discuss later.</p>
<p>Also, decision trees can be used to define LGD ratings. Remember, since the target is continuous, you need to use regression trees here as discussed earlier. The monotonicity constraint, as already discussed for PD, needs to be respected here. The LGD should monotonically increase as the rating decreases. Finally, also an objective function can be optimized. Here you can see an example of this. The number of LGD ratings is fixed up front to K. For example, K can be 6, 8, or 10. The numerator then represents the difference of the average losses of adjacent LGD ratings, and the denominator the square root of the sum of the corresponding variances. Obviously, this objective function should be maximized. This can be done in a trial-and-error way using expert-based input, or by using optimization techniques such as genetic algorithms or simulated annealing. Calibration of LGD Models: Long-Run Default-Weighted Average LGD Let’s now have a closer look at the calibration of the LGD models. A very important Basel paragraph is paragraph 468 of the Basel II Accord. Let’s read it together.</p>
<p>“A bank must estimate an LGD for each facility that aims to reflect economic downturn conditions where necessary to capture the relevant risks. This LGD cannot be less than the long-run default-weighted average loss rate given default calculated based on the average economic loss of all observed defaults within the data source for that type of facility… a bank must take into account the potential for the LGD of the facility to be higher than the default-weighted average during a period when credit losses are substantially higher than average…this cyclical variability in loss severities may be important and banks will need to incorporate it into their LGD estimates… banks may use averages of loss severities observed during periods of high credit losses, forecasts based on appropriately conservative assumptions, or other similar method…using either internal or external data.”</p>
<p>In short, there are two key ideas presented here. The LGD should be higher than the long-run default-weighted average loss rate given default, and it should be an economic downturn LGD. We will discuss both into more detail in what follows. Let’s first discuss the various weighting methods available for averaging LGD.</p>
<p>Here you can see the various ways of calculating an average LGD. A time-weighted LGD first calculates the LGD for the individual years of observation and then averages those. A default-weighted LGD is calculated by dividing the total losses by the total number of defaults. An exposure-weighted LGD weighs each default by its EAD, whereas a default-count LGD assigns an equal weight to each default. Based upon this, an average LGD can be computed in four possible ways.</p>
<p>Here you can see the four possible average LGDs. A first option is default-weighted averaging using default counts. Each default has an equal weight and the defaults from all years are grouped into a single cohort. Option 2 is similar, except that each default is weighted according to the exposure at default. Option 3 is time-weighted averaging with default counts. Here, each default has equal weight within the annual cohort average. The final average is then calculated as the average of the annual averages. Finally, option 4 is similar to option 3, except that each default within an annual cohort is weighted by the exposure at default. To see the full table with the equations, click the Information button.</p>
<p>Here you can see an example of how the four LGDs can be calculated. Assume we have two years of data. In year 1, we have 20 defaults with an exposure of $40 and average loss rate of 10%. In year 2, we have 50 defaults with an exposure of $100 and an average loss rate of 90%. We also have 30 defaults with an exposure of $140 and an average loss rate of 60%. As you can see, year 1 is an upturn year, whereas year 2 is a downturn year with higher loss rates. Let’s now compute the four LGD averages. Remember, for option 1 (default-weighted averaging with default counts), the data from both years is pooled, so we have 20<em>10 + 50</em>90 + 30*60 divided by the total number of defaulters, which is 20 + 50 + 30 or 100, which gives us 67. Option 2 then works quite similar, but takes into account the exposure weights. This gives us 71, which is quite close to option 1.</p>
<p>Let’s now also consider time-weighted averaging. The idea here is to compute the average of years 1 and 2, and then average both. In option 3, each default has the same weight. This gives us 20<em>10 / 20 + 50</em>90 + 30<em>60 / 50 + 30, divided by 2, or 44. Note that this is a sharp decrease compared to the other two averages we already calculated. Option 4 then also takes into account the exposure weights, which gives us an LGD of 43%, which is again quite close to option 3. You can now clearly see the impact of time-weighted averaging. An upturn period, such as year 1, can compensate the effect of a downturn period, such as year 2. According to Basel, you can never do time-weighted averaging. In other words, the LGD should be bigger than the long-run default-weighted average loss rate given default, or thus 67% in our case. Calibration of LGD Models: Economic Downturn LGD Let’s now also have a look at the calibration of our LGD model. Remember, according to paragraph 468 discussed earlier, the LGD should be an economic downturn LGD. A first obvious question is: Why? Or, why do we need a downturn LGD and not a downturn PD? The answer to this question is provided by the Basel Capital requirement formula. Remember, this formula calculates the 99.9% worst-case PD, which we referred to as α</em> earlier. Let’s call it PDdownturn here. To be consistent, the capital should then become PDdownturn times LGDdownturn minus PDavg times LGDavg. The mapping of PDavg to PDdownturn is provided by the Capital requirements formula itself, which is based on the Vasicek model discussed earlier.</p>
<p>For LGD, no such mapping is available. The US regulators initially introduced the mapping function LGDdownturn=0.08 + 0.92*LGDavg, but this was abandoned later. Hence, because no other suitable alternative was available to map LGDavg to LGDdownturn, the regulators put the burden with the banks to come up with an LGDdownturn. Note that the above reasoning assumes a positive correlation between PD and LGD. In the unlikely event that there is no PD-LGD correlation, you can just use the LGDavg. However, in the more likely case of a positive PD-LGD correlation, the LGDdownturn needs to be used. This is also further discussed in Section 10 of the PRA document.</p>
<p>Here you can see an example of the default and loss rates for unsecured corporate bonds and loans during 1990 to 2011. The orange curve represents the yearly default rates, whereas the blue curve represents the yearly LGD rates. The average LGD is represented by the gray line and the downturn LGD by the black line, which is obviously above the gray line. By visual inspection of the graph, it can be clearly seen that there is a positive correlation between both. A handy way of coming up with a downturn LGD is by constructing the table as depicted here. Let’s assume we have K LGD ratings or segments. Assume period 1 is the year with the highest default rate, period 2, the two years with the highest default rate, etc. We can then calculate the average LGD for each of the cells in the table. If the averages reported within each of the columns are relatively similar, then it can be concluded that there is no PD-LGD correlation and no downturn calibration is needed. However, if the averages do vary substantially, then a correlation is present, and a downturn LGD can be obtained by only including the first 1, 2, or more rows.</p>
<p>Also, other methods can be used to calculate an economic downturn LGD. You could use the upper confidence level that comes with the LGD estimate from the distribution or regression model. Also, the mapping formula introduced by the US regulators is an option. Ideally, a downturn scenario should be defined and its impact on the LGD quantified. The Prudential Regulation Authority in the UK gave an example of this as follows:</p>
<p>“The PRA believes that an average reduction in property sales price of 40% from their peak price, prior to the market downturn, forms an appropriate reference point when assessing downturn LGD for UK mortgage portfolios. This reduction captures both a fall in the value of the property due to house price deflation as well as a distressed forced sale discount.”</p>
<p>You can also consider downturns in regional sub-portfolios and use that for extrapolation. Other scenarios that could be considered are a doubling of the default rate, an increase of the default rate with 0.05 to 0.10% for low default portfolios (as we will discuss later), or two quarters of negative GDP growth.</p>
<p>Also, a bootstrapping procedure can be used to come up with a downturn LGD. This works as follows: In step 1, create a bootstrap sample from the underlying LGD data within a specific rating or segment. Note that a bootstrap sample is a sample with replacement. The same observation can thus occur multiple times. In step 2, the mean LGD of the bootstrap is calculated. These two steps are then repeated multiple times, say 100,000 times, giving rise to the distribution of the mean LGD as depicted. The economic downturn LGD can then be set to the 90, 95, or 99% upper percentile of this distribution, in collaboration with the management.</p>
<p>Once the economic downturn LGD has been calculated, it will be used in the Basel Capital requirement formula to calculate the buffer capital. A question, which might arise now, is whether the economic downturn LGD should also be used for other business purposes and activities, such as pricing, limit setting, economic capital calculation, etc. Usually, this will not be the case. Other business activities will typically use the average LGD instead of the downturn LGD. Although this may be considered as a violation of the use test, as we will discuss later in the lesson on validation, many supervisors will actually tolerate this, provided the reasons for not using the economic downturn LGD are documented. Another important question is whether a downturn LGD and stressed LGD is the same thing. About this, the FSA, which is the predecessor of the PRA in the UK, mentioned earlier:</p>
<p>“When a firm assumes stress and downturn conditions that are similar, the LGD estimates used might also be similar.”</p>
<p>To conclude, a final issue concerns the exposures already in default. According to the Basel Accord, for defaulted exposures, the capital K equals the maximum of 0 and LGD minus ELbest, whereby the LGD is the loss forecast reflecting potential unexpected losses during the recovery period based on downturn conditions, and ELbest is the best estimate of expected loss given the current economic circumstances and the exposure status. ELbest can be the result of a regression model estimating the expected loss for an exposure given the current status of the collection process and the economy.</p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

