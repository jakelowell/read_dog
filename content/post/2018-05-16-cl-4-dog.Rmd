---
title: cl 4 dog
author: ''
date: '2018-05-16'
slug: cl-4-dog
categories: []
tags: []
---


Introduction
In this lesson, we focus on developing a credit scoring model in a PD context. We hereby assume that the data has been prepared and is now ready to be analyzed using a classification model. We will discuss how to develop classification models for both retail as well as corporate portfolios. 

The aim of classification is to discriminate between the good and bad obligors in terms of default behavior. In this lesson, we start by giving a clear explanation of the classification problem. This will be followed by a discussion of various techniques to tackle this problem. We will not focus on complex mathematical techniques, but rather study techniques that can be readily implemented for both retail as well as corporate credit scoring in a user-friendly and effective way.





 
Objectives

In this lesson, you learn to do the following:
•	describe the following methods of binary classification: linear regression, logistic regression, decision trees, and linear programming
•	describe the following methods of multiclass classification: cumulative logistic regression, decision trees, and one-versus-one and one-versus-all coding

Text Version	Collapse  Print
Fundamentals of Classification

Classification Problem
Let's first start by introducing the classification problem. In a classification context, you start from a set of observations, which are characterized by various predictor variables and one target variable. Typical for classification is that the target variable is categorical. In the example data set depicted, you can see that the target variable is binary: a customer can either be a good or a bad payer. The aim of building a classification model is then to relate the predictor variables Age, Income, Gender, and others to this categorical variable in a quantitative way using an analytical model. This model can then be used to assign an observation or customer with a given set of characteristics to a class from a predetermined set of classes. 

In the previous example, the target variable was binary, good or bad payer. The target variable can also have more than two values. Here you can see an example of a multiclass classification problem. The purpose is to predict the rating of a set of companies based upon predictor variables such as Return on Equity, Debt Ratio, etc. The target rating can have more than two values, ranging from AAA, the best rating, to D, the worst or default rating. Remember, as discussed in Lesson 1, in the shadow-rating approach, analytical models are built to predict credit ratings. As you can see now, these analytical models essentially solve a multiclass classification problem.
Classification Techniques
Various classification techniques have been developed, originating from a variety of different disciplines such as statistics, machine learning, artificial intelligence, pattern recognition, and data mining. The distinction between those disciplines is getting more and more blurred and is actually not that relevant. Hence, in this lesson, we will discuss classification techniques that have practical relevance and fulfill the needs to build successful retail and corporate credit scoring models. 

We will discuss classification techniques for both binary as well as multiclass classification. For binary classification, we will first discuss linear regression. Linear regression is a very popular classification technique, but it does have a couple of drawbacks. That will bring us to logistic regression. Logistic regression is undoubtedly one of the most popular statistical classification techniques used for both retail and corporate credit scoring in the industry. So I will extensively comment on logistic regression. We'll then also discuss decision trees. Decision trees are also quite popular, especially since they come up with very user-friendly, comprehensible, and transparent decision models. This will bring us to linear programming. Linear programming is, I would say, an outdated technique for credit scoring. It's kind of legacy, but every now and then, you might encounter credit scoring models that have been created using linear programming. So that's why we're going to spend some time discussing linear programming as well. 

For multiclass classification, we will start by talking about cumulative logistic regression. This is an extension of the binary logistic regression model to deal with an ordinal target variable such as a credit rating. This will be followed by a discussion of decision trees for multiclass targets. Finally, we will also discuss two coding schemes to map a multiclass classification problem to a set of binary classification problems: one-versus-one and one-versus-all coding.





 
Text Version	Collapse  Print
Binary Classification

Linear Regression
Let's first start by discussing linear regression for binary classification. Here you can see an example data set with predictor variables such as Age, Income, Gender, etc., and a binary target variable, which has values Good or Bad. Linear regression estimates a linear relationship between the target variable Y, representing the Good-Bad status, and the predictor variables. In order to do so, the target variable Y is represented in a numerical way: 0 if it's a bad customer and 1 if it's a good customer. The beta parameters are then typically estimated using the idea of ordinary least squares, OLS, whereby the parameters are estimated so as to minimize the sum of squared error terms. In SAS, this can be easily done using either PROC REG or PROC GLM. As part of the estimation output, you will then also get standard errors, confidence intervals, and p-values, which can be used to gauge the statistical significance of each of the predictor variables. A key advantage of linear regression is that it usually works well and is simple to interpret. However, it does have a few problems associated with it. 

A first problem is that there is no guarantee that the target variable will have values between 0 and 1. In a classification problem such as credit scoring, it would be handy to have a target variable between 0 and 1, because we could then interpret it as a probability. A second problem is that the target variable and errors are not normally distributed, but Bernoulli distributed, which is also not that nice. Note, however, that despite both problems, linear regression usually gives good performance for classification.
Overview of Logistic Regression
To solve the previously mentioned problems of linear regression for classification, let's start by introducing a bounding function. The idea of the bounding function is that it should be a function, which is always limited between 0 and 1. An example bounding function is shown here: f(z) = 1 divided by 1 + e to the power −z. Let’s study this bounding function into more detail. For z = to 0, it becomes 1 divided by 1 + e to the power 0, which is 1. So the function becomes 1 divided by 1 + 1, or a half. For z, a very big number, say 1000, it becomes 1 divided by 1 + e to the power −1000, which is very small, so the function goes towards 1. For z, a very small number, say −1000, it becomes 1 divided by 1 + e to the power minus −1000, which is very big, so the function goes to 0. In other words, for any value z, this function is always bounded between 0 and 1. Note that also other functions exist, which have this property, as we will discuss later. Let's now combine the idea of this bounding function with the idea of the linear regression as discussed earlier. 

When combining both the linear regression with the bounding function, one arrives at logistic regression, as you can see right here. You can see that the z in the bounding function has been replaced by the linear combination of Age, Income, Gender, etc., resulting in an outcome variable, which is always bounded between 0 and 1 and can thus be interpreted as the conditional probability that the customer will be a good payer given his/her characteristics such as Age, Income, gender, etc. Logistic regression is undoubtedly the most popular classification technique used for both retail as well as corporate credit scoring. It can be very easily estimated in SAS using PROC LOGISTIC. 

How does this work practically now? Well, you start from a historical data set with customer data. This data can then be used to estimate the beta parameters in the logistic regression model. Once the beta parameters have been estimated, the model can be used to score new observations. Now that we understand the logistic regression model formulation, let's play a bit with it. 

Let's say that our target variable is coded as Y, with Y equal to 1 representing a good payer and Y equal to 0 representing a bad payer. Our predictor variables are coded as X1, X2, etc., representing variables such as age, income, etc. 

The first line reformulates the logistic regression model by multiplying the numerator and denominator by the same amount, which is e to the power β0 + β1 times X1 + β2 times X2, and so on. 

We can now also calculate the probability of Y equaling 0, which is the probability of Bad, given X1, X2, etc. This probability is 1 minus the probability of being Good. It can be easily verified that both probabilities range between 0 and 1. 

Let's now express the model in terms of the odds. The odds are defined as the probability of Good divided by the probability of Bad. So, to give an example, if the probability of being Good equals 8 out of 10, then the probability of being Bad equals 2 out of 10, and the odds become 8 out of 10 divided by 2 out of 10 or 4. This implies that the probability of being Good is four times as high as the probability of being Bad. You can see that the logistic regression model is almost linear in the odds. To make it linear, we just have to apply the logarithmic transformation. 

Here you can see the logistic regression reformulated in terms of the log odds. You can clearly see that the logistic regression model is linear in terms of the log odds. This will have some important implications as we will discuss later. Note that the log odds is often referred to as the logit. 

Once the logistic regression model has been estimated, we can start doing all kinds of sensitivity analysis. The aim here is to analyze the impact of each of the predictor variables individually on the target variable. A first way to do this is by calculating the odds ratio. Let's say we want to see what happens if variable Xi (think of Age or Income for example) increases with one unit. We hereby assume that all other variables remain constant. Remember, this is the ceteris paribus assumption. Since the logit is linear, when increasing the value of Xi with one unit, the new logit is simply the old logit with the β coefficient of Xiadded. In terms of the odds, the new odds are then equal to the old odds, multiplied by e to the power βi. The latter is called the odds ratio. It is the multiplicative increase in the odds when Xi increases with one unit, assuming all other variables keep their values. So, in other words, when βi is positive, then the odds ratio is bigger than 1, and the odds and probability will increase with Xi. Vice versa, when βi is negative, then the odds ratio is smaller than 1, and the odds and probability will decrease with Xi. Another way to analyze the impact of an individual variable is by calculating the doubling amount. This is the amount of change that is needed for doubling the outcome odds. It can be easily verified that this amount is equal to the logarithm of 2 divided by the beta coefficient of the variable. 

The beta parameters of the logistic regression model are estimated using the maximum likelihood procedure. Remember, maximum likelihood optimization chooses the parameters in such a way so as to maximize the probability of getting the sample at hand. The first expression here shows the probability of observing either class. The small y variable is 1 for a good customer and 0 for a bad customer. These probabilities can then be multiplied for all observations to get the likelihood function for the entire data set. You can then take the logarithm of this likelihood function to facilitate the optimization procedure, and use the Newton-Raphson method to find the optimal beta parameters. 

Once the maximum likelihood estimates have been obtained, you can start doing hypothesis testing. The aim here is to compare the likelihood of the full model with the likelihood of the reduced model, assuming some null hypothesis H0. An example null hypothesis could state that one or more variables have coefficients of 0 and are thus not significant. If the likelihood of the full model, Lfull, is almost equal to the likelihood of the reduced model, Lreduced, then it is clear that you can accept the null hypothesis. If both are different, then the null hypothesis needs to be rejected. More formally, a chi-square test can be applied since −2 times the logarithm of the squared ratio of Lfull and Lreduced should follow a chi-square distribution with df degrees of freedom. Note that df corresponds to the number of independent equations in the null hypothesis. The Wald test is a special case of this likelihood test and tests the individual significance of the predictor variables, as we will discuss later.
Logistic Regression: Decision Boundary
Let's now have a look at the decision boundary modeled by the logistic regression. Remember, we are looking at classification and the decision boundary refers to the line, which is estimated in order to separate one class from the other. In other words, all observations on this line are the doubt cases for whom the probability of Good equals the probability of Bad and thus equals 0.5. As discussed previously, logistic regression is linear in the log odds. Hence, since the logarithm of 0.5 divided by 0.5 equals the logarithm of one, which is 0, the corresponding decision boundary becomes linear. Let's illustrate this with an example. 

Here you can see an example data set of good and bad payers according to their Age and Income. Small Age and small Income values are usually for the bad customers, whereas high Age and high Income values are for the good customers. This data set can be very well-separated with a linear decision boundary, so it would be very suitable for logistic regression. However, if there would have been a non-linear decision boundary between the Goods and the Bads, then logistic regression will not work well and other analytical techniques would be needed. Although it assumes a linear decision boundary, benchmarking studies have shown that it performs very competitive compared to other nonlinear classification techniques. Moreover, logistic regression is easy to understand and provides sufficient facilities for clarifying the default risk relationships in your data.
Logistic Regression: Other Bounding Functions
When we started discussing logistic regression, we introduced the bounding function f(z) = 1 divided by 1 + e to the power −z. We used that bounding function, since the outcome is always bounded between 0 and 1. However, as you may imagine, there exist other bounding functions, which are bounded between 0 and 1 as well. A first alternative is probit regression, whereby the cumulative standard normal distribution is used to limit the outcome between 0 and 1. It has been used in Moody's Analytics RiskCalc™ tool, which is a PD model developed by Moody's. It can be estimated in SAS using PROC LOGISTIC with the LINK=PROBIT option. 

Another example of a bounding function is the complementary log-log function, as you can see depicted right here. It can also be estimated in SAS using PROC LOGISTIC with the LINK=CLOGLOG option. 

Here you can see the three transformations depicted and contrasted with a linear transformation. Note that the probit transformation has thinner tails than the standard logit transformation. Also observe that the cloglog transformation is not symmetric and crosses the Y axis at around 0.6. It increases slowly from 0, but then approaches 1 quite suddenly. 

Empirical evidence has shown that all three transformations typically give very similar results in terms of predictive performance. Hence, in what follows, we will continue with the standard logit transformation that we introduced at the start of this session.
Logistic Regression: Weights of Evidence
Earlier on, we have introduced weights-of-evidence variables as a way to transform a variable such that it has a monotonic relationship to the target. Remember, higher weights of evidence mean less risk and vice versa. How can weights of evidence now be used with logistic regression? Well, this is quite easy. Here you can see a data set of customer information with a continuous Age variable. This variable is first coarse classified. Remember, for continuous variables, coarse classification was considered to model nonlinear or non-monotonic effects. In our case, age has been grouped into three categories: less than 22, between 22 and 35, and more than 35. For each of these categories, the weights of evidence can then be calculated. This weights-of-evidence variable can then be directly used in the logistic regression model as illustrated. This way, no dummy variables need to be introduced, hereby giving us a more robust model.
 Creating a Logistic Regression Model
In this demo, we will illustrate how to build logistic regression models in SAS. 

First, we start with building a logistic regression classifier. This can be done in SAS using PROC LOGISTIC. First, we identify the data set to be used, which, in our case, is mydata.applicants. So, in other words, we select the applicants data set from the mydata SAS library. Using the CLASS statement, we can define the categorical variables for which dummy variables need to be created. In our case, we want to create dummy variables for the checking and savings categorical variables. The GLM option indicates that we want 0/1 dummies instead of the default -1/+1 dummies. Note that, in this demo, we will not work with weight-of-evidence coding yet. The MODEL statement then defines the logistic regression model in terms of the dependent variable (good_bad) and the predictors (age, amount, duration, checking, and savings). The CTABLE option specifies that we want a classification table added to the output. Let's now run this MODEL statement and look at the output. 

In the output, we first see some general information about the data set, such as the number of response levels and the number of observations. Next, we see the target distribution, which is 300 Bads and 700 Goods. Then, we can see both categorical variables, checking and savings, and the 0/1 dummy indicators that have been defined. Remember, this was because we used the GLM option in the CLASS statement. This is followed by three fit statistics and three hypotheses, which test the model significance. Given the low p-values for all three hypotheses, we can conclude that the model has discriminatory power. We can then see the significance of the individual variables. Given the high p-value for the amount variable, 0.25, we can see that this variable is clearly insignificant. This is followed by a table with the estimates themselves. Here we can see the coefficients, the standard errors, Wald chi-square statistic, and p-value. Again, remember low p-values indicate important variables. Note also that because of the perfect multicollinearity, one of the dummy variables for checking and savings is always set to zero. The Odds Ratio table presents the odds ratio for each of the variables. Remember, the odds ratio is the multiplicative increase in the odds, if the variable increases with one unit whilst all other variables keep their values. The next table presents some performance statistics. One metric worth noting is the c coefficient. It is also referred to as the area under the ROC curve and will be explained later in the course. It is a number between 0 and 1, and higher values are to be preferred. In our case, it is 76.4%. The classification table then reports additional performance metrics for our logistic regression classifier. It hereby assumes various cutoffs. Remember, logistic regression gives a continuous score between 0 and 1, which can be mapped to a classification Good or Bad by assuming a cutoff. In our case, when the cutoff is 0.5, we can see that 73.5% of the observations are correctly classified. That means that out of 1000 observations, about 735 will be correctly classified. The table then also represents the false positives, false negatives, and other related numbers. 

Let's now change the transformation function and try out the probit and cloglog transformation. This can be done by adding the LINK option to the MODEL statement. First, we try out the probit transformation. The c coefficient now becomes 76.3%, which is very similar to the previous value, which was 76.4% in the case of the standard logistic regression transformation. Let's now also try a cloglog transformation. As you can see, also in this case, the c coefficient is 76.4%. So summarizing, all three transformations give very similar output and performance.

Decision Trees for Binary Classification
Decision trees are another important and popular classification technique for building credit scoring models in a Basel environment. Here is an example of a decision tree. The tree starts at the top or root node, which specifies a testing condition that needs to be evaluated. In our case, this condition is Income is bigger than $ 50,000 or not. If the answer to this question is Yes, then the right branch is followed. If No, then the left branch is taken. Both bring us to an internal node, which specifies another testing condition, in our case Job > 3 Years in the left part of the tree, and High Debt in the right part of the tree. The next level in the tree then brings us to the leaf nodes where the classifications are being made; in our case, Good Risk or Bad Risk. Given this tree, how would you classify a customer for whom the Income < $ 50,000 and who has been less than three years on his or her current job? Well, let's just read it from the tree. Income < $50,000 makes us take the left branch, which brings us to Job > 3 Years or not. In our case, the answer to this question is No, so we would predict this customer to be Bad Risk. 

Decision trees are often referred to as Recursive Partitioning Algorithms, or RPAs, because they recursively partition the data over and over again until a leaf node is reached, where a classification is made. 

We already introduced some terminology relating to decision trees, but let's refresh it here once more before continuing the discussion. The top node of the tree is called the root node. This is where the tree starts. Every node specifies a testing condition of which the outcome leads to a branch taking us to the next level of the tree. The testing conditions can be binary, as is the case in our tree, but can also have more than two outcomes. The tree ends with the terminal or leaf nodes. These nodes assign the classes, and in other words, make the predictions. Nodes sitting in between the root and leaf nodes are referred to as the internal nodes. 

Various tree induction algorithms have been introduced in the scientific literature to induce decision trees from data. Amongst the most popular are C4.5, or its newer version See5/C5, developed by Quinlan in 1993; classification and regression trees abbreviated as CART, developed by Breiman, Friedman, Olshen, and Stone in 1984; and chi-square automatic interaction detection abbreviated as CHAID, developed by Hartigan in 1975. These algorithms differ in their way of answering the key decisions that need to be made when building a decision tree, as we will discuss later. 

Decision trees can be used for both categorical as well as continuous targets. A decision tree with a categorical target is called a classification tree, whereas a decision tree with a continuous target is called a regression tree. Decision trees can thus be used for both PD modeling, where the target is binary, as well as LGD modeling, where the target is continuous. We will continue the discussion for classification. Regression trees will be covered in the lesson on LGD modeling. 

Here you can see the classification tree, which we already presented before. I wish to stress the fact that every split in this tree is a binary split, where the outcome can only take two values. Although multiway splits with more than two outcomes can also be used in decision trees, a key advantage of a binary split is that it essentially models a Yes/No answer, which is very easy to interpret. 

When estimating decision trees from data, three decisions should be made: the splitting decision, the stopping decision, and the assignment decision. Let's zoom into each of these in what follows. 

The splitting decision answers the question what variable to split at what value. In the example tree depicted, why is the root node a split based upon whether income is bigger or less than $50,000? Why not split income at $20,000? Or why not split age at 30, or marital status is married or not? The splitting decision will provide an answer to this question. 

The stopping decision answers the question when to stop growing the tree (in other words, when to stop adding nodes to the tree). In our example, why don't we add more nodes after having evaluated job is bigger than three years, or high debt. 

The assignment decision answers the question what class to assign to a leaf node. In our example, why do we predict Good Risk for all customers whose income is < $50,000 and job is bigger than three years, Bad Risk for all customers whose income is < $50,000 and job < 3 Years, and so on. Actually, this decision can be easily made by looking at the majority class in each leaf node. Consider, for example, the leaf node for Income > $50,000 and Job > 3 Years, which is the utmost left leaf node of the decision tree. If it turns out that there are 1000 customers in that leaf node out of which 800 are good risk, then we assign the class Good Risk to that leaf node. In other words, we always assign the class in a leaf node according to the majority. This idea is also referred to as winner take all learning, since the winner class makes the final assignment decision. So the assignment decision is quite easy to answer. In what follows, we will continue and discuss the splitting and stopping decision.
Decision Trees: Splitting Decision
The splitting decision splits a node N into two nodes, N1 and N2. PN1 is the proportion of observations that goes to the node N1, and PN2 is the proportion of observations that goes to the node N2. Various types of splits can be considered depending upon the type of the variable. For nominal variables, you can think of splits such as employment status is employed or not, marital status is married or not, etc. For ordinal variables, you can think of splits such as credit rating is less than or equal to BB. For continuous variables, you can think of splits such as age is less than 30. The idea is now to evaluate various splits and pick the best one. 

The question is, of course, how do we do this. In order to make the splitting decision, we need to introduce the concept of impurity. This is the same as chaos or disorder. A pure node is a node for which all observations are either Good or Bad. An impure node is a node where you have both Good and Bad observations. 

The idea is now to quantify impurity. If we could quantify impurity using a measure, then we could choose splits that reduce the impurity from the root node N to the nodes N1 and N2 as much as possible. Obviously, we should also take into account the proportion of observations going to N1 and N2, and evaluate the weighted decrease in impurity as depicted here. Various measures have been introduced to quantify impurity. A first straightforward measure is the minority class proportion. If this is 0, it means that all observations belong to one class, and the impurity is thus minimal. The minority class proportion can be maximally 0.5, in which case, you have an equal amount of Goods and Bads and thus maximal impurity. Other measures are the entropy and Gini. Let's have a look at these. 

Here you can see three sample data sets depicted with good customers represented in green and bad customers represented in red. For the left data set, you can see that all customers are good. Hence, this is minimal impurity. For the data set in the middle, you have the same number of good and bad customers. Hence, the impurity here is maximal. For the right data set, you see that all customers are bad, which is again equivalent to minimal impurity. This intuition will be perfectly captured by the entropy and Gini measure. The entropy is defined as minus the proportion of Goods times the second logarithm (of the proportion of Goods) minus the proportion of Bads times the second logarithm (of the proportion of Bads). It is used in C4.5 or See5/C5 to make the splitting decision. The Gini is defined as two times the proportion of Goods times the proportion of Bads. It is used in CART to make the splitting decision. Let's study the entropy first. 

In general, the entropy of a node is the sum of minus the proportion (of observations belonging to a class j) times the second logarithm thereof, summed across all classes. In the two-class case, it reduces to the formula in the second bullet as also discussed previously. The quality of a split can now be calculated as the gain, which is the weighted decrease in entropy when splitting a node N into two nodes, N1 and N2. 

Here, you can see a plot of the entropy measure in terms of the proportion of Goods. You can see that, when the proportion of Goods equals 0 or 1, the purity is maximal, resulting in an entropy of 0. You can also see that, when the proportion of Goods equals the proportion of Bads and thus equals 0.5, the impurity is maximal, resulting in a maximal entropy of 1. Let's now see how this can be used to make a splitting decision. 

Suppose we want to evaluate the quality of the binary split Age < 30. We start from a node with 400 Goods and 400 Bads. The age split splits the node into two nodes: one node with 200 Goods and 400 Bads, and one node with 200 Goods and 0 Bads. The gain of this split can now be calculated as follows. The entropy of the top node is maximal and thus 1. The entropy in the left node equals 0.91, because there are one-third Goods in the node and two-thirds Bads. The entropy in the right node is 0, since all observations are Good. The gain is then the entropy in the top node minus the entropy in the sub nodes, taking into account the proportion of observations that go to each of the subnodes, 600 divided by 800 and 200 divided by 800 respectively. This results in a gain of 0.32. 

We can now consider other splits, and each time calculate their gain. First, we can consider other age splits, let's say age less than 20, less than 40, less than 50, etc. Let's assume that the best age split in terms of gain is Age < 30. We can then evaluate the split marital status is married, and calculate the gain. Similarly, we evaluate different income splits in terms of their gain and keep the best one. Once we have considered all variables, we pick the split with the best overall gain. 

In our example, the best overall split is the split based upon Income bigger than $50,000. We now have the root node of our decision tree. The tree can now be built in a recursive way, since for every sub node, we can then again evaluate different splits, pick the best one in terms of gain, and continue the tree growing. Remember, that's why decision trees are often referred to as recursive partitioning algorithms. 

Another measure to quantify the impurity is the Gini measure. It works very similar to the entropy measure. It is the sum of all pairwise products of probabilities of belonging to different classes in a node. In the two-class case, it reduces to the formula already discussed before, being two times the proportion of class 1 multiplied by the proportion of class 2. Just as the entropy, it is minimal if either the proportion of Goods or Bads is 0, and is maximal if both proportions are the same and thus equal 0.5. You can now also define the gain by using the Gini instead of the entropy. The gain then becomes the weighted decrease in Gini. 

Here you can see the Gini measure depicted as a function of the proportion of Goods. You can see that it has a similar curvature as the entropy. One difference is that the maximum Gini equals 0.5, whereas the maximum entropy equaled 1. 

Here you can see the gain of the age split calculated using the Gini measure instead of the entropy measure. As with the entropy measure, multiple splits for multiple variables can then be evaluated and the one with the biggest gain can be kept. 

Note that whether you choose entropy or Gini as the impurity measure is usually not that very important. Empirical evidence has shown that usually both measures come up with similar splits.
Decision Trees: Stopping Decision
This brings us to the final decision that needs to be made when constructing a decision tree: the stopping decision. When you continue to add splits to the tree, what do you think will happen? Well, since each split further partitions the data, you will end up with fewer and fewer observations as you go down the tree. If you split until the extreme, you will end up with one observation in each leaf node, which means that you have perfectly filtered out the data and basically have a tree that perfectly fits your data. Is this what you want? Clearly not, because if you have a tree like this, it will also fit the noise, specificities, or idiosyncrasies of the data. In other words, this tree is too much focused on this particular data set. It is overfitting the data. It will perform extremely well on this data set, but not generalize well towards new data sets. Hence, how can we make the optimal stopping decision? In doing so, you need to make sure that you don't have too many splits, or otherwise, you are overfitting, but you also need to make sure that you don't have too few splits or you are underfitting, meaning that your tree is not complex enough to model the true relationships in the data. So let's get into some more detail about this stopping decision. 

You can think of two solutions to make the stopping decision. A first one is to stop adding splits to the tree if the gain is below some threshold. However, it is rather difficult to specify this threshold. Another strategy is to grow a full tree and then post-prune it to its optimal size. Let's explore this further. 

We start by splitting the data into two parts: a training data set and a validation data set. It is very important that both data sets are not overlapping. The training data set takes 70% of the observations and the validation data set 30% of the observations, let's say. The training data set will now be used to make the splitting decisions. In other words, we will use it to do all our gain calculations. The validation data set is an independent data set, which will be used to make the stopping decision, and thus decide on the optimal size of the tree. 

The stopping decision can now be made by means of the following plot. On the X axis, you can see the number of tree nodes, which obviously becomes bigger as more splits are being added to the tree. On the Y axis, you can see the misclassification rate depicted. This is the percentage of misclassified observations. So if you have 1000 observations and the tree classifies 800 of them correctly and thus 200 incorrectly, then the misclassification rate becomes 20%. We will now see how the misclassification rate on both the training and validation sets evolves as the tree is grown. On the training set, you will see that the misclassification rate continues to decrease. This is quite obvious as the splits become more and more tailored to the training set. In fact, if you could manage to put each training set observation into an individual leaf node, the training set error will become 0. On the validation set, we see another pattern. We see that initially the misclassification error will decrease, then stabilizes a bit, followed by an increase. When the validation set error starts to increase, it means that the tree becomes too specific to the training set and that's where overfitting starts to occur. Hence, the stopping decision should be made where the validation set error is minimal. A couple of things can be further mentioned here. First, instead of the misclassification error on the Y axis, you could also use other performance measures such as classification accuracy or even profit-related measures. Also, note that, sometimes, simplicity is preferred above accuracy, which means that you could also select a tree, which does not necessarily have minimum validation set error, but a lower number of nodes. This procedure is sometimes referred to as early stopping, because it allows to stop the tree construction process early by inspecting the validation set error. 

As we discussed earlier, logistic regression assumes a linear decision boundary. What is now the decision boundary of a decision tree? Let's have a look at the decision tree depicted at the right. It has two splits based upon Age and Income. In the left plot, you can see the data visualized. The first split evaluates whether Age is less than or equal to 30 or not. You can see that this creates a first decision line, which is orthogonal to the X axis, representing Age. To the right of this line, all observations are predicted as Good. To the left of this line, another split is added based upon Income less than or equal to 1200, which again corresponds to a decision line, but now orthogonal to the Y axis, which represents Income. Hence, you can see that the decision boundary of a decision tree consists of a set of lines, which are orthogonal to the axes. This allows to model more complex patterns in the data than a logistic regression model.
Using Decision Trees for Credit Risk Modeling
What are now the advantages and disadvantages of decision trees? In terms of advantages, decision trees are easy to interpret and understand. You can easily explain why a customer is classified as a good or bad payer. Note that, especially in credit scoring, being able to explain classifications is very important and required by various regulations available. Decision trees are also non-parametric, since no assumptions of normality, symmetric distributions, or independence are needed. Hence, no prior data transformations such as a logarithmic transformation or weights-of-evidence coding are needed. Finally, they are also very robust with respect to outliers. 

In terms of disadvantages, decision trees are very sensitive to changes in the training data. If you draw another sample from the same underlying data, you are likely to end up with another decision tree. A reason for this is that usually there are many competing good splits for the root node of the decision tree. So because of data variations, another sample might give another root node, and thus a totally different tree. That's why decision trees are often referred to as weak or unstable classifiers. A solution for this problem is to use ensemble methods, whereby multiple decision trees are estimated and combined into an ensemble. Popular methods here are bagging, boosting, and random forests. We will not discuss those further, and refer to other courses on the topic. 

To conclude, let's discuss how decision trees can be used in credit risk modeling. First, they can be used as an alternative to chi-square analysis to perform coarse classification. Remember the example we discussed earlier about coarse-classifying a residential status variable. Instead of calculating the chi-square value for each option, you could also calculate the gain for each option and pick the option with the highest gain. Note, however, that usually both chi-square and gain calculation give similar results in terms of the best coarse classification. 

Decision trees can also be used to screen variables and do variable selection. Variables that appear at the top of the tree are the most predictive. You can very easily use the gain measure to compare various variables in terms of their predictive power. Decision trees can also be used to perform segmentation. In this case, you could build a decision tree of only a few levels deep, say two, and then build a second-stage logistic regression model in each of the leaf nodes. You could also build a decision tree model and use that as your final application or behavioral scorecard. This is, however, not advised as decision trees typically give very bad ROC curves, as we will discuss later. Finally, you could also use regression trees for LGD modeling, as we will also discuss later.
 Creating Decision Trees
In this demonstration, we will illustrate how to build decision trees in SAS Enterprise Miner. 

We will use the applicants data set as our input data. The data set has already been imported into SAS Enterprise Miner and is ready for analysis. 

We start by creating a new diagram and name it decisiontree. We then drag and drop the applicants data set to the diagram workspace. From the Sample tab, we add a Data Partition node to the diagram workspace and connect it to the applicants data set. We create a training set, which consists of 70% of the observations and a validation set with 30% of the observations. Remember, decision trees use the training set for making the splitting decision and the validation set for making the stopping decision. In this demonstration, we will not use a test set. We set the partitioning method property to Stratified. This will make sure the Good-Bad odds are the same in both the training and validation sets. We now run the Data Partition node by right-clicking it and selecting Run. We now click OK. We can inspect the training and validation sets by clicking the button next to the Exported Data property. So, here you can see the training and validation sets. Let's select the validation set and click Explore. Here you can see the validation set observations and here you can see some sample statistics. 

From the Model tab, we now add a Decision tree node to the diagram workspace and connect it to the Data Partition node. The SAS implementation of decision trees finds binary or multiway splits for nominal, ordinal, and interval inputs. The splitting criteria and other options to determine the method of tree construction can be set in the Property panel. The options include a mix of C4.5 or See5, CART, and CHAID. Let's inspect some of these properties. Here you can see the impurity criteria that can be used for an interval, nominal, and ordinal target. Since our target is nominal, we can choose between chi-square (which is used in CHAID), entropy (which is used in See5), and Gini (which is used in CART). Let's set it to Entropy. Here, we can specify whether missing values should be used as a separate value in the split search or not. When setting the Use Input Once option to true, an input will be used at most once during the construction of the tree. The Maximum Branch property allows to determine the maximal number of branches per split. Here it is set to 2, which means we will use binary splits instead of multiway splits. The Maximum Depth property specifies the maximum number of levels in the tree. The Leaf Size option determines the minimum number of observations in a leaf. The Number of Rules option controls the number of competitor splits reported for each node. The best competitor split has the second highest value in entropy reduction compared to the optimal split. 

Here, we can specify the number of surrogate rules. A surrogate rule is a rule that closely agrees with the action of the primary split. The degree of agreement is defined as the proportion of training observations that the two rules assign the same branch. When the main splitting rule relies on an input whose value is missing (for example, because it was not in the training set), the first surrogate rule is used. If this one can also not be evaluated, the second surrogate rule is used and so on. 

In the Subtree subpanel, we can specify properties to make the stopping decision. When the Method property is set to Assessment, the smallest tree with the best assessment measure on the validation set will be selected. Here we can set the assessment measure. Let's put it to Misclassification. 

For the meaning of the other properties, we refer to other SAS courses on the topic of decision trees. In this demonstration, we will keep them to their default values. 

Let's now run the decision tree node and inspect its results. Let's first inspect the tree itself. The first split in the tree is based on the categorical checking account variable. Here we can see that this is not a further split and is a leaf node. Here another split is added, based upon duration. Note that the nodes of the tree are colored according to the entropy. Dark nodes represent low entropy, as you can see right here, where we have about 87% Goods and 13% Bads in the training data set. Lighter nodes indicate higher entropy, as you can see here, where we have about 44% Bads and 56% Goods in the training data set. The thickness of the branches is proportional to the number of observations that follows it as you can see right here. You can display the entire tree by right-clicking on the plot and selecting View, Fit to Page. Here you can see the entire tree. 

The tree map gives an alternative compact graphical display of the tree in vertical orientation. The node width is proportional to the number of observations in the node. The colors again represent the entropy. So, here we have the root node, and the split based on checking account splits it in almost two equal parts. Remember that this node is a leaf node and is not split further, whereas here we further split based upon duration. 

The fit statistics provide a numerical summary of model performance. One interesting statistic here is the misclassification rate. You can see that it is very similar on both the training and validation sets. The cumulative lift curve is a graphical representation of model performance, which we will discuss later. For the moment, it suffices by saying that it should be as high as possible at the start of the curve. 

The leaf statistics display the percentage of Goods in each leaf node for both the training and validation sets. The leaf with the highest percentage of Goods on the training data has a leaf index equal to 1. If we select the first leaf index in this plot, then the corresponding leaf in the tree window will be highlighted. 

Let's now also have a look at the early stopping plot. We select View, Model, Subtree Assessment Plot. We also select Misclassification Rate as our Y-axis. As you can see, the misclassification rate on the training set keeps on decreasing. This is because the tree splits become more and more tailored to the training set, and at some point, the tree will overfit the training data. On the validation set, the red curve, we see a different pattern. Here we can see that the misclassification error initially decreases, then flattens out, and then increases back again. The optimal tree can then be found where the validation set error is minimal with the minimum amount of tree leaves as you can see right here. 

Every decision tree can also be represented in an alternative way using IF-THEN rules, whereby one rule corresponds to one path from the root node to a particular leaf node. We can see these rules by selecting View, Model, Node Rules. Here you can see the rules that correspond to the decision tree.
Linear Programming
Linear programming is another technique to do classification. Although it is not that often used in the industry anymore, it was one of the first techniques that was used to build commercial credit scoring solutions. Actually, one of the first studies of using linear programming for classification was made by Olvi Mangasarian already in 1965. Let's discuss how it works. 

Assume I have a data set with good and bad payers and various characteristics such as Age, Income, Marital Status, etc. Let's first consider all the good payers. Assume we have ng good payers. For the good payers, we would like to ensure that their credit score is above some predetermined cutoff, c. Think of c as 0, 10, 100, or any other value. Let's set it to 100. Say that the first good payer is Bart whose Age is 30 and Income is 1000. If we only consider Age and Income, then Bart's credit score is w1*30 + w2*1000. Since Bart is a good payer, his credit score should be above 100. This is essentially what the first set of constraints in the linear program models. For every good customer, it computes a weighted sum of the customer's characteristics with weights w1>, w2, etc. and makes sure that the resulting score is above the cutoff c.

Once we have considered all the Goods, it's time to look at the Bads. Assume we have nb Bads. Let's say the first bad is Peter. Peter's Age is 18 and Income 800, so his credit score becomes 18*w1 + 800*w2. Since Peter is a bad payer, his credit score should now be below 100. This is essentially what the second set of constraints in the linear program models. For every bad payer, it calculates a score, which is also a weighted sum of weights w1, w2, up to wn, and the customer's characteristics, and which should now be lower than c. 

Note that it will be impossible to achieve perfect classification. In other words, due to noise in the data or overlapping class distributions of the Good and Bads, it will be impossible to assign all the Goods a score above c and all the Bads a score below c. Hence, we need to introduce positive error variables ei for each observation i, such that the constraints can be relaxed and the linear programming problem can be solved. In order to relax the constraints for the Goods, the positive error variable ei will be subtracted from the cutoff c. In order to relax the constraints for the Bads, the positive error variable ei will be added to the cutoff. 

Obviously, we want the error variables ei to be 0 as much as possible. That's why we will minimize the sum of all error variables ei in the objective function. As you can see now, this LP formulation aims at minimizing the sum of the absolute values of the deviations, and is hence referred to as the MSD formulation. Various variants to the MSD formulation have been introduced in the literature. 

A first variant is the minimize the maximum deviation, MMD, formulation. Here, only one positive error variable e is used in all constraints, and also minimized in the objective function. One of the big advantages of using linear programming for classification is that it is very easy to include prior business knowledge or experience by adding constraints to the program. Let's say that our business expert informs us that Income has a positive effect on the credit score. This knowledge can be easily embedded in the linear program by adding a constraint that says the weight of Income should be positive. Furthermore, if our business expert informs us that Income is more important than Age, then another constraint can be added specifying that the weight for Income is higher than the weight for Age. 

Finally, you can also think of an integer programming variant of the linear programming formulation. The left-hand side of the constraints is the same as in the previous formulations. In other words, it represents the credit score for the Goods and Bads. The right-hand side is somewhat different. The gi and bi variables are binary indicators and are either 0 or 1. M is a constant chosen up front and can take values such as 9, 99, 999, etc. Both M and the gi's and bi's are needed to relax the constraints. The gi's and bi's are then minimized in the objective function. Since they are binary indicators, the value of the objective function after optimization will be equal to the number of misclassifications. Trial and error can be used to set the value for M. If you set M to 9 and it turns out that no solution can be found, because given the data characteristics, the constraints cannot be sufficiently relaxed, then increase M to 99 and try solving the program again until a solution is found. 

Although, as already said, linear programming has been a popular technique for building credit scorecards, it has become legacy nowadays. It suffers from many disadvantages. First of all, there are no statistical procedures that come with it to do variable selection. This is especially relevant in behavioral scoring, where usually many variables are available. Next, since a constraint needs to be introduced for every observation, it is also very time consuming. That's why it is only recommended for small samples of, let's say, less than 500 observations. Note, however, that linear programming forms the basis for support vector machines, SVMs, which are powerful, non-linear classification techniques.
 Creating a Linear Programming Model
In this demonstration, we will illustrate how to build a linear programming-based classifier in SAS. We hereby use the example illustrated in the course and build an MMD or minimize the maximum deviation linear programming classifier. First, we create a data set, credit, with the objective function and the constraints. This data set has the following variables: an identifier, the weights for age, years at address and phone, one error variable e, the type (which for constraints can either be less than or equal for the Bads or greater than or equal for the Goods), and the right hand side of the constraint. Note that the phone variable has been coded as 1 if phone is yes and 0 otherwise. Also, note that the error variable e has been moved to the left hand side of the constraint. The first line of the data set then models the objective function as follows: minimize 0*w1 + 0*w2 +0*w3+ 1*e. Note that the objective function has no right-hand side; hence, the black dot. 

The first constraint then reads 24*w1 + 1*w2 + 0*w3 – 1*e should be less than or equal than 100 since it concerns a bad payer. The last constraint then reads 25*w1+ 5*w2+ 1*w3 + 1*e should be greater than or equal to 100, since it concerns a good payer. We can now solve this linear program using PROC LP. This PROC gives lots of output, which is not that relevant to us here. The most important part concerns the variables' section where we can see that w1 equals 0 (which implies that age is not important), w2 equals 9.52, and w3 equals 47.61. To use this scorecard for scoring new observations, we then simply need to calculate the score as follows: 0*age + 9.52*Years at Address + 47.61*Phone. If this score is above 100, the customer can be considered as a good payer. If it's below 100, the customer can be considered as a bad payer.

 

Text Version	Collapse  Print
Multiclass Classification

Cumulative Logistic Regression
Cumulative logistic regression, also called ordinal logistic regression, is an extension of logistic regression to deal with ordinal multiclass targets such as credit ratings. Remember, as discussed in Lesson 1, when adopting the shadow-rating approach, you want to build a classification model predicting ratings. Ratings are ordinal and have an ordering between them: AAA is better than AA, AA is better than A, and so on. In other words, when thinking about cumulative probabilities, then the probability of having a rating lower than or equal to AAA is higher that the probability of having a rating lower than or equal to AA, which is, in turn, higher that the probability of having a rating lower than or equal to single A, and so on. 

In cumulative logistic regression, the cumulative probabilities are modeled using a logit type of transformation. The formulation goes as follows: The probability that a firm with characteristics x has a rating lower than or equal to rating R is 1 divided by 1 + e to the power −θR + β1 times x1 + β2 times x2, and so on. The x variables represent firm characteristics such as accounting ratios, profitability information, stock price information, etc. This can then also be reformulated in terms of the odds as depicted on the second line. Taking the logarithm, you obtain a linear expression as indicated on the third line. Note that every rating category R has its own intercept θR. Since the probability of having a rating lower than the highest rating, AAA, is one, the intercept that comes with it is equal to plus infinity. The logit functions for all ratings are parallel, since they only differ in the intercept. Hence, this model is also referred to as the proportional odds model. 

Just as with binary logistic regression, the beta parameters of the cumulative logistic regression model can be estimated using the maximum likelihood procedure. In SAS, this is implemented by PROC LOGISTIC. The individual rating probabilities can then be obtained as follows: The probability of having the lowest rating D is equal to the cumulative probability of having a lower rating than D. The probability of having rating AA is equal to the probability of having a rating lower than or equal to AA minus the probability of having a rating lower than or equal to A. Finally, the probability of having the highest rating, AAA, is one minus the probability of having a rating lower than or equal to AA. 

When assigning a rating to a new observation using the cumulative logistic regression model, you can calculate all the rating probabilities and assign the observation to the rating with the highest probability. This is again the idea of winner-take-all learning. Here you can see some examples of rating probability distributions for various ratings in terms of the z-score, which is the linear combination of −θR and the weighted sum of the characteristics. It can be clearly seen that, depending upon the z-score, every rating pops up at least once as the winner.
 Performing Cumulative Logistic Regression
In this demonstration, we will illustrate how to implement a cumulative logistic regression classifier in SAS. 

We have defined a data set bondrate, which has bond ratings together with a set of continuous predictor variables. The predictor variables are logarithmic transforms of accounting variables such as operating margin, leverage, liquidity ratios, etc. Let's now run this data set. 

We will now recode the target variable in a numeric way. This will allow SAS to correctly determine the order between the different target values. We run this statement and inspect the bondrate2 data set in the work library. We can here see that the data set bondrate2 has been successfully created and that the rating is now numerical. 

We are now ready to run our PROC LOGISTIC. By default, PROC LOGISTIC will estimate a cumulative logistic regression model. We start by defining our model in terms of its dependent and independent variables. The results of this PROC will be stored in a new data set called bondpreds. With the option predprobs=individual in the OUTPUT statement, we can ask for the individual estimated probabilities per rating for each observation. Let's now first run the PROC and inspect its output. 

In the output, we can first see some general information like the number of observations, and the distribution amongst the target ratings. This is followed by model fit statistics and three hypothesis tests to verify whether the model has discriminatory power. As you can see, all three tests result in low p-values indicating that the model is significant. This is followed by the parameter estimates. Remember, we had seven ratings for which we need six intercept terms. By inspecting the p-values, we can see that not all coefficients are significant and further input selection might be beneficial. The output concludes with the odds ratio estimates and some measures of association between the target ratings and the predictions. 

Let's now also inspect the bondpreds data set in the work library. Here you can see that the individual probabilities for every target rating have been appended to the data set. Obviously, these probabilities sum to 1 for every observation. Also note the column, formatted value of the predicted response, which decides upon the prediction based upon the largest predicted probability. The performance can then be calculated by comparing the column's formatted value of the observed response and formatted value of the predicted response.


Decision Trees for Multiclass Classification
Decision trees can also be used for multiclass classification. In fact, all concepts that we discussed for binary classification trees can be easily generalized towards a multiclass setting. First, regarding the splitting decision, the entropy and Gini measures are now calculated for all K classes as depicted right here. The assignment decision in the leaf nodes is then based upon the class with the highest probability, again winner-take-all learning, remember. And finally, the stopping decision is also made using a validation set and an early stopping plot, whereby the error on the Y axis now represents the multiclass misclassification error.
Coding Schemes
You can also use coding schemes to map a multiclass problem to a set of binary classification problems. Two examples of coding schemes are one-versus-one and one-versus-all. In one-versus-one coding, the multiclass problem is tackled by estimating binary classifiers discriminating every possible pair of classes. In other words, for k classes, this means k*(k-1)/2 classifiers need to be estimated, each time on a reduced data set. A new observation is then classified by letting all these classifiers cast a vote about the target class. In one-versus-all coding, a binary classifier is estimated to contrast every class against all other classes. For k classes, this means k classifiers need to be estimated. A new observation is then classified in the class with the highest posterior class probability. Let's illustrate both these schemes. 

Here you can see a data set of firms. Every firm has two characteristics: accounting ratios 1 and 2, depicted on the X and Y axis respectively. There are three possible ratings: A, B, and C. We would now like to classify the blue triangle using the coding schemes discussed earlier. Let's start with one-versus-one coding. The first binary classifier that we estimate distinguishes between A and B. Let's say that we build a logistic regression classifier for this. Remember, logistic regression assumes a linear decision boundary. From the plot, it can be seen that the blue triangle is to the left of the decision line, so the classifier will classify it as A. 

The second binary classifier, which can then be constructed, is a logistic regression discriminating between rating B and rating C. Since the blue triangle is below the decision line, the logistic regression will classify it as C. What do you think will be the next classifier to be estimated now? Indeed, a logistic regression to discriminate between rating A and C. This classifier will classify the blue triangle as A. Hence, we have two votes for A and one vote for C, such that the overall prediction becomes A. 

Let's now see how this would be solved using one-versus-all coding. First, we estimate a binary classifier, say again, logistic regression, discriminating A versus the rest, or thus A versus B and C. The probability for the blue triangle to be rating A equals 0.92, let's say. The next classifier discriminates B versus the rest. The probability for the blue triangle to be rating B equals 0.18, let's say. The third classifier discriminates C versus the rest. The probability for the blue triangle to be rating C equals 0.12. The biggest probability is for rating A, so this becomes our final prediction.
 Building Analytical Models
In this demonstration, we will illustrate how to build analytical models in SAS Enterprise Miner. The purpose is not to discuss the models and results into big detail but rather illustrate the process flow of building analytical models. 

We will use the applicants data set as our input data. The data set has already been imported into SAS Enterprise Miner and is ready for analysis. 

We start by creating a new diagram and name it creditscoring. We then drag and drop the applicants data set to the diagram workspace. From the Sample tab, we add a Data Partition node to the diagram workspace and connect it to the applicants data set. In the Property panel, we choose a training set of 50%, a validation set of 30%, and a test set of 20%. We set the partitioning method property to Stratified. This will make sure that the Good-Bad odds are the same in all three data sets. We now run the Data Partition node by right-clicking it and selecting Run. 

From the Model tab, we add a Regression node to the diagram workspace and connect it to the Data Partition node. Note that since we have a binary target variable, SAS will estimate a logistic regression model as you can see right here. We run the node and quickly inspect the output. Here you can see information about the coefficients of the model and its performance. Both will be discussed more extensively in the course. From the Model tab, we also add a Neural Network and Decision Tree nodes to the diagram workspace and connect both to the Data Partition node. We run both nodes. 

From the Assess tab, we add a Model Comparison node and connect it to the Regression, Decision Tree, and Neural Network nodes. This node will compare all three models and select the best one according to the Model Selection options set in the Property panel right here. Let's run the node. Here you can see the various performance metrics and plots. You can also see that the regression model has been selected as the best model. 

We are now ready to score new data. We add the score data to the diagram workspace and set its role to score. From the Assess tab, we add a Score node to the diagram workspace. We then connect both the Model Comparison node and the score data to the Score node. We run the Score node. We now inspect the results by clicking the button next to Exported Data Property in the Property panel. We explore the score data. Here we can see the columns Predicted: good_bad=good and Predicted: good_bad=bad. Both give the probability that the customer is Good and Bad, respectively, as calculated by the logistic regression model. This Into: good_bad column then gives the classification based upon the biggest probability.



