---
title: cl dog eleven
author: cl
date: '2018-05-16'
slug: cl-dog-eleven
categories: []
tags: []
---


Introduction
Let's now assume that our PD, LGD, and EAD models have been built. The next step is then to put them into production and start monitoring or validating them. In this lesson, we will discuss various ways of validating credit risk models. In what follows, you will first gain insight into the regulatory aspects of validation, as well as learning key terms and principles. Next, you will learn about the characteristics of quantitative and qualitative validation.





 
Objectives

In this lesson, you learn to do the following:
•	describe the main validation procedures recommended by regulatory agencies
•	define validation terminology, including benchmarking and backtesting
•	describe the requirements for a validation framework
•	identify common validation principles and issues
•	describe the characteristics and potential problems of quantitative validation
•	describe the levels of backtesting for PD, LGD, and EAD models
•	describe backtesting statistics, including the binomial test, the Hosmer-Lemeshow test, the Vasicek one-factor model, and the normal test
•	describe the traffic light approach to backtesting
•	identify the characteristics of benchmarking
•	describe the following aspects of qualitative validation: use testing, data quality, model design, documentation, and corporate governance and management oversight
•	Text Version	Collapse  Print
Fundamentals of Validation

Regulatory Perspective on Validation
Let's first start by discussing the regulatory perspective on validation. Here you can see some paragraphs from the Basel Accord on the topic. Let's read them together and then add some discussion afterward. 

"The institution shall have a regular cycle of model validation that includes monitoring of model performance and stability, review of model specification, and testing of model outputs against outcomes". 

Banks conduct validation on a regular basis. This usually occurs on a monthly basis and sometimes even more frequently. Obviously, the more frequently it is done, the more quickly performance deviations can be detected. 

"Institutions shall have robust systems in place to validate the accuracy and consistency of rating systems, processes, and the estimation of all relevant risk parameters. 

This paragraph takes a very broad view on validation. A key point to remember here is that it is not only about the estimation of all relevant risk parameters, which constitutes a narrow view on validation, but also about the accuracy and consistency of the rating systems and processes as a whole, which is a much broader perspective on validation. 

"Institutions shall regularly compare realized default rates with estimated PDs for each grade and, where realized default rates are outside the expected range for that grade, institutions shall specifically analyze the reasons for the deviation" … (similar for LGD and EAD)…"This analysis and documentation shall be updated at least annually." 

Comparing realized default rates with estimated PDs refers to the quantitative validation activity of backtesting. Also important to note here is that this must be done at least annually, although as already said, banks will typically do this more often. 

"Institutions shall also use other quantitative validation tools and comparisons with relevant external data sources." 

This is the idea of benchmarking, another key quantitative validation activity. The purpose here is to compare internal models and/or estimates with an external reference model and/or estimates. The aim of benchmarking is to find model deficiencies and identify ways to improve those. 

"Institutions shall have sound internal standards for situations where deviations in realized PDs, LGDs, conversion factors and total losses, where EL is used, from expectations, become significant enough to call the validity of the estimates into question." 

This paragraph basically says that validation encompasses two activities. First, a validation framework should be able to provide a diagnosis. It should tell us if the rating system is healthy or ill. Once a diagnosis has been obtained, the validation framework should also foresee action plans. For example, if the diagnosis is that the rating system is sick and that the PDs are systematically underestimated, then an action plan is needed to remedy this situation. 

Here you can see the general overview on validation as it was introduced in the BIS 14 working paper. Basically, it's the supervisor who will evaluate the internal validation conducted by the bank. Usually, a bank will have both a modeling and a validation or audit team. The purpose of the latter is to closely inspect and validate all PD, LGD, and EAD models, both quantitatively and qualitatively. Many banks adopt the principle of a "Chinese wall" separation between both the modeling and validation team to enforce an independent, unbiased, and fair evaluation. Validation encompasses both the validation of the rating system as well as the validation of the rating process. In terms of the rating system, a first activity concerns the validation of the model design. This refers to the definition of the model, its perimeter, and scope. A next activity here concerns the validation of the risk components. This includes both the backtesting and benchmarking of the PD, LGD, and EAD estimates. In terms of the rating process, a first validation activity concerns the issue of data quality. As said earlier, data is the key ingredient of an analytical PD, LGD, or EAD model and its quality should thus be optimally safeguarded. A next activity concerns the reporting and problem handling. This basically relates to the reports and documentation available about the various steps and results of the rating process. Finally, it is of key importance that the analytical PD, LGD, and EAD models are not only used for Basel Capital calculation purposes, but also for other activities and business purposes. This is the so called use test, which we will also clarify in what follows.
Validation Terminology
Let's briefly refresh the key activities of quantitative validation: backtesting and benchmarking. Backtesting refers to comparing ex-ante-made estimates to ex-post-realized numbers. Here you can see an example of this. Suppose we have four ratings with corresponding PDs: 2%, 3%, 7%, and 20% in our case. Each rating has a number of observations assigned to it and a number of observed defaults. Both allow us to calculate the default rate. Backtesting refers to comparing this default rate to the estimated PD. For example, for rating D, backtesting will test whether the observed default rate of 25% is significantly different from the predicted PD of 20%. 

Benchmarking is another key quantitative validation activity. The idea here is to compare internal models and/or estimates with a reference model and/or estimates. Note that validation is more than just backtesting and benchmarking, as we will discuss in what follows. 

In this lesson, we will discuss both quantitative and qualitative validation. In terms of quantitative validation, we will discuss both backtesting and benchmarking. In terms of qualitative validation, we will discuss data quality, use test, model design, documentation, and corporate governance and management oversight.
Common Validation Issues
Before we continue the discussion, let's start from some starting insights and observations. Banks employ a wide range of techniques to validate internal ratings. The techniques used to assess corporate and retail ratings are substantially different. One of the reasons behind this is that, contrary to retail portfolios, in many corporate portfolios, data availability is a key issue. Hence, in case of a shortage of data, the credit risk models developed will be different, and so will be their validation. You can here think of expert-based qualitative credit risk models, for example. 

Ratings validation is not an exact science. Absolute performance measures are considered counterproductive by some institutions. It is really hard to come up with minimum performance benchmarks which PD, LGD, or EAD models need to achieve, in order to be considered satisfactory. This typically depends upon the data characteristics, portfolio composition, and strategy of the financial institution. Hence, any performance metric reported should be interpreted in terms of its own specific context. 

Expert judgment is critical. Data scarcity makes it almost impossible to develop statistically based internal-ratings models in some asset classes. This refers to our earlier point concerning the lack of data. You can think here, for example, about the issue of low default portfolios, whereby a portfolio has an insufficient number of defaulters in order to come up with a meaningful statistical model. Hence, the role of credit experts and qualitative validation will be very important here. 

Data issues center around both quantity and quality. Default data, in particular, is insufficient to produce robust statistical estimates for some asset classes. For PD modeling, the quality of the data is usually satisfactory. Sometimes the issue of quantity, in terms of number of defaulters, complicates the development of statistical models, as already discussed before. For LGD and EAD modeling, data quality is often a key concern. That is one of the major reasons why LGD and EAD models typically have a low predictive performance, as discussed in previous lessons.
Validation Principles
Here, you can see some general validation principles that have been put forward by the Basel Committee Validation Subgroup. 

Principle 1: Validation is fundamentally about assessing the predictive ability of a bank's risk estimates and the use of ratings in credit processes. 

This basically refers to the ideas of backtesting and use testing. 

Principle 2: The bank has the primary responsibility for validation. 

It is not the supervisor who will do the validation. The bank is internally responsible to do the validation itself. The supervisor will then review the validation conducted by the bank. 

Principle 3: Validation is an iterative process. 

Validation is not a single shot, sequential activity. Quite on the contrary, it is a continuous, ongoing, iterative process; sometimes even very ad hoc. 

Principle 4: There is no single validation method. 

As already said, validation depends upon the context. This can refer to the type of portfolio, the strategy of the firm, the quality of the data, etc. 

Principle 5: Validation should encompass both quantitative and qualitative elements. 

We already discussed this. Remember, quantitative validation refers to backtesting and benchmarking. Qualitative validation refers to data quality, use test, model design, documentation, and corporate governance and management oversight. 

Principle 6: Validation processes and outcomes should be subject to independent review. 

This refers to the supervisor reviewing the validation of the bank, as already discussed earlier. 

Actually, validation is a very difficult activity to optimally organize from an organizational perspective. When adopting a strict split-up between the modeling and the validation team, whereby the latter is conceived as the watchdog of the former, then friction may arise between both teams. I have observed this many times in financial institutions worldwide. Actually, to be successful, it is of key importance that validation is constructive and focuses on giving appropriate feedback on the developed credit risk models, rather than unnecessarily criticizing them. Again, remember that validation does not provide a fixed decision but rather a suggestion for further action and study. Hence, both model diagnostic frameworks as well as action plans need to be developed. Finally, validation methods are not allowed to change with the economic cycle unless this is clearly and thoroughly documented.
Developing a Validation Framework
When working out a validation framework, various things need to be considered. First, the validation needs must be unambiguously diagnosed. What credit risk models must be validated in which portfolios? Then, the various validation activities need to be worked out in detail. All of this should be put into a timetable, specifying what validation activity should be conducted by when. Also, the various statistical tests and analyses must be clearly defined. Obviously, this will highly depend upon the type of credit risk parameter (for example, PD, LGD, or EAD) to be validated. Finally, the actions must be defined in response to the potential findings. Suppose the validation exercise tells us that the LGD is systematically underestimated. To remedy this, an action plan must be defined. To summarize, the validation policy should clearly specify the why, what, who, how, and when of the whole validation exercise.





 
Text Version	Collapse  Print
Quantitative Validation

Introduction to Quantitative Validation
We will first zoom into quantitative validation. Remember, the goal here is to verify how well the various ratings predict default or thus PD, loss or thus LGD, or exposure or thus CCF. As mentioned in the Basel II Accord, the burden is on the bank to satisfy its supervisor that a model or procedure has good predictive power and that regulatory capital requirements will not be distorted as a result of its use. Actually, the whole idea of quantitative validation boils down to comparing realized numbers to predicted numbers. It speaks for itself that those numbers will seldom be identical. Hence, various performance metrics and test statistics must be specified to assist in this comparison. When using these, appropriate cutoffs such as significance levels must be set. The severity of this cutoff will then determine the severity of the whole validation exercise. A more severe cutoff will result into a more conservative validation, which will thus more quickly detect a performance difference. Also, when doing validation, you should carefully think about the split-up of the data. In other words, on what data are we going to calculate the various validation performance metrics and statistics. Let's zoom into this in what follows. 

This figure displays the various ways of splitting up your data for doing validation. A first way is out-of-sample validation. This works by splitting up a data set observed during a particular time frame into a training and test set. Remember, the training set is used to develop the model, whereas the test set is used to calculate its performance in an independent way. In this method, both the training and test sets are overlapping in time. This is the most commonly used method of doing validation during the development of your PD, LGD, and EAD models. In out-of-sample/out-of-time validation, there is a strict time difference between the test set and training set. In other words, the test set comes from a subsequent time period. This is typically the type of validation, which you will do during model usage, or after the models have been deployed and put into production. Out-of-universe validation refers to validating a model on another population than which it was developed on. As an example, think of a PD model developed on American SMEs, small and medium-sized enterprises, and being validated on a set of Canadian SMEs. Out-of-universe validation directly relates to the model perimeter, since it tells us how well a model generalizes beyond its original scope. This is very important given the many mergers and acquisitions seen in the financial industry lately, and whereby banks suddenly have multiple credit risk models for similar types of portfolios. Finally, the most ambitious validation setup is out-of-universe and out-of-time validation, whereby a credit risk model is validated on data from another population and subsequent time frame. 

Various challenges arise when doing quantitative validation. A first one concerns the sources of variation you are being confronted with. The difference between the predicted PDs and observed default rates can be caused by at least three reasons. A first one concerns sample variability. This is the variability due to the fact that the predicted PDs have been calculated using a limited sample of observations. A next one is external effects. Macroeconomic up- or downturns will also have an impact on default rates. Finally, there are internal or endogenous effects due to a change in portfolio composition, strategy shift, or a merger or acquisition, for example. Suppose now that you only focus on sample variation and the PD for a rating is 100 basis points. Let's now say that you want to be 95% confident that the actual PD is not more than 20 basis points off from the estimate. When modeling this using a binomial confidence interval, the number of obligors you would need equals the square of the following expression: 1.96 times the square root of PD times 1−PD divided by 0.002, or in other words, 9500 obligors. In retail portfolios, this is usually no problem. However, in corporate portfolios, the number of obligors may be substantially less, hereby increasing the confidence bounds and thus uncertainty around the PD estimates. 

Another complication is the statistical independence assumption, which is often assumed when building credit risk models. This assumption is often untrue in reality. Think about the correlation between defaults, the correlation between PD, LGD, and EAD, for example. Hence, this further complicates the validation exercise. Finally, as already said, data availability can also be a concern, especially in corporate portfolios.
Backtesting PD: Levels and Traffic Light Method
In what follows, we will discuss backtesting PD models. We will hereby use the same approach that we used to model PD. At Level 0, we will start by checking the data stability. In other words, we will measure to what extent the population that was used to construct the PD rating system is similar to the population that is currently being observed. At Level 1, we will measure how well the PD rating system provides an ordinal ranking of the risk measure considered. Finally, at Level 2, the mapping of the rating to a quantitative risk measure will be evaluated. A rating system is considered well-calibrated if the (ex-ante) estimated risk measures deviate only marginally from what has been observed ex-post. 

In the context of PD models, at Level 0, the stability of the internal, external, and expert judgment data needs to be backtested. At Level 1, the application and behavioral scorecard will be evaluated. Both will typically have been constructed using logistic regression models. At Level 2, the risk ratings and PD calibration will be backtested. 

When backtesting PD models, one often adopts a traffic-light-indicator approach to encode the outcome of the various performance metrics or test statistics. A green traffic light means that everything is okay, and thus the model predicts well and no changes are needed. A yellow light indicates a decreasing performance and early warning that a potential problem may arise soon. An orange light is a more severe warning that a problem is very likely to arise. This should be more closely monitored. A red light then indicates a severe problem that needs immediate attention and action. Depending upon the implementation, more or less traffic lights can be adopted. Note that within the context of PD modeling, dark green can also be used to refer to the fact that the risk measure is becoming too conservative. Usually, the traffic lights will be related to the p-values of a statistical test as follows:
•	A p-value less than 0.01 corresponds to a red light.
•	A p-value between 0.01 and 0.05 corresponds to an orange light.
•	A p-value between 0.05 and 0.10 corresponds to a yellow light.
•	A p-value higher than 0.10 corresponds to a green light.
Here you can see an example of a traffic-light-indicator approach applied to backtesting PD models at the calibration level. It can be easily seen that, from 2001 onwards, the calibration is no longer satisfactory, because of the many red lights.
Backtesting PD at Level 0
When validating data stability at Level 0, one should check whether internal or external environmental changes will impact the PD classification model. Examples of external environmental changes are new developments in economic, political, or legal environment, changes in commercial law, or new bankruptcy procedures. Examples of internal environmental changes are changes of business strategy, exploration of new market segments, or changes in organizational structure. A two-step approach can be suggested as follows: 

Step 1: Check whether the population on which the model is currently being used is similar to the population that was used to develop the model. 

Step 2: If differences occur in Step 1, verify the stability of the individual variables. 

For Step 1, a population or system stability index (SSI) can be calculated. This is also called a deviation index in SAS. It is calculated by contrasting the expected or training and observed or actual population percentages across the various score ranges. In other words, it is calculated as the sum across the ranges of (A, from actual, minus T, from training,) times the logarithm of the ratio of A versus T. 

Important to note is that the percentages reported in the table are the percentage of the population and thus not default rates or percentages. In other words, they nicely add up to 100%. Also observe that the system stability index is defined in a similar way as the information value, which we discussed in the lesson on data preprocessing. A rule of thumb can then be defined as follows:
•	if the SSI is less than 0.10 , no significant shift (or green traffic light)
•	if the SSI is between 0.10 and 0.25, moderate shift (yellow traffic light)
•	if the SSI is above 0.25, significant shift (red traffic light)
It is also recommended to monitor the system stability index through time, as illustrated in this table. The bottom two rows contain two SSI values. The first one compares the observed or actual population with the expected or training population. The second one then compares the observed or actual population at time t+1 with the population at time t. This allows to see the evolution of the SSI through time and detect when important changes occur. The same traffic light coding can be used as discussed previously. 

When population instability has been diagnosed, one can then verify the stability of the individual variables. Again, a system stability index can be calculated at the variable level, as illustrated in this table for the variables income and years client. Note that also histograms and/or t tests can be handy tools to diagnose variable instability. 

Characteristic analysis is another way to inspect the stability of a variable distribution. Here you can see a variable Age, which has been categorized into various ranges. Besides only looking at the training and actual distribution, we now also take into account the scorecard points, which have been assigned to each of the ranges. For the range 18 to 24, the difference between the actual and training percentage equals 9%. Multiplied by the score of 10, this gives us 0.9. We can then do likewise for all other variable categories and sum the obtained values. This gives us a total value of −2.63. This represents the average score change between the training and actual population on the Age variable. Unfortunately, no rules of thumb are available here, and it is the expert who will have to decide what traffic light to assign to the result.
 Calculating the System Stability Index
In this demonstration, we will illustrate how to compute the System Stability Index in SAS. 

We start by creating a data set population. It contains three variables: the score range, the percentages of the actual population, and the percentages of the training population observed during development. 

Let's run this data set and inspect it in the work library. Here you can see that the data set has been successfully created. 

We now create another data set SSIdata, which contains all the intermediate results needed to calculate the System Stability Index. The index variable contains, per score range, the product of the difference between both percentages and the logarithm of their ratio. We will now sum these terms using PROC MEANS with the SUM option. 

Let's run these statements and inspect the result. 

We can see that the SSI equals 0.05. Since this is smaller than the reference value of 0.1, it can be concluded that there is no population instability. In other words, a green traffic light can be assigned to this result.


Backtesting PD at Level 1
We are now ready to climb up one level in the credit risk model architecture and validate the application or behavioral scorecard. It is recommended to first have a look at the scorecard itself. For example, what was the logic behind the model used? Were there any assumptions made such as independence or normality? Also important is to verify the sign of the regression coefficients. Are the signs as anticipated? Are there any unexpected signs? Suppose that your scorecard tells you that a higher debt ratio corresponds to a better credit score. This is clearly counter-intuitive and needs to be further investigated since no one will be prepared to use a scorecard with this pattern. It is important to inspect all the p-values, and model significance. Also, the input selection procedure adopted and any remaining multicollinearity issues need to be clarified. Finally, the various data preprocessing activities, such as missing values, outlier handling, and coarse classification, need to be verified. 

Obviously, the discrimination performance should also be considered at Level 1. Two key performance metrics here are the receiver operating characteristic curve and the area underneath, and the cumulative accuracy profile and the accuracy ratio, as discussed earlier. Remember that both are linearly related as follows: accuracy ratio equals 2*AUC−1 . You could also consider to construct confidence intervals for both measures in order to get an idea about their stability. As a reference, the corporate credit risk models tested by Moody's had accuracy ratiosin the range of 50% to 75% for out-of-sample and out-of-time validation tests. 

Here you can see an example of how the area under the ROC curve and accuracy ratio can be coded using traffic lights. If the AUC is below 0.5, or the AR is below 0, then the model cannot find any discrimination. If the AUC is between 0.5 and 0.7, the discrimination is poor. If the AUC is between 0.7 and 0.8, the discrimination is acceptable. If the AUC is between 0.8 and 0.9, the discrimination is excellent, and if it's above 0.9, then the discrimination is exceptional. Note that an AUC of above 0.95 is suspicious and usually a sign that something went wrong in the modeling (for example, a variable, which should not be included since it contains too much information about the default). Obviously, these benchmarks should be interpreted in the right context in terms of type of portfolio, type of application, data quality, etc. Earlier benchmarking studies found that application scores usually have an AUC between 0.7 and 0.8, whereas behavioral scores have a somewhat higher AUC between 0.75 and 0.85. This can be motivated by the fact that behavioral scores are typically built using more variables than application scores. 

Here you can see an example of a table, which can be used to monitor the accuracy ratio. The first row reports the accuracy ratio of the model as it was developed. Note that it is also important to include the number of observations, the number of defaulters, and a corresponding traffic light. This will allow to interpret all numbers from the right perspective. Subsequent rows then represent the accuracy ratio and other numbers for subsequent years. Once a sufficient number of observations have been gathered, averages can be computed as indicated in the last two rows. These averages can then be compared using a Student t test, for example. 

Here you can see an example of this. The accuracy ratio of the model equals 85%. A drop of 5% in accuracy ratio then implies a color change. In other words, between 80% and 85%, the color is green; between 75% and 80%, yellow; and below 75%, red. The number of colors and the definition thereof can be adjusted depending upon the type of portfolio or scorecard. 

The importance of both the AR and the AUC has been stressed by the Basel committee, as you can see in this quote from the BIS 14 Working Paper: 

"The group has found that the Accuracy Ratio (AR) and the ROC measure appear to be more meaningful than the other above-mentioned indices because of their statistical properties." 

The BIS 14 paper then also provides a benchmark range as follows: 

"Practical experience shows that the Accuracy Ratio has tendency to take values in the range 50% and 80%. However, such observations should be interpreted with care as they seem to strongly depend on the composition of the portfolio and the numbers of defaulters in the sample." 

It is however important to note that a benchmark is always relative and depends upon the characteristics of the portfolio, application, data quality, etc. It is also recommended to use other performance measures, besides AUC and/or Gini, as illustrated by this quote: 

"It is difficult and probably inappropriate to rely on a single measure, such as the widely used Gini coefficient." 

Hence, it is advised that banks also consider other add-on performance metrics such as the Kolmogorov-Smirnov index or related Pietra index, entropy measures, Bayesian error rate, and lift curves. Overrides can occur at Level 1 of the credit risk model architecture. An override means that a human expert will overrule the score output by the scorecard. This can be motivated by extra information available to the expert. About overrides, the EU regulation says: 

"For grade and pool assignments, institutions shall document the situations in which human judgment may override the inputs or outputs of the assignment process and the personnel responsible for approving these overrides. Institutions shall document these overrides and note down the personnel responsible. Institutions shall analyze the performance of the exposures whose assignments have been overridden." 

Let's discuss this into a bit more detail. Here, you can see an example of an override report. The score interval has been categorized and the thick black line denotes the cutoff at which the bank operates. All scores below the line should be rejected, and all scores above the line accepted. However, we can clearly see that there are overrides. Some of the ones below the cutoff line have been accepted. These are the low-side overrides and they total to 1 + 2 + 2 + 3 or 8. Also, some of the ones above the line have been rejected. These are the high-side overrides and they total to 2 + 2 + 1 or 5. When Basel says that we should monitor the behavior of the overrides, they are particularly referring to the low side overrides, since these are the ones that end up receiving credit and thus give us credit risk. The high side overrides are not that important for Basel since these are the ones, which are rejected anyway. It would however be nice to monitor the behavior of these customers via the credit bureau to see how these customers behaved at other financial institutions. This will allow us to see whether we were right in making the override decision or not.
Backtesting PD at Level 2
Let's now move to Level 2 of our credit risk model architecture, which is the level of the ratings and the calibration. This is probably the most important level as this will give us the PDs, which will be used to calculate the capital requirements. Key questions, which should be answered here, are:
•	Is there a sufficient number of ratings? Is there a master scale defined?
•	Are the credit characteristics of obligors in the same rating sufficiently homogeneous?
•	Are there enough ratings to allow for an accurate and consistent estimation of default risk per rating?
When backtesting PD at Level 2, you should investigate whether the ratings provide you with a correct ordinal ranking of risk and a correct cardinal measure of risk. In terms of the former, it should be verified whether the default rates are properly ranked through the ratings. In other words, the default rate of rating A should be less than the default rate of rating B, which in turn should be less than the default rate of rating C. In terms of cardinal measures of risk, the calibrated PD should be as close as possible to the realized default rates. 

Various test statistics can be used to compare the estimated PDs to the realized default rates. The most popular are the binomial test, the Hosmer-Lemeshow test, the Vasicek one-factor model, and the normal test. All these tests suffer from a couple of complications, such as an insufficient number of defaults, the fact that defaults are typically correlated, and the issue of choosing an appropriate significance level. Hence, since each statistical test has its shortcomings, they are typically used as early warning indicators. Note that also the impact of the rating philosophy (point-in-time or through-the-cycle) is important to consider. 

Let's first start by defining another performance measure at Level 2, the Brier score. This score measures the mean squared deviation between the calibrated PD (PD[hat]i and a binary indicator θi), which is 1 if the obligor went into default and 0 otherwise. Obviously, the Brier score is always bounded between 0 and 1, and lower values are to be preferred. Note, however, that this score is only occasionally used in the industry.
Binomial Test
A very popular test for backtesting PD calibration is the binomial test. Remember, three key assumptions of a binomial experiment are as follows:
•	It should be an experiment with only two outcomes, success or failure.
•	The experiment should be repeated multiple times.
•	There should be independence between the outcomes of the individual experiments.
Here, two of these requirements are nicely fulfilled. We have only two outcomes, default or non-default, and multiple obligors are considered. Due to the correlation between the default behavior, the independence assumption is not fulfilled. Hence, the binomial test will be used as a heuristic or early-warning indicator. 

The null hypothesis, H0, states that the PD of a rating is correct. The alternative hypothesis can be two sided or one sided. From a regulatory perspective, it is important that capital is not underestimated, so let's make the alternative hypothesis, HA. The PD of the rating is underestimated. 

As already explained, to use the binomial test, we are assuming that the default events are uncorrelated. Given a confidence level, α (for example, 99%), the null hypothesis H0 is rejected if the number of defaulters X in the rating is greater than or equal to k*, which is obtained as follows: It is the minimum k, such that the cumulative probability, as quantified using the binomial distribution of observing between k and n defaulters, is less than or equal to 1−α. 

The central limit theorem can now be used for large n, and when nPD hat is bigger than 5 and n(1−PD hat) is bigger than 5. The number of defaulters X can then be modeled as a normal distribution with expected value nPD hat and variance nPD hat(1−PD hat). We can now look for the k* value such that the probability that (X ≤ k*) equals α. 

Hence, we have P z less than or equal to k*−n times PD hat divided by the square root of n times PD hat times (1−PD hat) equals α, with z following a standard normal distribution. The critical value k* can then be obtained as follows: k* equals N−1 (of α) times the square root of n times PD hat times (1−PD hat) + n times PD hat, with N−1 (of α) the inverse cumulative standard normal distribution. 

In terms of a maximum observed default rate p*, we have p*=N−1 (of α) times the square root of PD hat times (1−PD hat) divided by n + PD hat. 

To summarize, we can reject H0 at significance level α, if the observed default rate is higher than p*. Again, note that the binomial test assumes that defaults are uncorrelated. If default correlation is present, which is more likely, there is a higher probability to erroneously reject H0, which is a Type I error. Hence, it is advised to use the binomial test as an early-warning system and be aware of its limitations. 

Here you can see a plot of the critical value as a function of the number of observations n for a reference PD of 1.5%. Note that the critical value is bigger for a higher confidence level. Also, it decreases with a growing number of observations, which makes it interesting to perform backtests on an aggregated level. 

The Hong Kong Monetary Authority, HKMA, has given some further recommendations about confidence levels to be adopted for the binomial test. More specifically, it stated: 

"For example, if a binomial test is used, AIs can set tolerance limits at confidence levels of 95% and 99.9%. Deviations of the forecast PD from the realized default rates below a confidence level of 95% should not be regarded as significant and remedial actions may not be needed. Deviations at a confidence level higher than 99.9% should be regarded as significant and the PD must be revised upward immediately. Deviations, which are significant at confidence levels between 95% and 99.9%, should be put on a watch list, and upward revisions to the PD should be made if the deviations persist." 

These reference values can then also be used to define the various traffic lights. 

Here you can see an example of an extension to the binomial test, which takes into account default correlation in the denominator. Because of including the correlation, the z-statistic becomes smaller and thus less conservative compared to no correlation. Hence, by ignoring default correlations, a more conservative test is obtained. 

Here you can see the table we discussed earlier. The first row includes the estimated PDs for the various ratings. Subsequent rows then represent the observed default rates. The colors indicate the severity of the difference between the observed and estimated values and are defined using the binomial test. Green means no significant difference at the 90% level. Yellow means a significant difference at the 90% level, but not at the 95% level. Orange means a significant difference at the 95% level, but not at the 99.9% level. Red means a significant difference at the 99.9% level. More or less traffic lights can be used and defined accordingly.
 Performing the Binomial Test
In this demonstration, we will illustrate how to implement the binomial test in SAS. 

We start by creating a data set ratings which contains four variables being the rating, the calibrated PD, the number of observations, and the number of observed defaults. 

The data set binomialtest then performs all necessary calculations. It starts from the ratings data set, calculates the actual default rate, followed by the critical value. The latter is calculated using the formula discussed in the course. Note that it uses the probit function to calculate the inverse cumulative standard normal distribution at the value of 0.99. Let's run both data sets and inspect the results. 

We can see that, for ratings A, C, and D, the actual default rate is below the critical value. However, for rating B, the actual default rate is 5% and above the critical value. Hence, for this rating, it might be considered to calibrate the PD upwards.


Hosmer-Lemeshow Test
A disadvantage of the binomial test is that you need to perform a test for each individual rating. The Hosmer-Lemeshow test can be used to test several ratings simultaneously. As the binomial test, it also assumes independence of defaults. 

The test statistic is defined as follows: It is the sum of i equal 1 to k of (ni times PD[hat]i−θi squared) divided by ni times PD[hat]i times (1−PD[hat]i), whereby ni is the number of obligors with rating i, PD[hat]i is the estimated PD of rating i, θi is the number of observed defaulters with rating i, and k is the number of ratings. It originates by applying a binomial distribution per rating, approximating this by a standard normal distribution, which is then squared to get a chi-square distribution with one degree of freedom, which is then, in turn, summed across all the ratings to get a chi-square distribution with k degrees of freedom for k ratings. So, to summarize, the test statistic T converges toward a chi-square distribution with k degrees of freedom. Just as with the binomial test, the p-value can be computed and represented as a traffic light. Also here, the Type I error is underestimated when correlation is present.
 Performing the Hosmer-Lemeshow Test
In this demonstration, we will illustrate how to implement the Hosmer-Lemeshow test in SAS. 

We start by creating a data set ratings, which contains four variables, being the rating, the calibrated PD, the number of observations, and the number of observed defaults. 

The data set chisquaredtemp builds further upon the ratings data set and creates the chisquared terms that need to be summed. This summation is then done by PROC MEANS resulting in the sum_chisquaredterm variable, which contains the chi-square statistic. Let's run all these statements. We close the results. 

In the HLstatistic data set, we then calculate the p-value for this chi-square statistic. Note that we hereby make use of the CDF function to refer to the cumulative distribution of the chi-square distribution. Also observe that the _FREQ_ variable refers to the degrees of freedom. Let's run this and inspect the data set HLstatistic. We can see that the p-value equals 0.15. This implies that, at a 99% confidence level, we can conclude that there is no significant difference between the observed and estimated PDs. However, when using a 95% confidence level, there is a difference. Using a traffic light indicator approach, we could assign an orange traffic light to this result.



Normal Test
The normal test is a multi-period test of correctness of a default probability forecast for a single rating. The null hypothesis states: None of the true probabilities of default in the years t from 1 to big T is greater than its corresponding forecast. The alternative hypothesis then states the complement thereof, or not H0. H0 will then be rejected at confidence level α if the sum of t going from 1 to big T of (DRt−PD[hat]t) divided by the square root of big T times τ is bigger than zα, whereby big T is the number of time periods considered, DRt is the default rate observed in time period t, PD[hat]t is the probability of default for time period t, τ is a standard error-alike term as you can see defined here, and zα is the standard normal α-percentile. Note that this test is only very occasionally used in the industry.
Vasicek One-Factor Model
None of the tests discussed thus far take into account default correlation. The Vasicek one-factor model is a first test, which will take into account default correlation. It can be applied as a backtesting test at the portfolio level. When discussing the Merton and Vasicek model in the lesson on the Basel Capital requirement formulas, we explained that the percentage of defaults that will not be exceeded at the 99.9% confidence level, α*, can be computed as you can see right here. 

Hence, this α* can now be used to construct our confidence interval. which ranges from 0 to α*. You can clearly see here that the default correlation enters the calculation of α* via the asset correlation factor. You can then use the asset correlation values as reported in the Basel Accord. Remember, for mortgages, the asset correlation was 0.15; whereas for qualifying revolving exposures, it was 0.04. To make the backtesting more conservative, you can also consider to use half of the asset correlations. By halving the asset correlation, the unexpected loss becomes smaller and thus the critical value will also become smaller, hereby more quickly detecting a performance difference. Note that this approach assumes an infinitely granular portfolio. For finite, small samples, Monte Carlo simulation procedures can be adopted.
 Performing the Vasicek One-Factor Test
In this demonstration, we will illustrate how to implement the Vasicek test in SAS. 

We start by creating a data set ratings, which contains four variables, being the rating, the calibrated PD, the number of observations, and the number of observed defaults. 

Remember, to compute the Vasicek test, we first need to specify the asset correlation. Let's assume that we have a portfolio of mortgages and set the asset correlation to 0.15. 

The data set Vasicek then implements the calculations as discussed in the course. Let's run all these statements and inspect the Vasicek data set. We can see that the actual default rate is lower than the critical value for all ratings. Note that the critical values are actually quite high. 

Let's now make the back testing more conservative by halving the asset correlation to 0.075. We run the data set again and inspect it. We can see that this has reduced the critical values, but they are all still higher than the observed default rates for all ratings.


Data Aggregation
As we already mentioned before, data aggregation can be interesting to consider during backtesting. Assume we have a portfolio with N obligors and n ratings. This implies that there are approximately N/n observations per rating. Hence, more ratings make backtesting more difficult, since there will be fewer observations per rating, which will increase the standard errors and consequently the critical values. To improve the significance of the backtesting, data aggregation can be considered. One example would be to merge ratings with a low number of observations (for example, AA+ and AA and AA−) into one overall rating (AA). The aggregation can then also be considered for important segments or even at the overall portfolio level.
Implications of Risk Rating Philosophy for Backtesting
The risk-rating philosophy should also be taken into account during backtesting. Earlier, we introduced the point-in-time and through-the-cycle rating philosophies. PIT ratings take into account both cyclical and non-cyclical information. Hence, the backtesting should find that the realized default rates are close to the forecast PD, or in other words, the PIT PDs should be validated against the 12-month default rates. TTC ratings only consider non-cyclical information and are thus more stable. Hence, backtesting should find that the realized default rates vary around the forecast PD, rising in downturns and falling in upturns, or in other words, the TTC PDs should be validated against cycle average default rates. 

Here you can see a graph depicting the change in portfolio distribution in terms of default ratings. In case of PIT ratings, the change can be due to cyclical effects. In case of TTC ratings, the change can be due to more systematic, non-cyclical changes in the population.
Example Traffic Light Dashboard
Up till now, we have discussed various backtesting performance metrics and statistics to backtest PD models at Level 0, 1, and 2 of the credit risk model architecture. It is important that all these backtesting statistics are now combined in a traffic-light-indicator dashboard. Here you can see an example of this starting at Level 2. You can see that both quantitative tests, as well as qualitative tests, are included. The binomial, Hosmer-Lemeshow, Vasicek, and normal tests are the quantitative tests. Qualitative tests are more subjective and based on expert evaluation. Here we included an inspection of the portfolio distribution, the overall difference between the estimated and realized default rates, and an evaluation of the portfolio stability. Note that the result of each of these tests has been encoded using three traffic lights. 

This can then be continued at Level 1. Here you can see quantitative tests based upon the accuracy ratio, the area under the ROC curve, and the overall model significance. Qualitative checks inspect the data preprocessing activities conducted, the coefficient signs of the scorecard, the number of overrides, and the model documentation available. Again, the outcome of each of these tests is represented as a traffic light. 

Finally, at Level 0, the quantitative tests include the system stability index at both population and attribute level, and a t test at attribute level. Qualitative checks include characteristic analysis and histogram inspection. 

Based upon all these tests, a decision needs to be made whether the PD model is okay or not. Remember, in case there are issues with the PD model, a backtesting action plan should be available to remedy the situation. This plan will specify what to do in response to the finding of the PD backtesting exercise.
Action Schemes
Here you can see an example of an action scheme for PD backtesting. If the model calibration is okay, you can continue to use the PD model, since the capital is appropriately calculated and there is no problem. If the model calibration is not okay, you need to verify the model discrimination or ranking at Level 1. If this is okay, then the solution might be to simply re-calibrate the probabilities upward or downward using a scaling factor. If not, the next step is to check the data stability at Level 0. If the data stability is still okay, you may consider to tweak the model and see whether you can remedy the situation as such. Note that this is, however, not that straightforward and will often boil down to completely re-estimating the PD model, as is the case when the data stability is not okay.
Backtesting LGD and EAD Models
In what follows, we will discuss backtesting LGD and EAD models. Remember, both models are typically developed using similar methodologies. Hence, the backtesting procedures will also be similar. As with PD backtesting, we will here also make use of the multilevel model architecture. At Level 0, we will backtest the stability of the data. At Level 1, we will verify the discrimination, and at Level 2 the calibration. For backtesting the stability of the data, a system stability index can be used, just as with PD. This SSI will then contrast the distribution of the actual population with that of the training population across the various LGD ranges. 

At Level 1, the discrimination can be verified using the R square, the correlation, the mean squared error, the mean absolute deviation, various CAP plots, and corresponding accuracy ratios. All of these were discussed before. You can then come up with a table like the one depicted where the performance measure of the model (in this case, the MSE) is monitored through successive years. To correctly interpret the obtained values, it is also recommended to add the number of defaulters. The traffic light can then be assigned based upon rules of thumb specified by the modeling expert. As with PD, when a sufficient number of values has been obtained, averages per period can be computed. 

Since the theoretical distributions of the MSE, MAD, and R-square measures are unknown, it is not that straightforward to use them in a statistical way for backtesting. One interesting approach could be to apply a bootstrapping procedure. The idea here is to statistically test the difference between a performance measure, P, calculated on the training or model development data set, and the same performance measure, P, calculated on an out-of-time test set. In other words, the null hypothesis H0 becomes Ptest = Ptrain, and the alternative hypothesis then becomes Ptest < Ptrain. The bootstrapping procedure then proceeds as follows:
•	Pool the training and test set observations with the predicted LGD into one larger sample.
•	Draw a training and a test set bootstrap sample with the same size as the original training and test set. Remember, as discussed earlier, a bootstrap sample is a sample with replacement. The same observation can thus occur multiple times in a bootstrap sample.
•	Calculate the difference for P between the bootstrap training and the bootstrap test sample.
•	Repeat 1000 or more times to get the distribution and statistically test whether the difference is 0 or not.
You can also test the error dispersion to verify whether the error distribution is getting wider on the new out-of-time sample compared to the training sample. The null hypothesis then becomes σ2test equals σ2trainversus the alternative hypothesis σ2test is bigger than σ2train. 

This can be statistically evaluated using an F statistic, which compares the error variance in the test sample with the error variance in the train sample. This F statistic follows an F distribution with degrees of freedom equal to ntest−1 and ntrain−1, whereby ntest is the number of observations in the test sample and ntrain is the number of observations in the train sample. 

As an alternative, the Ansari-Bradley test can be used as a nonparametric variant. 

At the calibration level, the idea is to contrast the estimated LGD with the actual LGD. Here you can see a table, which illustrates this. The first row contains the estimated LGD for each LGD rating. Note that we also included a column for the non-rated exposures and an average column. Subsequent rows then indicate the actual LGD for each year. Once a sufficient number of years of data have been collected, averages can be computed for different periods, as illustrated in the bottom two rows. 

A first backtesting test for LGD calibration is a parametric Student t test. This test statistically evaluates the significance of the error, defined as the difference between the LGD observed (LGDobs), and the LGD predicted (LGDpred). The t test then verifies whether the mean out-of-time sample error (µE) equals 0 or not. The null hypothesis then becomes H0: µE = 0 versus the alternative hypothesis, HA: µE > 0. The test statistic is as follows: ē divided by (Se divided by the square root n) should follow a Student's t distribution with n−1 degrees of freedom. Note, ē is the average error; Se, the standard of the error; and n, the number of observations. A p-value can then be computed and represented as a traffic light. 

Also, a nonparametric Wilcoxon signed rank test can be used to test whether the median error equals 0 or not. The null hypothesis then becomes H0: ME=0 versus the alternative hypothesis, HA: ME > 0. This test starts by ranking the absolute value of the nonzero errors in ascending order, whereby the smallest error is assigned rank 1, and so on. r+ is then defined as the sum of the ranks of the positive errors such that the test statistic becomes r+ − (n times n + 1 divided by 4) divided by the square root of (n times (n + 1) times (2n + 1)) divided by 24. This follows a standard normal distribution. Again, a p-value can be computed and represented as a traffic light. 

Also, exposure weighted measures can be used, such as the loss shortfall. This is defined as 1 − the sum for i going from 1 to big N of LGD[hat]i times EADi divided by the sum of i going from 1 to N of LGDitimes EADi, whereby LGD[hat]i is the predicted LGD; LGDi, the actual LGD; and EADi, the actual exposure at default. In other words, it measures how much the loss at default is lower than the predicted loss. It can be thought of as a conservative test verifying whether the loss is not underestimated. In the citation at the bottom, a study was done for Rabobank, whereby the following traffic light coding was suggested:
•	LS > 0 or LS ≤ −0.2: red
•	−0.20 < LS ≤ −0.10: yellow
•	−0.10 < LS ≤ 0: green
Here you can see the mean absolute deviation defined in an exposure weighted way as the sum for i going from 1 to big N of the absolute value of the difference between LGDi and LGD[hat]i multiplied by EADidivided by the sum for i going from 1 to big N of EADi. Traffic lights also suggested in the below reference are
•	MAD ≤ 0.10: green
•	0.10 < MAD ≤ 0.20: yellow
•	0.20 < MAD: red
Benchmarking
Benchmarking is another important quantitative validation activity. The idea here is to compare the output and performance of the analytical PD, LGD, or EAD model with a reference model or benchmark. This is recommended as an extra validity check to make sure that the current credit risk model is the optimal one to be used. Various credit risk measurements can be benchmarked (for example, credit scores, ratings, calibrated risk measurements such as PDs, LGDs, CCFs, or even migration matrices). Various benchmarking partners can be considered. Examples are credit bureaus, rating agencies, data poolers, and even internal experts. As an example of a simple benchmarking exercise, consider benchmarking an application score against a FICO score. 

The benchmark can be externally or internally developed. Various problems arise when doing external benchmarking. A first one is that there is no guarantee that external ratings are necessarily of good quality. Think about what happened during the credit crisis, when many rating agencies were criticized because their ratings turned out to be overly optimistic. Next, the external partner might also have a different portfolio composition and adopt different model development methodologies and/or processes making a comparison less straightforward. Also different rating philosophies might be used, whereby the benchmark rating system is either more point-in-time or through-the-cycle. The default and loss definitions might differ, different LGD weighting schemes can be adopted, different discount factors, collection policies, etc. External benchmarking might also be complicated because of legal constraints whereby, due to banking secrecy regulation, information cannot be exchanged. Credit risk is typically also an endogenous phenomenon, which is highly dependent upon the internal credit culture and/or process. There is also a risk of cherry-picking, whereby a close-match external benchmark is selected without further critically evaluating it. 

Given these complications with external benchmarking, the idea of internal benchmarking has been advocated. It was first introduced by the Hong Kong Monetary Authority (HKMA), as illustrated by this quote: 

"Where a relevant external benchmark is not available (e.g., PD for SME and retail exposures, LGD, and EAD), an AI should develop an internal benchmark. For example, to benchmark against a model-based rating system, an AI might employ internal rating reviewers to re-rate a sample of credit on an expert-judgment basis." 

The internal benchmark can be a statistical or an expert-based benchmark. Consider, for example, a PD model built using a plain, vanilla, logistic regression model. You can then consider building a neural network benchmark, for example. The performance of both the logistic regression and the neural network can then be contrasted. Although the neural network is clearly a black-box model and can thus not be used as the final credit risk model, the result of this benchmarking exercise will tell us whether there are any nonlinear effects in the data. If it turns out that the neural network performs better than the logistic regression, you can then start looking for nonlinear effects or interactions and try and add them to your logistic regression model to further boost its performance. The benchmark can also be expert based. Remember, an expert-based benchmark is a qualitative model based upon expert experience and/or common sense. An example of this could be an expert committee ranking a set of small- and medium-sized enterprises (SMEs) in terms of default risk, by merely inspecting their balance sheet and financial statement information in an expert-based, subjective way. The ranking obtained by an expert-based rating system can then be compared to the ranking obtained by the logistic regression, for example. 

A champion-challenger approach can be used when doing benchmarking. The current model is the champion, which is challenged by the benchmark. If the benchmark beats the champion in performance, then it can become the new champion. This way, models are continuously challenged and further perfected.
Benchmarking Tools
A first, handy tool to benchmark ratings is a rating-difference histogram. This is a graphical representation of the distribution of the rating differences between the internal model and the external benchmark. The closer it is concentrated around 0, the bigger the similarity. In our histogram, a negative difference implies that the bank assigns a higher rating than the benchmark. The numbers can also be represented in a tabular format, as you can also see right here. The rating differences can then be monitored across successive years, and once a sufficient amount of observations have been gathered, averages across periods can be computed. 

Popular agreement statistics for benchmarking are Spearman's rank-order correlation, Kendall's tau, and the Goodman-Kruskal gamma. 

Spearman's rank-order correlation measures the degree to which a monotonic relationship exists between the scores or ratings provided by an internal scoring system and those from a benchmark. It starts by assigning 1 to the lowest score, 2 to the second lowest score, and so on. In case of tied scores, the average is taken. Spearman's rank-order correlation is then the linear Pearson correlation between the ranks and can be approximated, assuming no ties, as follows: 1 − 6 times the sum of i going from 1 to N of d2i divided by n times (n2 − 1), whereby n is the number of observations, and di the difference between the scores. Spearman's rank-order correlation always ranges between -1, which represents perfect disagreement, and +1, which represents perfect agreement. 

Kendall's tau works by first calculating the concordant and discordant pairs of obligors. Two obligors are said to be concordant if the obligor who has a higher score assigned by the internal model, also has a higher score assigned by the benchmark. If there is disagreement in the scores, then the pair is said to be discordant. Note that, if the pair is neither concordant nor discordant, it is tied, meaning the two obligors have identical scores assigned by the internal model, or by the benchmark, or by both. Kendall's tau is then calculated as follows: A − B divided by ½ times n times n − 1, whereby n is the number of obligors; A, the number of concordant pairs; and B, the number of discordant pairs. Note that the denominator gives all possible pairs for n obligors. Kendall's tau is 1 for perfect agreement and -1 for perfect disagreement. 

Kendall's tau basically looks at all possible pairs of observationsobligors. The Goodman-Kruskal gamma will only consider the untied pairs (either concordant or discordant) and is calculated as follows: A − B divided by A + B. 

The Goodman-Kruskal gamma is +1 if there are no discordant pairs (perfect agreement); -1, if there are no concordant pairs (perfect disagreement); and 0, if there are equal numbers of concordant and discordant pairs. 

Here you can see an example of all three agreement statistics. The internal credit score is benchmarked against the external FICO score, which is a well-known credit bureau score in the US ranging between 300 and 850. First, the ranks of both scores are computed. Then, the differences are calculated, squared, and summed. This allows us to calculate the Spearman rank-order correlation, which becomes -0.025 in our example. The number of concordant and discordant pairs is then calculated. C1 and C3 are a concordant pair since both the internal score and the FICO score assign the highest score to C1. C1 and C2 are a discordant pair since the internal score assigns the highest score to C2, whereas the FICO score assigns the highest score to C1. The pair C1 and C5 is a tie since both get the same score by the internal score. Kendall's tau then becomes 0.1, and the Goodman-Kruskal gamma 0.11. 

Here you can see another example of a simple benchmarking exercise, whereby internal ratings are compared with externally obtained ratings. By comparing the rating criteria of its internal rating system with those of Moody's, an institution concludes that 50% of the obligors assigned to its rating grade B would have Moody's ratings Baa1, 25% A3, and 25% Ba1. In the past five years, average annual default rates of these Moody's ratings were 3%, 2%, and 4% respectively. The benchmark PD of rating grade B can then be estimated as follows: 50% x 3% + 25% x 2% + 25% x 4% = 3%. This can then be compared with the internally obtained PD. The comparison can even be further statistically analyzed using a binomial test, for example.
 Performing Benchmarking
In this demonstration, we will illustrate how to do benchmarking in SAS. 

We start by creating a data set ratings containing the ratings of a set of obligors assigned by three different institutions. Let's run this data set and inspect it in the work library. We can see that the data set has been successfully created. 

Before we can compare and benchmark these ratings, we first code them in a numerical way in the data set, ratings2. Rating A is coded as 1, Rating B as 2, Rating C as 3, and Rating D as 4. We also run this data set. We now inspect it in the work library where we can see that also this data set has been successfully created. 

We are now ready to calculate the correlation using PROC CORR. The Spearman and Kendall options indicate that we want the PROC to compute the Spearman rank order correlation and Kendall's tau. We run this statement to compute these correlations between the ratings of institutions 1 and 3. In the output, we can see that Spearman's rank-order correlation equals 0.21. The p-value that comes with it indicates that it is not significantly different from zero. We get similar results for Kendall's tau, which equals about 0.14 with a high corresponding p-value. 

Let's now also compute the correlations between institutions 1 and 2. Here we can see that both Spearman's rank-order correlation and Kendall's tau are high with low corresponding p-values. This indicates a high degree of agreement between institutions 1 and 2. 

We will now also create a histogram displaying the rating differences. First, we create a data set, histo, containing these differences. We run this data set. 

We can now run PROC UNIVARIATE to display the histogram. This histogram shows the differences between institutions 1 and 3. We now also plot the histogram to compare institutions 1 and 2. This histogram confirms the earlier correlation computations by showing a clear agreement.


Qualitative Validation

Introduction to Qualitative Validation
To conclude this lesson, we will also say a few words about qualitative validation. We will hereby consider the following topics: use testing, data quality, model design, documentation, corporate governance, and management oversight.
Use Testing
The idea of use testing is to use the IRB models and estimates not only for Basel capital calculation, but also for other business activities such as credit pricing, credit approval, economic capital calculations, and others. This can be illustrated with the following regulatory articles:<
•	"Internal ratings and default and loss estimates must play an essential role in the credit approval, risk management, internal capital allocations, and corporate governance functions of banks using the IRB approach." (par. 444, Basel II Accord; Art. 144 EU)
•	"The systems and processes used by a bank for risk-based capital purposes must be consistent with the bank's internal risk management processes and management information reporting systems." (Federal Register)
•	"When institutions use different estimates for the calculation of risk weights and for internal purposes, it shall be documented and be reasonable." (Art. 179, EU)
So, to summarize, the IRB estimates must play an essential role in other business activities. This, however, does not mean an exclusive role. Earlier, the FSA put forward three conditions that should be satisfied in order to meet the use test requirement.
•	Consistency: The information the IRB estimates (PD, LGD, and EAD) are based on should be consistent with internal lending standards and policies.
•	Use of all relevant information: Any relevant information used in internal lending standards and policies must also be used in calculating the IRB estimates.
•	Disclosure: If differences exist between the calculation of the IRB estimates and those used for internal purposes, then they must be documented and the reasonableness demonstrated.
Here you can see some examples of use-test issues. For application scoring, many firms use a time window of 18 months. Remember however, that for PD, Basel requires a time window of only 12 months. Also, the Basel default definition is 90 days, whereas some financial institutions like to use their own definitions of a bad payer. Most regulators will tolerate these differences, as long as they are properly documented and the reasonableness thereof demonstrated.

Another issue concerns the use of downturn LGD for other business activities. This is often perceived to be too conservative for other applications. About this, the FSA mentioned earlier:

"Firms can use different LGDs for business purposes to those used for regulation and not fail the use test, provided that the rationale for their use and differences/transformation to capital numbers is understood."

Hence, average LGD values can be used for economic capital calculation, IFRS provisions, and other accounting applications.
Data Quality
Data is the key ingredient to any credit risk model, be it a PD, LGD, or EAD model. Hence, it speaks for itself that to have good models, data should be of high quality. About this, the regulators said:<
•	"The institution shall have in place a process for vetting data inputs into the model, which includes an assessment of the accuracy, completeness and appropriateness of the data." (Art. 173, EU)
•	"The data used to build the model shall be representative of the population of the institution's actual obligors or exposures." (Art. 173, EU)
•	"The PRA expects a firm to set standards for data quality, aim to improve them over time and measure its performance against those standards." (section 10, PRA)
Data quality can be measured in various ways. A first important dimension is accuracy. The aim here is to verify if the inputs measure what they are supposed to measure. Earlier on, the FSA introduced the data accuracy scorecard to measure this. Bad data accuracy can be caused by data entry errors, measurement errors, and outliers.

Another important dimension is data completeness. Observations with missing values can only be removed if sound justifications can be given. About this, the CEBS mentioned earlier:

"While missing data for some fields or records may be inevitable, institutions should attempt to minimize their occurrence and aim to reduce them over time."

Data timeliness refers to the recency of the data. Data should be updated at least annually, although higher updating frequencies are recommended for the riskier obligors. Data should also be appropriate in the sense that there should be no biases or unjustified data truncation. Data should also be appropriately and unambiguously defined. As an example, consider the definition of a ratio variable, commonly used in corporate credit risk models. Ratios are defined as a numerator divided by a denominator. Both should be clearly defined. It should also be mentioned what happens to the ratio when the denominator equals zero. Missing values should also be defined in an unambiguous way and not coded as zero, as is often the case.

To summarize, it is very important that financial institutions set up master data management and data governance initiatives. This not only applies to internally collected, but also to externally obtained data. Here you can see some of the results we obtained by conducting a worldwide survey with more than 50 banks on the topic of data quality. Note that the focus of the survey was on credit risk analytics. The main findings were:
•	Most banks indicated that between 10% to 20% of their data suffer from data quality problems. This was quite surprising to us.
•	Manual data entry is one of the key problems together with the diversity of data sources and the consistent corporate-wide data representation.
•	Regulatory compliance is the key motive to improve data quality, rather than strategic or competitive advantage, for example.
For further information about our study, we refer to the citation below. Please also see the Information button for the various data quality dimensions we introduced earlier.
Model Design
A next qualitative validation activity relates to model design. Some example questions that need to be answered here are:
•	When was the model designed and by whom?
•	What is the perimeter of the model in terms of counterparty types, geographical region, industry sectors? For example, the model was developed using Belgian SMEs active in the agricultural sector.
•	What are the strengths and weaknesses of the model?
•	What data was used to build the model? How was the sample constructed? What is the time horizon of the sample? Which default definition was adopted?
•	Is human judgment used and how?
Being able to adequately answer all these questions is very important to correctly use the model and facilitate model maintenance. It is important that all of this is appropriately documented.
Documentation of the Rating System
All steps of the credit risk model development and monitoring process should be adequately documented. This can be illustrated by means of the following regulatory quotes:
•	"All material elements of the internal models and the modeling process and validation shall be documented." (Art. 188; EU)
•	"Documentation should be transparent and comprehensive." (Federal Register)
•	"Documentation should encompass, but is not limited to, the internal risk rating and segmentation systems, risk parameter quantification processes, data collection and maintenance processes, and model design, assumptions, and validation results." 
Documentation is needed both for internally developed as well as externally purchased models. It is advised to use document management systems with appropriate versioning facilities to keep track of the different versions of the documents. An ambitious goal here is to aim for a documentation test, which verifies whether a newly hired analytical team could use the existing documentation to continue development or production of the existing analytical PD, LGD, and EAD models.
Corporate Governance and Management Oversight
A final qualitative validation issue concerns corporate governance and management oversight. The idea here is to pursue active involvement of the board of directors and senior management in the implementation and validation process of the various credit risk models. About this, the EU regulation mentioned:

"All material aspects of the rating and estimation processes shall be approved by the institution's management body or a designated committee thereof and senior management. These parties shall possess a general understanding of the rating systems of the institution and detailed comprehension of its associated management reports." (Art. 189, EU)

In other words, senior management should demonstrate active involvement on an on-going basis, assign clear responsibilities, and put into place organizational procedures and policies that will allow the proper and sound implementation and validation of the IRB systems. The outcome of the validation exercise must also be communicated to senior management, and if needed, accompanied by appropriate response.





 

