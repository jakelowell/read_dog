---
title: copulas
author: ''
date: '2018-06-04'
slug: copulas
categories: []
tags:
  - credit
---



<p>When I started writing the paper [19] in 2003 a Google search of the word “copula” gave 10,000 responses. In September 2005 the same search gives 650,000 responses. There is an explosion of activity. What is going on? Many of the web-sites1 found in the Google search are related to mathematical finance, statistics, extreme value theory, and risk management. Everybody who opens any journal on stochastic processes, probability theory, statistics, econometrics, risk management, finance, insurance, etc., observes that there is a fast growing industry on copulas. The commercial statistics software Splus provides the module FinMetrics that includes copula fitting which is written by R. Carmona, see also Carmona [4]. One can also get copula modules in other major software packages (R, Mathematica, MatLab, etc.). The International Actuarial Association [14] in its hefty paper on Solvency II 2 recommends using copulas for modeling dependence in insurance portfolios. Moody’s uses Gaussian copulas for modeling credit risk and provides software for it which is used in many financial institutions. Since Basle II 3 copulas are now standard tools in credit risk management. The main purpose of this paper is to ask some na¨ive questions about the fast ascent of copula technology that has become so fashionable. My main concern is that this very simple concept might be something like the emperor’s new clothes because it promises to solve all problems of stochastic dependence but it falls short in achieving the goal. Although I do appreciate that practitioners, in contrast to academic researchers, have to come up with solutions to their risk problems within deadlines and that “quick and dirty methods” cannot always be avoided. Yet one may of course ask how much safety the banking and insurance industry (and maybe the rest of the world) really gains by using the copula concept. I have been watching my colleagues for some years and have been wondering why more and more of them became immersed in copulas without discussing the pros and cons of the concept. I suspect that some include the word “copula” in the title of their papers not because they contribute to the theory on copulas, but because they believe that one can publish easier. Some have adopted the language of copulas which has led them to publish papers with complicated technical assumptions, whereas the results are not new when considered in the usual language of distributions. I also Date: November 25, 2005. † Mikosch’s research is partially supported by the Danish Research Council (SNF) Grant No 21-04-0400. This is a discussion paper which was initiated at the 4th International Conference on Extreme Value Analysis in Gothenburg, 15–19 August, 2005; see <a href="http://www.math.ku.dk/~mikosch/maphysto" class="uri">http://www.math.ku.dk/~mikosch/maphysto</a> extremes 2005/extremes. It will appear in the journal Extremes together with a discussion. 1Of course, the word “copula” is also used in rather distinct non-mathematical contexts. 2Solvency II will be a treaty for insurers similar to the Basle I and II treaties for banks. The European Commission writes about Solvency II: It contains a fundamental and wide-ranging review of the current regime in the light of current development in insurance, risk management, finance techniques, financial reporting, etc. One of the key objectives of Solvency II is to establish a solvency system that is better matched to the true risks of an insurance company. 3See the website www.bis.org/publ/bcbsca.htm of the Bank for International Settlements for a guide to credit and market risk. 1 2 T. MIKOSCH observed that my students are likely to be attracted to copulas than to stochastic processes. A possible reason is that one needs less than 10 minutes to understand the fundamentals of copulas, but many years of studies in order to get an idea of a genuine stochastic process. I got the impression that some of those who use copulas or write about them think that all the world’s problems related to stochastic dependence and multivariate distributions can be solved via copulas. It is my primary objective to caution this optimism. A second goal is to indicate that there are statistical problems in handling copulas, one of them being the curse of dimensionality. A third aim is to indicate that copulas do not really fit into the theory of stochastic processes and time series analysis. This is a pity because a new strong idea (if it is one) should inherit the main body of a well established theory in which some of the finest minds of probability theory have been working for about 100 years. I am aware that it is difficult to interfere with a development which is already driving at a very high speed, but an academic person without commercial interests may ask questions which do not make all people in the community happy. In what follows, I will ask questions about copulas and what one gains scientifically if one uses them. 2. What is a copula? We start with a real-valued random variable X with continuous distribution function F which we also assume to have an inverse F -1 on its support. It is easily checked that X d= F -1 (U) where U is uniformly distributed on (0, 1) and F(X) d= U and d= refers to equality in distribution. Now consider an R d -valued random vector X = (X1, . . . , Xd) with respective marginal distribution functions Fi which are again assumed to be continuous and have inverse on the support of Xi . 4 Then the same arguments as in the one-dimensional case yield P(X1 = F -1 1 (x1), . . . , Xd = F -1 d (xd)) = P(F1(X1) = x1, . . . , Fd(Xd) = xd) = C(x1, . . . , xd), where on the right-hand side the d-dimensional distribution function C has support [0, 1]d and uniform marginal distribution functions. The distribution function C is the copula of the vector X. On the other hand, given the copula C of X we can reconstruct the distribution function of X by the distribution functions Fi : P(X1 = y1, . . . , Xd = yd) = P(F1(X1) = F1(y1), . . . , Fd(Xd) = Fd(yd)) = C(F1(y1), . . . , Fd(yd)). Finally, any distribution function with support on [0, 1]d and uniform marginals is called a copula. Under the given assumptions on the marginal distribution functions Fi , there is a 1-to-1 correspondence between the copula C and the distribution of X. This means, in a sense, that the dependence structure of X1, . . . , Xd can be reconstructed from the copula and the marginal distributions Fi . But this does not mean that the dependence structures of the vectors X = (X1, . . . , Xd) with copula C and of the vector Y = (F1(X1), . . . , Fd(Xd)) with distribution function C are the same. For example, if the correlations of the vector X are defined, they will usually be different from those of Y. To illustrate this fact more drastically, assume that X1, . . . , Xd is a sample from a second order stationary process with zero autocorrelations. The corresponding sample F1(X1), . . . , Fd(Xd) can be highly correlated and will in general not be second order stationary. Similarly, the spectral measure (which is an extremal dependence measure, see Section 8.2) can make sense for X with unbounded support, but it does not for Y. 4These assumptions can be weakened but this paper does not aim to discuss the most general setting. See Nelsen [21] for an introduction to general copulas and their properties. COPULAS: TALES AND FACTS 3 3. Why did copulas become popular in risk management? One of the driving forces for the popularity of copulas is their application in the context of fi- nancial risk management. Standard theory in mathematical finance prescribes Gaussian processes among others to log-prices and interest rates.5 This is due to the dominating role of Brownian motion and martingales in models of mathematical finance. The distribution of a Gaussian process is determined by its finite-dimensional distributions, which are Gaussian and, in turn, are completely characterized by their mean values and covariances. In particular, a linear combination of the components of a mean zero Gaussian vector is Gaussian and its distribution is determined purely by its covariance matrix and the weights of the linear combination. This is convenient because in risk management one is often interested in linear combinations of prices or log-prices from different assets which constitute a portfolio. Quantities such as Value at Risk (VaR — a low quantile of the Profit and Loss distribution) can be calculated if one knows the covariance matrix and the number of shares one holds in the various assets. Unfortunately, real-life data often turn out to be highly non-Gaussian, see for example Chapter 6 in Embrechts et al. [5] or Mikosch [18] for some evidence of Pareto like tails for returns. Returns (the increments of log-prices over successive periods of time with equal length, such as days) have heavy-tailed distributions, are dependent through time and may be non-stationary. There is no simple alternative to the Gaussian distribution in the non-Gaussian world. In particular, one needs multivariate models for portfolios with different marginal distributions (including different tail behavior) and a dependence structure which is determined not only by covariances. Many of the well known multivariate distributions are not flexible enough to allow for different tail behavior in different components. Therefore copulas seem to be the right tools in order to overcome the mentioned difficulties: they generate all multivariate distributions with flexible marginals in the way described in Section 2. So far so good. But at least at this point any critical person might have some questions. For example the following one. 4. Why does one transform the marginals to a uniform distribution? The author has not found a scientific answer to this question. But my optimistic guess is that this transformation is related to the fact that one can visualize data from a two-dimensional copula on a computer screen which has some similarity with the unit square [0, 1]2 . Such two-dimensional visualizations have become rather standard as an exploratory tool. Another explanation might be that the transformation F -1 is well known as the quantile function of F and that the relationship between the uniform distribution, F and F -1 is part of any advanced probability course, hence very familiar. However, what does such a visualization tell or not tell us? It would be useful if one could interpret the dependence structure of vectors with uniform marginal distributions. But there are infinitely many such dependencies and to the best of my knowledge there is no way of classifying or quantifying them. It is even hard to imagine what dependence of two uniformly distributed random variables means. Hence: There is no particular mathematical or practical reason for transforming the marginals to the uniform distribution on (0, 1). With the same right, one could transform the marginal distributions of a vector to any other nice continuous distribution function. For example, again assuming X to have a continuous distribution function F, the transformation - log(1-F(X)) yields a standard exponentially distributed random 5For example, the celebrated Black-Scholes model assumes a Brownian motion; the popular Vasicek interest rate model requires an Ornstein-Uhlenbeck process. 4 T. MIKOSCH variable: P(- log(1 - F(X)) = x) = P(1 - e -x = F(X)) = 1 - e -x , x &gt; 0 . Or one can transform the marginal distributions to standard normal ones. Then one will in general deal with a multivariate distribution which is not Gaussian. Such examples are sometimes considered in a course on probability theory when one wants to explain the fact that the (Gaussian) marginals do not determine a multivariate (Gaussian) distribution. A multivariate non-Gaussian distribution with Gaussian marginal distributions is usually considered pathological and not of much practical use. But such a distribution corresponds to a copula. Would this copula not be “pathological” as well? Later, in Section 7.2, we will discuss Gaussian and t-copulas which are generated from multivariate Gaussian and t-distributions, respectively. One wonders why, in this context, one does not transform the marginals to Gaussian or t-distributions. In contrast to the corresponding copulas the characteristics of these distributions have a straightforward interpretation. 5. Why does one transform the marginals to a standard distribution? In various situations one can apply increasing transformations to the data without destroying certain aspects of the underlying dependence structure. This can be advantageous if one gets simpler representations or more accessible formulas for certain probabilistic quantities. Multivariate extreme value theory is in part responsible for this approach. Standard multivariate extreme value theory deals with the vector of component-wise maxima of a sample of iid random vectors and its limit distribution under affine transformations.6 If the components of these vectors come from different distributions, the question about the limit distribution of a vector of component-wise maxima can be rather unpleasant. For example, different marginal distributions require different normalizations or, if one chooses the normalization from a dominating component, the remaining components might vanish in the limit. Therefore it has become rather common to assume that one transforms the components to a standard distribution.7 Those include the uniform distribution, but also the Pareto, the unit Fr´echet distribution F1(x) = e -1/x , x &gt; 0, and the Gumbel distribution ?(x) = exp{e -x}, x ? R. The advantage of choosing the Pareto or the unit Fr´echet distributions is that they allow for an interpretation of the spectral measure, i.e., the distribution of the directions of multivariate extremes (see Section 8.2) given one believes that, after the transformation to these standard marginal distributions, the vector has a distribution in the maximum domain of attraction (MDA) of a suitable multivariate extreme value distribution (EVD).8 Increasing transformations of the marginals make sense in the context of multivariate extreme value theory. For example, consider the iid R 2 -valued vectors (Xi , Yi) and transform the first and second components by increasing functions f1 and f2, respectively. Then the components of the transformed vectors (f1(Xi), f2(Yi)) have the same ordering as before, and with the back transformations f -1 1 and f -1 2 one can reconstruct the distribution of the original vectors (Xi , Yi). Of course, apart from keeping the order of the components unchanged by these monotone transformations, one destroys many other aspects of the dependence structure of the points (Xi , Yi). 6This is what the layman thinks about multivariate extreme value theory and he is right when he reads most of the literature, including textbooks. But component-wise maxima of a multivariate sample give a rather limited picture of the world of multivariate extremes. In my view, it is more important that the theory provides maximum domain of attraction (MDA) conditions which justify the assignment of probabilities to multivariate rare events which, possibly, have not happened before. 7This approach is advocated in Resnick’s monograph [24], where he lays the probabilistic foundations of multivariate extreme value theory. It is done by an easy probabilistic argument, but when dealing with data this is hard statistical work because one does not know the marginal distributions; see e.g. de Haan and de Ronde [11] and the literature cited therein. The problem is the same when one wants to verify the goodness-of-fit of a copula. 8As most conditions in extreme value statistics these are statistical believes which can hardly be tested on data. COPULAS: TALES AND FACTS 5 For many other functions of random vectors increasing transformations of the components do not really help one to evaluate the distribution of this function. For example, when one is interested in VaR, see Section 3, i.e., in a certain quantile of a linear combination of the components of a random vector, increasing transformations of those components are not useful: the copula of the vector does not help one to evaluate VaR. However, if one believes in a particular copula C and in particular marginal distributions Fi , then one can possibly simulate iid copies from the underlying distribution and, in turn, approximate the distribution of the function by crude Monte Carlo. This method has attracted some attention in risk management because it gives one a convenient Monte Carlo method for determining the VaR of a portfolio for a flexible number of shares (portfolio weights). Of course, this Monte Carlo method also works if one simulates directly from a multivariate distribution without knowing its copula. Copulas do not solve the problem of simulating from an arbitrary multivariate distribution. Since one does not understand all dependence structures of a vector with values in [0, 1]d and uniform marginals, one cannot simulate from any copula C. Most copulas from which one can simulate are derived from well known multivariate distributions such as the Gaussian or elliptical distributions (see Section 7.2) and then one can simulate directly from these distributions. The crude Monte Carlo method for simulating the quantile VaR is a rather expensive one. One has to simulate from the whole multivariate distribution in order to calculate the quantile of a 1-dimensional object — a linear combination of the components of the vector. One may wonder whether there do not exist theoretical means for approximating the VaR for a sufficiently large class of high-dimensional distributions without knowledge of the distribution in its whole domain. Or formulated as a question: Given a high-dimensional random vector, which part of its support is responsible for the far out upper/lower tail of a linear combination of its components? For elliptical distributions (see Section 7.2) one has explicit formulas for VaR, depending only on the parameters of the distributions. This fact contributed to the popularity of these distributions. In a way, the distribution of a linear combination of a high-dimensional vector of prices and of a composite stock index (such as the Dow Jones, S&amp;P 500, DAX, etc.) are similar objects; composite stock indices are weighted averages of a large number of speculative prices. Over the last 20 years econometrics has quite successfully developed time series models for returns of speculative prices and composite stock indices such as the ARCH family and stochastic volatility models. This development was crowned by two Nobel Prizes for Economics in 2003. Weighted averages of prices have their own dynamics and lead to interesting space-time dependence structures. If one believes in the feasibility of the copula approach one might wonder whether it would yield a better approximation to the marginal distribution of the returns of a composite stock index than what is obtained by fitting a time series model. To the best of my knowledge such a comparison has not been provided. It is not only the ordering of a sample that is preserved under increasing transformations. Various quantities of interest are invariant under increasing transformations. Among those are some quantities which depend only on 2-dimensional distributions and which are considered as some kind of dependence measure: the tail dependence parameter (see Section 8.1), the concordance measures Kendall’s t and Spearman’s ? (see Nelsen [21], Section 5.1). One says that two points (a, b) and (c, d) in R 2 are concordant if (a-c)(b-d) &gt; 0 and discordant otherwise. If one transforms the first or second coordinates by increasing functions, the resulting points remain concordant or discordant. Since t and ? measure the degree of concordance of a 2-dimensional random vector they remain unchanged under increasing transformations of its components and only depend on the copula. The mentioned concordance measures do not characterize the dependence of a 2-dimensional vector with uniform marginals; they describe a very particular geometric property of an iid or strictly stationary ergodic sequence of 2-dimensional random vectors. The rank correlations t and ? are often mentioned in the literature on risk management as alternative measures to covariances 6 T. MIKOSCH and correlations. They are not relevant for measuring dependence and risk because they do not depend on the magnitude of the data and therefore they neglect large and small values. In the history of probability theory a whole universe of dependence measures of random vectors, stochastic processes and their distributions has been developed which describe very different aspects of stochastic dependence. Among them is the classical theory of probability metrics and distances as presented in the work of Zolotarev and summarized in his monograph [27]. As a matter of fact, there has always been some interest in a probabilistic theory of distributions with fixed marginals; for recent contributions see the work by Rusc ¨ hendorf and his co-authors. 6. How does one fit the marginal distributions and the copula? In classical statistics one fits a multivariate distribution by extracting information out of the data about the distribution as an entity, e.g. by using maximum likelihood for a multivariate parametric family of distributions. The copula technology is different since it suggests the possibility of a two stage statistical procedure: estimate the marginal distributions and the copula function separately from each other. By the choice of the marginal distributions one determines the copula, hence the chosen dependence structure, and therefore different statistical tools for fitting the marginals may generate distinct dependencies. In most parts of the existing literature the question about the fit of the marginals is not discussed. An exception is Section 5.5 in McNeil et al. [17] where three possible approaches are mentioned: (1) Fit parametric distributions to the marginals. (2) Replace the marginal distributions Fi by their empirical distributions F (n) i . (3) Replace the marginal distributions Fi by their empirical counterparts F (n) i and add some tails, e.g. generalized Pareto distribution (GPD) tails from above or below a certain threshold. From 1-dimensional statistical theory it is known that any of these approaches can go terribly wrong. For example, if one wants to estimate high or low quantiles the empirical distribution can be a poor guide, even when the sample size is large. If one estimates the marginal distributions by its empirical counterparts the estimation of the copula is entirely based on the relative ranks of the components: the data can be multiplied or divided by 1050 and the statistical analysis will be the same. Can the estimation of the dependence structure only be a matter of relative ranks? From a 1-dimensional extreme value point of view the approximation of the tails of the marginal distributions by GPDs might be appropriate if one is interested in an extreme value problem, but the estimation of the copula depends on all data, not just on the extremes, even if the copula comes from an EVD. Any of these approaches will only work under particular mathematical conditions. None of these methods can be trusted unless a rigorous mathematical proof is given. There exists very little statistical theory about fitting multivariate data by using copulas and about their goodness-of-fit; most papers focus on some particular 2-dimensional copulas (Archimedean, Gaussian, t-copula, etc.); see e.g. Genest and Rivest [9] or Song et al. [26]. For statistical applications it is not enough to prove that maximum likelihood estimation works for some parametric families of copulas and marginal distributions. A major problem is the goodness-of-fit of a copula which critically depends on the fit of the marginal distributions. In financial applications copulas are often used for highdimensional data (e.g. Moody’s credit risk models), and it is not clear what is the trade-off between the sample size and the dimension. The number of parameters might be larger than the sample size. Of course, the same problem occurs if one fits a Gaussian distribution in a na¨ive way to two years of return data, a sample of 500 points say, to a portfolio which consists of 200 assets, i.e., the return data are 200-dimensional. It is hard to believe that any copula technique will sweep away these statistical problems. COPULAS: TALES AND FACTS 7 The separation of marginal distributions and dependence function when using distributions generated from a copula implies a much higher statistical uncertainty than usually encountered when fitting a distribution. Instead of fitting one multivariate distribution as an entity to the data, one has to fit all marginal distributions as well as the copula by using one sample. This increases the usual estimation efforts by far. It is unclear how sensitive the estimation techniques for the copula are to the estimation of the marginal distributions. It is also unclear how sensitive the estimation of the distribution of the multivariate data is to the estimation of the marginal distributions and the copula. Here is a natural question: Given the fitted marginal distributions and copulas are “close” in some sense to their counterparts in the distribution of the data, how “close” is the estimated distribution (generated by the fitted marginal distributions and estimated copula) to the distribution of the data? 7. How does one choose a copula? The copula C of a random vector X is a transformation of the distribution of X. As for the distribution of X we have infinitely many choices for C. If one chooses a copula it should be related to the problem at hand. For example, if we are interested in multivariate extremes the copula should be related to multivariate extreme value theory; see Section 8 for some discussion in this context. In the literature various families of copula families with a name have been introduced. Their choice is not always based on reasoning but on mathematical convenience. In what follows, we comment on some of the popular copula families. 7.1. The Archimedean copula. This copula has attracted a lot of attention, see e.g. Nelsen [21], Chapter 4. In its simplest form it is defined as follows: let ? : [0, 1] ? [0,8] be a continuous decreasing function such that ?(0) = 8 and ?(1) = 0. Moreover, for d &gt; 2 the function ? -1 on [0,8] also has to be completely comonotonic.9 Then the following function on [0, 1]d is a copula, see Nelsen [21], Theorem 4.6.2 on p. 122: C(x1, . . . , xd) = ? -1 X d i=1 ?(xi) ! . Calculation yields P(X1 = x1, . . . , Xd = xd) = C(F1(x1), . . . , Fd(xd)) = ? -1 Xn i=1 ?(Fi(xi))! = ? -1 (- log [P(Y1 = x1)· · · P(Yd = xd)]) , where Y = (Y1, . . . , Yd) is a vector with independent components with distribution functions P(Yi = x) = e -?(Fi(x)) , i = 1, . . . , d . This means that the distribution function of X is a monotone transformation of the distribution function of a vector Y with independent components. The Archimedean copula is a textbook toy example where one can explain various theoretical properties of a copula and calculate quantities such as the rank correlations. However, in view of the above calculation one may ask: In which practical situation would one choose an Archimedean copula for modeling genuine dependence in the vector X? 9? -1 is completely monotonic on [0, 8] if (-1)k d k? -1 (t)/dtk = 0 for all t ? (0, 8) and k = 0, 1, 2, . . .; see Nelsen [21], p. 122. 8 T. MIKOSCH 7.2. The Gaussian, t- and other copulas of elliptical distributions. The Gaussian copula is perhaps the most popular distribution in applications. It is simply derived from a multivariate Gaussian distribution function FS with mean zero and correlation matrix S by transforming the marginals by the inverse of the standard normal distribution function F: C(x1, . . . , xd) = FS(F-1 (x1), . . . , F -1 (xd)). The t-copula is another popular model. It is derived in the same way as the Gaussian copula. Given a multivariate centered t-distribution function tS,? with correlation matrix S, ? degrees of freedom and with marginal distribution function t?, this copula is given by C(x1, . . . , xd) = tS,?(t -1 ? (x1), . . . ,t -1 ? (xd)). The Gaussian and t-copulas are copulas of elliptical distributions; they are not elliptical distributions themselves. A vector X with an elliptical distribution in R d satisfies the identity in law X d= R A S, where S is uniformly distributed on the unit sphere {x : |x| = 1} with respect to the Euclidean norm | · |, R = 0 is a radial random variable, independent of S, AA0 = S and S is a correlation matrix. One may wonder how flexible and reasonable elliptical distributions and copulas are for the purposes of risk modeling. The multiplicative shock R is present in all components of X with the same magnitude. This would also imply that any observation Xi from such a distribution would be equally important for statistical analyses, independently of the order of magnitude of |Xi |. Such an approach is in contrast to the usual reasoning in extreme value statistics, where one first separates the data in the center of the distribution from those far away from zero and then conducts distinct statistical analyses in the different parts of the sample. The vector AS determines the likelihood of the directions of X. Since S is uniformly distributed on the unit sphere of Rd , the distribution of AS is also smoothly distributed in space; in particular extremes of an iid sample X1 = X, . . . , Xn may occur in all directions whereas pronounced directions for extremes are excluded in these models. The dependence in elliptical distributions is essentially determined by covariances. Covariances are rather poor tools for describing dependence for non-Gaussian distributions, in particular for their extremal dependence; see Embrechts et al. [6] for a critique of using covariances in risk modeling and Glasserman [7] for advocating t-distributions for risk management. 7.3. Extreme value copulas. An extreme value copula is derived from a multivariate EVD by transforming its marginals to the unit cube [0, 1]n . Multivariate EVDs occur as weak limits of affinely transformed vectors of component-wise maxima of iid random variables; see Resnick [24], Chapter 5. The marginal distributions are necessarily one-dimensional EVDs: they are either of the type Fr´echet, Gumbel or Weibull. A popular extreme value copula is the Gumbel copula C(x1, . . . , xd) = exp ( - X d i=1 (- log xi) 1/a!a) , for some a ? (0, 1]. Writing ?(x) = exp{-e -x}, x ? R, for the Gumbel distribution function, the corresponding EVD is obtained by the transformation C(?(x1), . . . ,?(xd)) = exp ( - X d i=1 e -xi/a!a) . Other named (parametric) multivariate EVDs are given for example in Galambos [8], Chapter 5, or in Kotz and Nadarajah [15], Chapter 3. However, there exist infinitely many distinct multivariate EVDs with Gumbel, Fr´echet or Weibull marginal distributions and therefore it is not quite clear why the Gumbel copula in particular has gained such popularity; it may perhaps be because it is COPULAS: TALES AND FACTS 9 also an Archimedean copula. As explained above, such copulas describe a rather limited kind of dependence structure of a random vector. Extreme value copulas are not EVDs themselves, i.e., one loses the interpretation of an EVD as a max-stable distribution. EVDs should be fitted to data only if the data are generated by an “extremal mechanism”, i.e., if they can be interpreted as component-wise maxima of iid random vectors. In the one-dimensional case, the method of annual maxima (see e.g. Embrechts et al. [5], p. 317) is known to create such maxima. The method straightforwardly extends to higher dimensions. However, in the context of risk management it is less likely that, for example, daily return data of speculative prices are generated by an extremal mechanism, i.e., it is difficult to interpret them as maxima. For example, the use of extreme value copulas for the calculation of functionals such as VaR is out of context. 8. Upper tail dependence and multivariate extremes If one browses on the Internet and searches for papers on copulas, one often finds arguments for discriminating between different distributions based on the value of the upper tail dependence parameter. For example, “the t-copula allows for upper tail dependence whereas the Gaussian copula does not and therefore the t-copula is better for modeling dependence of multivariate extremes” or “the upper tail parameter of the Gumbel copula is smaller than that of the t-copula and therefore the t-copula models stronger extremal dependence”. In what follows, we will shed some light on these statements. 8.1. The tail dependence parameter. We start by defining the upper tail dependence parameter:10 given a 2-dimensional vector (X, Y ) with X d= Y and right endpoint xF of the underlying distribution, assume that the following limit exists ? = lim u?xF P(Y &gt; u | X &gt; u). The parameter ? is called the upper tail dependence parameter. If ? &gt; 0 then X and Y are upper tail dependent and asymptotically independent otherwise. If X and Y do not have identical distributions, one can also define ? via the copula C of (X, Y ): ? = lim u?1 1 - 2u + C(u, u) 1 - u = 1 + lim u?1 C(u, u) - u 1 - u (8.1) , although it is not clear what one gains if one does this. In what follows, we will assume that X d= Y . A sufficient condition for the existence of ? is regular variation of (f(X), f(Y )) for some increasing transformation f; see Section 8.2 for a description and Resnick [23, 24, 25] for a rigorous definition of multivariate regular variation. It can be relaxed to the weaker notion of hidden regular variation, see Heffernan and Resnick [12], Maulik and Resnick [16]. The quantity ? lies between zero (e.g. when X and Y are independent) and 1 (e.g. when X = Y ). It is is not very informative as regards the joint extremal behavior of the vector (X, Y ). In particular, it is restricted to the 2-dimensional case. Of course, in certain circumstances one can calculate the dependence parameters ?ij in a pairwise fashion between the components Xi and Xj of the vector X = (X1, . . . , Xd), but these numbers do not determine the (extremal) dependence structure of the vector; for any set of upper tail dependence parameters one can find infinitely many vectors with the same parameters ?ij . 11 Since the tail dependence parameter can be expressed only by the copula without knowing the marginals, see (8.1), the value ? is not a genuine extreme value concept which would have to take into account both the magnitude and the direction of extremes. 10The lower tail dependence parameter limu?0 P(Y = u | X = u) is defined in an analogous way. 11Assuming a multivariate regularly varying vector X, this claim follows from the representation (8.4) of ?. 10 T. MIKOSCH 8.2. The maximum domain of attraction (MDA) of the multivariate Fr´echet distribution. The upper tail dependence parameter describes only one aspect of the problem of extremal dependence, and it is restricted to the 2-dimensional case. By its definition, the parameter ? measures the relative deviation of the probabilities P(X &gt; u, Y &gt; u) = P((X, Y ) ? u (1,8] 2 ) from the probability P(X &gt; u) = P((X, Y ) ? u (1,8] × [0,8]) for large u. For an R d -valued vector X it is much more informative to consider the relative deviation of the probability P(X ? u A) from the probability P(X ? u B) for any two sets A and B bounded away from zero and for large u. This includes the 2-dimensional case with A = (1,8] 2 , B = (1,8] × [0,8]. If B is fixed — it is common to choose the set (8.2) B = {x : |x| &gt; 1} for a given norm | · |; a reasonable assumption (given the components of X have unbounded support) for conducting extreme value analysis is to require that the limits ?B(A) = limu?8 P(X ? uA) P(X ? uB) (8.3) exist for smooth12 sets A bounded away from zero and are non-zero for some A 6= B. The quantities ?B(A) constitute a measure ?B on the Borel sets of (R ? {8, -8}) d{0} and one necessarily has ?B(tA) = t -a?B(A), t &gt; 0, for some a &gt; 0, see Hult and Lindskog [13]. The condition (8.3) is referred to as (multivariate) regular variation. It is slightly more general than the condition of the same name used as the MDA condition for multivariate EVDs with Fr´echet marginals Fa(x) = e -x -a , x &gt; 0, where one assumes that X has non-negative components; see Resnick [23, 24, 25]. For the 2-dimensional vector (X, Y ) the upper tail dependence parameter ? is obtained by specifying A = (1,8] 2 and B = (1,8] × [0,8], hence ? = ?(1,8]×[0,8] ((1,8] 2 (8.4) ). In contrast to the measure ?B, this number gives a rather limited view at 2-dimensional extremes which are required to assume values in (u,8] 2 for large u. For example, the existence of the limit ? &gt; 0 does not imply that multivariate extremes have a limit distribution under affine transformations, i.e., in general the distribution of (X, Y ) is not in the MDA of a 2-dimensional EVD because the considered events {(X, Y ) ? (u,8] 2} for large u contain rather restricted information about the location of extremes in R 2 . 13 If we specify the set B as in (8.2), a measure theoretic argument shows that multivariate regular variation is equivalent to the existence of the limits t -aG(S) = limu?8 P(|X| &gt; tu , X/|X| ? S) P(|X| &gt; u) , t &gt; 0 , for smooth14 sets S ? {s : |s| = 1} and a probability distribution G on {s : |s| = 1}. This distribution is called the spectral measure of X. It has interpretation as the limit distribution of the direction X/|X| of X, conditional on |X| &gt; u as u ? 8. Relation (8.3) offers a na¨ive means for approximating probabilities of rare events. For example, for positive t and an iid or weakly dependent stationary sample X1 = X, . . . , Xn, P(X ? tuA) ~ t -a?B(A) P(|X| &gt; u) ˜ t -ab ?bB(A) n -1Xn i=1 I(u,8) (|Xi (8.5) |). 12This means that ?B(?A) = 0. 13This means that the sets (u, 8] 2 do not generate the vague convergence relation (8.3) on the Borel s-field of [0, 8] 2 {0}. 14This means that G(?S) = 0. COPULAS: TALES AND FACTS 11 The right-hand expressions in (8.5) are estimators of the corresponding deterministic quantities. This simple-minded approach is similar to the GPD approximation by peaks over thresholds in the 1-dimensional case. In addition to the 1-dimensional difficulties (trade-off between bias and variance of the estimators depending on the threshold u) the complexity of estimation in higher dimensions increases enormously since one also has to estimate the measure ?B (or the corresponding spectral measure). 8.3. Are copulas suitable for modeling multivariate extremes? Copulas generate any multivariate distribution. If one wants to make an honest analysis of multivariate extremes the distributions used should be related to extreme value theory in some way. In Section 7.3 we have briefly discussed extreme value copulas which correspond to multivariate EVDs. They are not appropriate, unless one believes that the underlying data are generated by some extremal mechanism. For financial or insurance data this assumption is questionable. An alternative approach may be based on distributions in the MDA of an EVD. Then the data are not necessarily created by an extremal mechanism; see Section 8.2. Such an approach requires estimation of the measure ?B or of the spectral measure G. These are not easy tasks, but they reflect the difficulty of multivariate analyses in general. In particular, large sample sizes are required. Statistical analyses have been performed and statistical theory has been provided in the 2- and 3-dimensional cases; see de Haan and Resnick [10], de Haan and de Ronde [11], Einmahl et al. [22]. Applications of the statistical theory for copulas (see e.g. Genest and Rivest [9] or Song et al. [26]) are also restricted to low-dimensional settings. The statistical problems mentioned will not be swept away by using copulas. In some parts of the literature the notion of copula domain of attraction has been coined, see e.g. p. 315 in McNeil et al. [17]. Assume Y1, . . . , Yn is an iid sample whose common distribution is a copula C. Write Mn for the vector of component-wise maxima of those vectors and Cn for the copula of Mn. Then C is in the copula domain of attraction of some extreme value copula Ce if Cn(x1, . . . , xd) = C n (x 1/n 1 , . . . , x 1/n d ) converges weakly to Ce (Cn w? Ce). This notion is redundant since it is equivalent to the concept of the MDA of a multivariate EVD. To see this, transform the marginals of C e.g. to unit Fr´echet ones and denote the resulting multivariate distribution by F. Then Cn w? Ce is equivalent to the fact that F is in the MDA of a multivariate EVD with unit Fr´echet marginals; see Galambos [8], Theorem 5.2.3. There is no reason to introduce copula domains of attraction; they only create theoretical confusion and one does not gain new information about multivariate extremes. 9. How do copulas fit into the models of time series analysis and the theory of stochastic processes? Consider one of the standard time series models such as FARIMA, linear processes, GARCH, etc.; see Brockwell and Davis [2, 3] and Mikosch [18] for definitions and properties of these models. Then it is in general impossible to say anything about the distribution of the lagged vector (X1, . . . , Xn). Well known exceptions are linear processes with Gaussian or infinite variance stable innovations. In general we do not even know the marginal distributions of a GARCH or an ARMA process and it can even be hard work to find asymptotic expressions for the tails of these distributions. In the case of regularly varying tails for linear processes (including ARMA) under the weakest possible conditions, see Mikosch and Samorodnitsky [20], and Basrak et al. [1] for power law tails of GARCH processes. The same statement can be made about any standard stochastic process such as Markov process, martingale, diffusion, L´evy process, random measure, any interesting process which has been 12 T. MIKOSCH considered in the history of stochastic processes. Copulas completely fail15 in describing complex space-time dependence structures. Their focus is on spatial dependence and the related statistics (mostly parametric maximum likelihood) are aimed at iid data. It is contradictory that in risk management, where one observes a lot of dependence through time, copulas are applied most frequently. 10. Do copulas avoid the curse of dimensionality? Since a copula is only a reformulation of the original distributional problem one cannot expect that a copula does better than any other high-dimensional model. Copulas will usually even increase the number of parameters to be fitted. For example, for the t-distribution one needs to estimate the underlying correlation matrix S and the degrees of freedom ? &gt; 0; see Section 7.2. For a model generated by the t-copula one needs in addition to estimate the parameters of the underlying marginal distributions. If one has deeper insight into the underlying dependence structure of the data one can try to reduce the dimension d of a parametric model (such as the Gaussian copula) by assuming functional dependencies between the parameters (e.g. the covariances), or one might want to use dimension reduction techniques. The problem of reducing dimensions for distributions related to multivariate extreme value theory is an important unsolved problem: why should one deal with the extremes in all components of a multivariate sample if some of the components dominate the others? 11. The copula fashion — the emperor’s new clothes? Here are some final remarks which summarize my opinion about copulas. • There is no particular advantage of using copulas when dealing with multivariate distributions. Instead one can and should use any multivariate distribution which is suited to the problem at hand and which can be treated by statistical techniques. • The marginal distributions and the copula of a multivariate distribution are inextricably linked. The main selling point of the copula technology — separation of the copula (dependence function) from the marginal distributions — leads to a biased view of stochastic dependence, in particular when one fits a model to the data. • Various copula models (Archimedean, t-, Gaussian, elliptical, extreme value) are mostly chosen because they are mathematically convenient; the rationale for their applications is murky. • Copulas are considered as an alternative to Gaussian models in a non-Gaussian world. Since copulas generate any distribution the class is too big to be understood and to be useful. • There is little statistical theory for copulas. Sensitivity studies of estimation procedures and goodness-of-fit tests for copulas are unknown. It is unclear whether a good fit of the copula of the data yields a good fit to the distribution of the data. • Copulas do not contribute to a better understanding of multivariate extremes. • Copulas do not fit into the existing framework of stochastic processes and time series analysis; they are essentially static models and are not useful for modeling dependence through time.</p>
