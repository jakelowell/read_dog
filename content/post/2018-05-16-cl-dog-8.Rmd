---
title: cl dog 8
author: ''
date: '2018-05-16'
slug: cl-dog-8
categories: []
tags: []
---

Lesson Overview

Introduction
Now that we have created our application and behavioral credit scoring models, we can continue to Level 2 of our credit risk model architecture by defining the default ratings and calibrating the PD. This is the final step in the PD modeling process. It is, however, an essential step, since the PDs will directly be input into the Basel Capital requirements formula to calculate the necessary buffer capital.

In what follows, we will provide an overview of various methods to define default ratings. We will then discuss rating migration analysis to study the volatility of our rating system. Based upon this, we will define the rating philosophy as either point-in-time or through-the-cycle. We will conclude by reviewing various methods for PD calibration.



Objectives

In this lesson, you learn to do the following:
•	describe the following methods of defining ratings: mapping onto a rating agency scale, regression trees, directly optimizing the objective function, and exponential risk evolution
•	describe rating migration analysis
•	define a rating philosophy
•	describe the procedures for PD calibration
•	Text Version	Collapse  Print
Defining Default Ratings

Understanding Ratings
Remember, ratings are defined at Level 2 of the credit risk model architecture as we introduced it previously. A rating is defined as a homogeneous pool of obligors, which are similar in terms of default risk. Note that, depending upon the geographical location, ratings might also be referred to as grades, segments, or pools. An obvious question is, "Why do we define ratings and why don't we work directly with the underlying application or behavioral credit scores instead?" Well, the reason for defining ratings is that the credit scores are too fine granular, or in other words, too much detailed. When working directly with the credit scores, a slight change in customer characteristics might give a different score, and hence, different capital. This would make the capital very volatile, which is clearly undesirable. Ratings, on the other hand, provide an ordinal and more stable measure of credit risk, hereby also giving a more stable capital.

In case of logistic regression, which is the most commonly used classification technique for both application and behavioral scoring, the problem of creating ratings boils down to categorizing the log odds, which is linear in the variables, as we discussed earlier. We will refer to the log odds as the z-score in what follows.

Here you can see the problem visualized. The X axis represents the continuous z-score, which needs to be categorized into various ratings, ranging from AAA (the best rating) to D (the worst rating), for example. In the figure, you can see how the z-score is categorized to define ratings BBB, BB, and B. Every rating will then come with a calibrated PD, as we will discuss later.

When defining the ratings, it is important that the performance of the underlying application or behavioral scorecard remains as much as possible unaffected. Hence, the discrimination, as measured by the area under the ROC curve, should be as similar as possible between the original scorecard and the ratings. Basically, the ratings should approximate the ROC curve piecewise linearly as closely as possible. To measure the quality of the approximation, you can calculate the difference between the original AUC and the AUC of the piecewise linear approximation provided by the ratings.

Here you can see two illustrations of piecewise linear approximations of ROC curves. The figure above shows an example of a good approximation with almost no difference between the original ROC curve and the piecewise linear approximation. In the figure below, the piecewise linear approximation is clearly inferior to the original ROC curve.
Methods for Defining Ratings
Common approaches to define ratings are: map onto an agency rating scale, classification trees, directly optimizing an objective function, and enforcing an exponential risk evolution. We will discuss those into more detail in what follows.

Rating agencies, such as Moody's, Standard & Poor's, and Fitch, have provided commonly used rating scales with accompanying one or multi-year default rates. These rating scales and default rates can now be used to define the internal ratings. In other words, you make sure that your internal best rating corresponds as closely as possible to the rating agency's best rating in terms of default rate, your second best internal rating to the second best agency's rating in terms of default rate, and so on. Although it is quite unlikely to obtain a perfect match, a close enough approximation could be satisfactory.

Another approach to define ratings is by using classification trees. Let's assume we start from the following data set with the z-scores, which are the log odds of the logistic regression, remember, and a binary defaulter/non-defaulter status variable. We can now construct a classification tree, which uses the z-score as its input and the binary defaulter/non-defaulter variable as its target. A problem that could occur here is the occurrence of non-monotone effects in the tree. Remember, the ratings should be monotonically related to the z-score, such that a lower z-score corresponds to a lower rating.

Here, you can see an example of a classification tree. There is clearly a monotonicity problem, because the default rate is non-monotonically increasing as you go down the tree. More specifically, from a default rate of 0.22, it goes up to 0.62 and then back down to 0.60. This is clearly undesirable and should be avoided.

Here, you can see another example of a classification tree without monotonicity violations. The default rate increases monotonically as the z-score becomes smaller.
The monotonicity constraint needs to be enforced during the tree construction process. This can be done either during the splitting decision by avoiding non-monotone splits, or during the stopping decision by not splitting a node that leads to a violation of the monotonicity property.

Another way of creating ratings is by directly optimizing an objective function. Remember, when defining ratings, it is important that the performance or area under the ROC curve remains, as much as possible, unaffected. The aim here is to define an objective function, which is the difference between the AUC of the original scorecard and the AUC of the ratings. You can then start this procedure as follows: Specify a given number of ratings, say between 7 and 15. Put some random cutoffs on the z-score range. You can also start by distributing them uniformly across the z-score range. This represents our first set of ratings. Calculate the value of the objective function. Change the cutoff values so as to minimize the objective function. This can be done manually, expert-based, or using sophisticated techniques, such as genetic algorithms or simulated annealing. To check the robustness of the final solution, it is recommended to try out different starting cutoff values.

Finally, the ratings can also be defined by imposing an exponential relationship between the default rates and the ratings, as is also the case for the rating scales of the rating agencies. A popular example here is to define the ratings in such a way that the default rates double per rating decrease, as you can see illustrated in the figure.
Rating Migration Analysis
Once the ratings have been defined, you can start further analyzing them. One important type of analysis here is rating migration analysis. The purpose is to see how obligors migrate from one rating to another during a specific period of time (for example, 12 months). Usually, this is represented as a migration matrix as you can see depicted. In the first column, you can see the From rating. In the first row, you can see the To rating. For example, 91.30% of the customers that had rating AAA-AA previously kept their AAA-AA rating in the next period. 5.71% of the customers that were previously rated as A got downgraded to BBB. 7.02% of the customers that were previously rated as BBB got upgraded to A. Above diagonal elements represent downgrades and below diagonal elements represent upgrades. Note that the numbers in a particular row always sum to 1. This is obvious, since a customer can either keep the same rating, downgrade, or upgrade. Also note that many migration matrices are diagonally dominant, which means that many obligors tend to keep their rating.

We can now use the idea of Markov chains to do simulations. Remember, the Markov assumption states that the state of the system at time t only depends upon the state of the system at time t- 1. Translated to our setting, this implies that the rating at time t only depends upon the rating at time t-1. The default state can be considered as an absorbing state. So, once in default, you cannot get out of it. Suppose now that we start with 1000 obligors with the highest rating, AAA-AA. We can represent this as a row vector P as depicted. Given the Markov assumption, the distribution after one period of time can then be found by using simple matrix multiplication of P times the migration matrix M. In our example, we can see that, after one period of time, 913 obligors are in rating AAA-AA, 56 in rating A, 11 in rating BBB, 10 in rating BB, and so on.

The distribution after two periods of time can then be found by multiplying the previously obtained vector again by the migration matrix M. In other words, an n-period migration matrix can be easily obtained by multiplying the migration matrix M n times by itself, which is the same as M raised to the power n. Extensions to this basic setting, such as the mover-stayer model, have been introduced, whereby a further distinction can be made between the stayers, which never change rating, and the movers, which do change rating. Note that all these simulations depend upon the Markov assumption, which has been questioned in the literature as empirical evidence seems to suggest that downgrades tend to be more easily followed by further downgrades, which is the idea of autocorrelation over time, or in this case, a negative spiral effect. A duration dependence effect has also been reported, whereby the longer an obligor keeps the same rating, the lower the migration probability. Finally, migration probabilities also tend to be correlated with the business cycle.
Rating Philosophy
The rating philosophy is a very important concept and refers to the assignment horizon of a borrower rating system. It needs to be clearly articulated in the bank's rating policy. As indicated in the US regulation for wholesale exposures, a bank's rating policy must describe its rating philosophy and how quickly obligors are expected to migrate from one rating to another in response to economic cycles. In other words, the rating philosophy can be analyzed using the migration matrix.

Broadly speaking, two rating philosophies can be distinguished. Point-in-time, or PIT, ratings are ratings that take into account both cyclical and non-cyclical information. Hence, PIT ratings are very volatile and change rapidly with the macroeconomic situation, hereby giving lots of rating mobility. The PD that comes with each of the PIT ratings is the best estimate of the obligor's default during the next 12 months. Through-the-cycle, or TTC, ratings are ratings which only take into account non-cyclical information. Hence, the rating is very robust and rather unaffected by temporary macroeconomic shocks, hereby giving very limited rating mobility. The PD that comes with each of the TTC ratings is the best estimate of the obligor's default during a credit cycle. Note that both point-in-time and through-the-cycle ratings represent the stylized extremes of a continuum. Usually, a bank will have a hybrid-rating philosophy situated somewhere along this continuum, either closer to the PIT or closer to the TTC end.

Here you can see a further comparison between point-in-time and through-the-cycle ratings, as provided by the Prudential Regulation Authority in the UK. When entering an economic downturn, PIT ratings will get downgraded, whereas TTC ratings will remain unaffected. For PIT ratings, both the actual default rate and the PD for a rating remain unchanged. For TTC ratings, the default rate for a rating will increase, whereas the PD of the rating remains unchanged. Hence, when using PIT ratings, the downgrade will increase capital requirement, whereas when using TTC ratings, the capital requirements will remain unchanged since there is no downgrade.
Rating Mobility
Rating mobility analysis can be performed to determine whether your rating system is more point-in-time or through-the-cycle. The most extreme example of a through-the-cycle rating migration matrix is the identity matrix. Remember, the identity matrix has ones on the diagonal and zeros for all off-diagonal elements. Hence, this represents a rating system with no migrations or mobility at all. In other words, you can now contrast your migration matrix with the identity matrix and quantify the difference using a mobility metric. The higher the mobility metric, the more your migration matrix is different from the identity matrix, and thus the more point-in-time your rating system is. Here you can see two examples of mobility metrics, based upon the absolute and squared difference of the migration probabilities, respectively. You can now compute these metrics for each of your rating systems. It is, however, hard to come up with benchmarks for these metrics. You could compare the values obtained across your internal portfolios, but also with external benchmarks, such as the migration matrices provided by the rating agencies. The latter are supposed to be examples of through-the-cycle rating systems.

Calibrating PD

Understanding Calibration
Once the ratings have been obtained and analyzed, you can start doing the PD calibration. The aim here is to assign a PD to each rating using long-term historical data. The calibrated PDs should provide a cardinal measure of credit risk. Calibration tries to deal with three key sources of default rate variability. The first one is sampling variability, which is the variability caused by working with limited historical samples. The second one is the external economic conditions, which are continuously changing with macroeconomic upturns being followed by macroeconomic downturns and vice versa. Also, internal or endogenous effects will cause fluctuation of default rates. Think of changes in portfolio composition due to mergers and acquisitions, adoption of new strategies, changing customer behavior, etc. The Basel Accord requires the availability of at least five years of historical data to appropriately calibrate the PD. Note that you are not required to assign equal weight to each of those years. So, if you think older data is less relevant, you can assign a lower weight to it. Finally, many banks will calibrate the PD by adding a conservative add-on to their best estimate PD. This is to compensate for the different sources of variation and take into account model risk, which is the risk caused by model imperfection.
PD Calibration Procedures
When calculating the historical default rates, you can either use the uncorrected cohort or withdrawal corrected cohort approach. The uncorrected cohort approach simply starts from all customers who are still at risk of the beginning of the year t, say Nt. It then calculates the number of customers from Nt that defaulted during the year t, which is Dt. The default rate is then simply the ratio of Dt to Nt. This calculation is then made for each rating separately. The withdrawal corrected cohort approach takes into account withdrawals such as matured loans. It assumes that withdrawals occur uniformly during the year t. Hence, the average number of customers at risk equals Nt−0.5*Nw, with Nw being the number of withdrawals. This is then used as the denominator when calculating the default rate. 

Once the historical default rates have been calculated for each rating, you can start doing the calibration. One simple method to do calibration is to compute the historical average of the default rates within a rating. A more conservative method would be to use the maximally observed historical default rate, or an upper percentile of the distribution. You could also use a binomial confidence interval to account for sample variability as depicted. Note that N−1 represents the cumulative standard normal distribution, α the significance level, and Nobs the number of observations on which the default rate has been computed. 

A more sophisticated calibration procedure would be to link the default rates to the macroeconomy using time series models such as ARIMA and GARCH. Popular macroeconomic variables, which could be useful, are the gross domestic product (GDP), inflation, and unemployment. Also lagged effects can be considered whereby the impact of a variable measured at time t extends to subsequent time periods beyond time t. Note that this approach requires sufficient historical information in order to be able to properly estimate the time series models. Once you have these models estimated, you can not only use them for calibration, but also for stress testing, as we will discuss later. 

To summarize, here you can see a plot of the historical default rates for a particular rating B. Based upon the calibration methods discussed earlier and your future expectations, you can opt to calibrate according to the downturn scenario, which is the red curve, the upturn scenario, which is the green curve, or the intermediate scenario, which is the yellow curve. 

Finally, as a closing note on calibration, I would like to briefly discuss the variable scalar approach, which was introduced by the UK banks. Typically, banks only have data for a limited time horizon, which means the corresponding ratings and PDs will have a strong point-in-time character. Scalars allow to convert point-in-time to through-the-cycle estimates, hereby making the capital more stable. Consider the following simplified example. Let's say that you can calculate a long-run average portfolio default rate of 8% using, for example, external data and/or econometric models. Let's now assume that the recent portfolio default rate equals 2%. The scalar is then the long-run default rate divided by the recent default rate, or 8% divided by 2%, which equals 4. This scalar can then be used to adjust the default rates of the rating, hereby making sure that the final portfolio level PD equals the portfolio long-run average. Note that a scalar can be determined for each rating separately.



