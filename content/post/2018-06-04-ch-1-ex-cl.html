---
title: ch 1 ex cl
author: ''
date: '2018-06-04'
slug: ch-1-ex-cl
categories: []
tags:
  - credit
---



<p>Hi there, welcome to our first video lesson. The aim of this lesson is to give a proper definition of credit risk. Why do we need it? I am pretty sure that every one of you has his or her own definition of credit risk, and this is the point: we need to agree on one single definition. One definition that we can share when we approach credit risk, when we model credit risk. And I am also sure that for you credit risk is not just a synonym of default risk, because we know that this is naive. Credit risk is made up of many different sub- risks. How many? Two, three, ten? We will see that. We will see that these sub-risks are, in reality, the true objects we approach and we model, that credit risk as a whole is a combination of different aspects. Credit risk is one of the main risks under the Basel framework, and it is time for us to define it. Ok, let’s go. Citing the Basel Committee on Banking Supervision, “Credit risk is most simply defined as the potential that a bank borrower or counterparty will fail to meet its obligations in accordance with agreed terms.” And they also add that “the effective management of credit risk is a critical component of a comprehensive approach to risk management and essential to the long-term success of any banking organisation.” Having this very general definition in mind, let’s try to be a little more specific. In the very nice book of McNeil and friends, we can read “Credit risk is the risk of a loss arising from the failure of a counterparty to honor its contractual obligations. This subsumes both default risk and downgrade risk.” Very nice. We are better specifying what credit risk is for us. Let’s see in further detail. Default risk is rather simple from a conceptual point of view. It is the risk connected with the default of the counterparty, which may declare bankruptcy or go into liquidation, and so on. Associated to default risk, we have the so-called probability of default, and the typical loss is equal to the product of the exposure at default and the loss given default (LGD). Downgrade risk, more correctly known as migration risk, is the risk related to a deterioration in the counterparty’s creditworthiness. The term downgrade risk is mostly used for counterparties with external credit ratings issued by some rating agency. In migration risk, we deal with the possibility for the counterparty to migrate from one risk class to another, a worse one typically. The worst the new class, the larger the reduction in the value of the credit exposure. But is this really enough? No, the answer is no. Credit risk is also made up of other sub-risks. In particular spread risk and recovery risk are very important. Spread risk is the risk associated with a rise in the spreads required by the market for borrowers. If risk aversion increases on the market, the spread associated to a given probability of default may increase. This will decrease the asset value of the relative securities, without any explicit reduction of the issuer’s credit rating. Recovery risk is the risk that the recovery rate actually recorded after the liquidation of the insolvent counterparty’s assets will be smaller than the original estimate. We will see that this may happen because of our overestimation of the actual liquidation value, or because of a recovery process taking more time than what was expected. Recovery risk is strictly connected, as we will see, to the estimation of the loss given default. Please notice that all the previous sub-risks are strictly interconnected and, in the real world, they affect one another. So, just to summarize: for us credit risk is essentially made up of four different components, four different sub-risks. Default risk, migration risk, spread risk and, last but not least, recovery risk. I said essentially, because we could continue our list of sub-risks constituting credit risk - and this is cacophony - by introducing substitution risk and country risk. Now, substitution risk is related to OTC derivatives, when our counterparty becomes insolvent before the maturity of the contract, and this requires us to replace it, on the market, at new market conditions, potentially generating losses. Country risk is the risk related to a counterparty that is a non-resident counterparty, a counterparty in another country, and it is the risk related to the impossibility of our counterparty to meet its obligations, because of events of political and legislative nature. We will not cover these two sub-risks in detail.</p>
<p>The Standardized Approach (also Standardised, or STA as an acronym) is the simplest approach to credit risk under the Basel framework. Here we are not considering it in detail, as we prefer to focus on the IRB ones, more interesting from the risk management point of view.</p>
<p>In the Standardized approach, the main goal is to compute RWA as the weighted sum of on-balance and off-balance sheet items, weighted using some specific risk weights. These weights are provided by the Regulator.</p>
<p>You can find some basic information about the Standardized Approach here. A relevant (even if probably boring) reading is this one.</p>
<p>We can expect novelties in the Standardized Approach, but also in the IRBs, quite soon. We can quote a recent speech of Mr Stefan Ingves, Chairman of the Basel Committee and Governor of Sveriges Riksbank.</p>
<p>“I will start by discussing the work on standardised approaches. Standardised approaches facilitate the comparability of banks’ capital ratios. But the crisis highlighted a range of shortcomings with the existing approaches. The Committee is working on revising the standardised approaches across the regulatory framework to enhance their robustness and risk sensitivity. This includes revisions to the following standardised approaches:</p>
<p>Credit risk: The Committee consulted on proposals to revise the standardised approach for credit risk in December 2014, and will consult on revised proposals by the end of the year. Market risk: The Committee’s fundamental review of the trading book, which will be finalised by the end of 2015, will include a revised standardised approach that is sufficiently risk sensitive to act as a credible fallback to the modelled approach. Operational risk: The Committee consulted last year on revising the standardised approach for operational risk. The Committee is considering changes to its proposal and will consult on a revised standardised approach by the end of the year.</p>
<p>Let me now say a few words about internal models. As I mentioned earlier, the use of internally modelled approaches was a defining feature of Basel II. But even at the time when Basel II was still under development, the Committee had concerns about the reliability and robustness of models. For example, let me read the following excerpt from a speech by one of my predecessors, Chairman Tom de Swaan, in 1998:</p>
<p>“There are still serious obstacles… Firstly, credit risk models come with substantial statistical and conceptual difficulties. To mention just a few: credit data are sparse, correlations cannot be easily observed, credit returns are skewed and… backtesting in order to assess a model’s output may not be feasible. Clearly, there are model risks here.”</p>
<p>Since then, ample evidence has accumulated to suggest that the current role of internal models in the regulatory framework does not strike the right balance between simplicity, comparability and risk sensitivity.</p>
<p>The Committee will publish proposals around the end of the year related to the use of models. In some cases, the proposals will remove internally modelled approaches for some risk categories. One example is operational risk, where most would agree that the benefits of the Advanced Measurement Approaches are not proportionate to the related costs and complexity. In other cases, the proposals will consist of introducing additional constraints to internally modelled approaches. More detail on the Committee’s thinking in these areas will come in due course.&quot;</p>
<p>The text of the full speech can be found here. Something will surely happen in Basel during this course. If major changes in regulations appear, we will have the possibility of discussing them together.</p>
<p>Internal-rating based (IRB) approaches are more sophisticated than the standardized approach, and they require more work and attention. Capital requirements, as obtained from IRB methods, are generally lower than those given by the standardized approach. This is due to a more accurate - and less conservative - computation of the RWA.</p>
<p>Under the IRB approaches, the value of the RWA is obtained using a set of formulas, which we will consider later on during the course. However, we can already state some basic facts.</p>
<p>Under the IRB approaches, risk-weighted assets are computed using 2 different elements:</p>
<p>Risk parameters Things like the Probability of Default (PD), the Exposure at Default (EAD), the Loss Given Default (LGD), and the so-called Maturity (M). Risk-weight functions These are functions defined in the Basel II-III Accords, and they are meant to compute the RWA given the risk parameters of the previous point. An example of risk-weight function is the following</p>
<p>As we will see, these functions derive from certain modeling choices, like the Gaussian copulas. As you already know, there are two main approaches in the IRB family:</p>
<p>The Foundation approach, or F-IRB In the F-IRB, banks can compute the probabilities of default (PD) of their counterparties, using the methods they prefer (provided the Regulator agrees). The formulas to compute all the other risk parameters are provided by the national Regulator, together with the risk-weight functions for RWA, according to the rules of Basel II and III. The Advanced approach, or A-IRB In the A-IRB, banks are - in practice - free to calculate all their risk parameters using internal proprietary models. The only condition is that computations respect some minimum guidelines, which are set by the Regulator. The Regulator also checks the statistical soundness of the proposed models. Not all banks are allowed to use the IRB approaches. In fact, in order to be granted the possibility of using one of the two approaches, a bank needs to satisfy some minimum requirements. Minimum requirements may vary from country to country, but they share some common elements. The most relevant ones include:</p>
<p>Composition A bank using IRB methods must guarantee that its estimates of the risk parameters are able to reflect and respect the characteristics of each counterparty. In simple words this means that risk parameters must differentiate among the different types of clients. The risk related to a retail customer cannot be assessed as the risk of a corporate client. Risk parameters should encourage risk diversification, and they should be consistent with their use in risk management decisions. From a statistical point of view, this could be read as a suggestion to use coherent measures of risk (and then the VaR?!). Compliance When adopting any IRB approach, banks are supposed to demonstrate ongoing compliance with all the minimum requirements. In the case in which, at a certain time, a bank does not satisfy one (or more) of the minimum requirements, for whatever reason, it is obliged to inform the national regulator, proposing a plan about the strategies the bank intends to implement in order to return to compliance. If compliance is not respected, the regulator can impose penalties. Risk Rating Design The requirement of rating design is related to a set of statistical and technological rules, meant to guarantee the quality of the estimation of the risk parameters. Banks have the possibility to use many different models and ratings systems for the distinct exposures, but the methodology according to which an exposure is assigned to a particular rating system must be logical and documented. Notice: a bank should not make use of a given rating system, only because it minimizes regulatory capital requirements. In other words, a bank always has to justify its choice with sound modeling arguments. Corporate Governance In any bank using IRB methods, the rating system has to be approved by the board of directors, who need to regularly check all the management reports that are written as part of the implemented rating systems. In order to ameliorate the risk management performances of a bank, its senior management should often review the used rating system, identifying those areas needing improvement. Disclosure In order to be authorized to the use of IRB approaches, a bank must necessarily comply with the disclosure requirements of the third pillar (market discipline) of Basel II. These requirements include transparency, due diligence, etc.</p>
<p>We have given a definition of credit risk, on which we can now all agree.</p>
<p>It is therefore the time to briefly define all the main quantities we will play with during the course. Nice guys like the EAD, the PD, the LGD, Maturity, etc. These quantities are called Risk Parameters, under the Basel Framework.</p>
<p>Quantities that, depending on the modeling approach we choose, can be assessed and computed in different ways.</p>
<p>Why are we interested in credit risk?</p>
<p>Naturally because we want to be able to quantify the possible credit losses, so that we can intervene. This requires us to distinguish between expected and unexpected losses; “what we think we will lose on average” and “how reliable this thought is”. This distinction will allow us, in the next weeks, to deal with all the subtle interesting features of credit (and model) risk.</p>
<p>Another quantity we will deal with very soon are Risk-Weighted Assets (RWA), which are the bank’s on- or off-balance-sheet exposures weighted according to risk. The computation of RWA depends on the approach to credit risk we choose, among the three possibilities offered by the Basel Framework: Standardized Approach, F-IRB Approach and A-IRB Approach. Given RWA we can then compute the capital requirements for credit risk (and all the other risks).</p>
<p>As said, in this course we mainly focus our attention on the IRB approaches to credit risk. We will give the Standardized approach almost for granted.</p>
<p>Risk Parameters The so-called Risk Parameters are the most important quantities in the IRB approaches. Each of them defines a fundamental aspect of credit risk. Yes, we also have the risk-weight functions, but we will introduce them later on, when we are able to estimate the risk parameters, and ready to discuss the modeling choices.</p>
<p>Starting from this week, and going more and more in detail in the next ones, we will take into consideration the following quantities.</p>
<p>Probability of Default - PD The Probability of Default (PD) is the likelihood of a default over a given time horizon. The PD gives us an estimate of the possibility that a counterparty will not be able to meet its debt obligations with us in a certain time period. The quantification of the PD, as we will see, highly depends on the definition of default.</p>
<p>Let’s make a simple example: we toss a fair dice and we define “default” the event “we observe a 5”. The PD is 1/6. Do you agree? :-) Now, let’s change the definition and see what happens. Say that now a default is the event “we observe at most a 5” (thus a 1, 2, 3, 4 or 5, but not 6). The new PD is 5/6.</p>
<p>Coming back to financial terms, a counterparty is usually said to be defaulted when it is more than 90 days (national laws may vary) past due on his/her credit obligation, or it is considered unlikely that the obligor will repay his/her debt without giving up any pledged collateral. But other definitions can be given. As we can easily understand, the wider the definition, the higher is likely to be the PD. We will be back soon with more details, but I hope you got the message.</p>
<p>Exposure at Default - EAD EAD is a measure of the extent to which a bank is exposed to a counterparty, in the case in which that counterparty defaults. There are cases in which EAD is known exactly, for example when dealing with fixed exposures like term loans. In other situations, EAD can only be approximated, as in the case of revolving exposure, such as lines of credit. More details in the next pages already.</p>
<p>Loss Given Default - LGD The LGD is the percentage (%) of loss over the total exposure, in the case in which a counterparty defaults. Hence LGD is a percentage of EAD. LGD is strictly related to the concept of recovery rate: , where is the recovery rate of the exposure under consideration. We will see how the LGD (or the Recovery Rate) can be estimated and what are the main problems involved in such a process. It may seem strange, but LGD is never completely known, until the recovery process is over. It is a rather tricky quantity.</p>
<p>Maturity - M Maturity is simply the final payment date of a loan or another financial instrument/security. In other words it gives us information about the timespan over which a given obligation/financial instrument remains outstanding, or it is defined. For example a 2-year bond has a maturity of 2 years. A 5-year mortgage has simply a maturity of 5. And so on. More sophisticated definitions of maturity can be given, but we will consider them in due time.</p>
<p>Hi there, welcome back, welcome to our second video lesson together, and welcome to my office, featuring my noisy colleagues. Now, today I would like to discuss with you two important quantities related to credit risk, that is to say the expected loss and the unexpected loss. As you can imagine, as you know, these two quantities are strictly related. But, as we will see when introducing the different models to credit risk, we will handle these two quantities, the expected loss and the unexpected loss, in a different way. So, let’s start. The expected loss is what we expect to lose in case of default of a counterparty, simply the expected value of the future loss distribution for that counterparty. The computation of the expected loss for a given counterparty requires the knowledge of three different quantities: the Exposure at Default, the Probability of Default and the Loss Given Default. We will consider the probability of default starting from next week, week 2, where we will start modeling credit risk and actually computing the probability of default of our counterparties. The Loss Give Default, on the contrary, is the topic of the next video lesson in this week, so I just refer you to that video lesson. Here, in the next minutes, I would like to say something more about the Exposure at Default, which is the third component that we need to compute the expected loss. First of all, notice that in principle the Exposure at Default is NOT a deterministic quantity, but a random variable representing the current exposure plus its possible variation from now to the date of the possible default. Its variability depends on the on the type of facility under scrutiny. Naturally there are situations in which the EAD is deterministic. For example in most bonds or loans, where exposure is not random, but follows a pre-determined repayment plan for both capital and interests. When exposure at default is stochastic, we can experience what goes under the name of Exposure Risk, that is the risk that the actual exposure is larger than the amount we originally expected. Interestingly, differently from default risk and recovery risk, this risk is rarely really taken into account in modeling. Can you think of a situation, an example of exposure risk? Assume that a company is granted a loan and that this company may vary, within certain limits, the size of this loan. Empirical evidence shows that companies in financial difficulties tend to use their loans up to their maximum. This makes the exposure at default increase, close to default. The estimation of EAD typically requires the measurement of three quantities: the drawn portion, the undrawn portion and the credit conversion factor. This last quantity accounts for the proportion of undrawn portion that could be used close to default. Empirical studies show that CCF is typically between 40% and 75%. On screen you can see a simple example of computation of the exposure at default for a 1 million dollar credit line. Notice that including the undrawn portion leads us to estimate a greater expected loss, and a higher price for the loan. No need to say that, in order to be competitive on the market, this increase should not be entirely transferred to the spread applied to the drawn portion. Ok, so, that was the expected loss. Now I would like to tell you something about the unexpected loss. I will be quite general for the moment. We will see unexpected loss in its generality, because we will be back quite soon, when we consider the different approaches to credit risk, the different models, with the details about the computation of the unexpected loss, which is the real thing when we consider credit risk. Don’t forget it! The unexpected loss it is the variability of the possible loss around its expected value, the expected loss. When we consider a diversified portfolio of exposures, the expected loss of the portfolio is the sum of the expected losses on the single exposures. Why? Because the expected value, in mathematical terms, is a linear operator. The expected value of X plus Y is equal to the expected value of X plus the expected value of Y. And so on. The portfolio unexpected loss, on the contrary, is generally lower than the sum of the unexpected losses. Again, why? Because, as we will see, the variability represented by the unexpected loss is essentially measured in terms of standard deviations, and we know that standard deviation is a sub-additive measure of risk. If you need to refresh these concepts, just check the extra materials of this week. Summarizing, the expected loss of a portfolio cannot be reduced by diversifying the portfolio. On the contrary, the unexpected loss can be reduced with suitable portfolio strategies. And this is why diversification can actually reduce credit risk</p>
<p>In this lesson, we go a little deeper into the estimation of the Loss Give Default or, if you prefer, of Recovery Rates. We will discuss how this relates to Recovery Risk.</p>
<p>We will see that LGD is a rather tricky quantity, whose actual value is not easy to assess. At least not before the end of the recovery process…</p>
<p>LGD can be estimated using very different techniques, depending on the type of exposure we are considering, on the availability of data, and on the degree of sophistication we aim to achieve.</p>
<p>LGD is such an interesting topic that we could discuss it for an entire week.</p>
<p>we will deal with LGD, the loss given default, and its estimation. We will consider the different factors affecting the value of LGD, and the way in which we can estimate it, and we will see different approaches to its estimation. Now, LGD is a very interesting topic that could represent the subject of one entire week. If you are really interested in all the small details involved in the computation, in the statistical estimation of LGD, my suggestion is to ask for it to be the extra topic of week 6. In this week, in this video lesson, and with the online materials that you can find on the course platform, we will consider the most important characteristics of LGD. The Loss Given Default, we know, is the loss rate experienced on a credit exposure if the counterparty defaults. It can take any value between 0% and 100% and it is computed in terms of Recovery Rate. LGD is equal to 1 minus double R. It may seem weird, but LGD is never known in advance. Sometimes not even at default, if no secondary market for the defaulted exposure is present. Only when the whole recovery process is over, we can fully assess the actual LGD. Before Basel II, most banks used to assess LGD and PD jointly, computing the so-called Expected Loss Rate. Now things have changed. A very very interesting topic is the relationship between LGD and PD, that is to say between what we expect to lose and the probability of a default. Now, most models that we will consider together assume the independence of these two quantities, LGD and PD. You can imagine that this is not really realistic, and obviously this accounts for what goes under the name of model risk. It is one of the possible features of model risk. We will see this in more details, later on, when we consider the different models, and we will also discuss a little bit about the empirical findings related to the relationship. So, just be patient, and we will discover a lot of things together. When considering the factors affecting LGD or Recovery rates - it is essentially the same: remember the strict relationship between LGD and recovery rates - we can essentially group the different factors into four different groups. One is related to the characteristics of the exposure. One is related to the characteristics of the counterparty. Some factors are related to internal factors of the bank, while the last group is concerned with external factors, the macroeconomic factors. If we consider the characteristics of the exposure, we ask questions like: Is there any collateral? What is the degree of effectiveness? That is, how easily can the exposure be liquidated? What is the level of seniority? And its priority level? Are there guarantees by third parties? As you can imagine, different answers to these questions lead to different recovered amounts. A collateral always guarantees a higher recovery rate. Talking about the characteristics of the counterparty, it is important to know the industry it belongs to, as this can influence the likelihood to find a buyer for the defaulted company and the price at which it is likely to be sold. Similarly, the legislation of the country of the counterparty may affect the duration of the recovery. Finally, an extremely leveraged counterparty is typically something we do not wish for. Our internal efficiency also plays a role, especially the best practices and performances with out-ofCourt settlements. This is an example of internal factors. Finally we have external factors, typically macro-factors. Later on we will see that the state of the economy clearly affects recovery rates (and thus the LGD). We will also see that the business cycle influences the PD and other quantities. And how it impacts on wrong way risk. Regarding interest rates, no need to say that their level clearly influences the present value of recoveries. When we deal with the estimation of LGD or recovery rates, we can typically distinguish two main approaches: Market LGD and Workout LGD. Market LGD assumes that we have the prices of the defaulted exposures, and that we can use these prices as an estimate for the recovery rate. Naturally, you understand, this information is not always available. And that’s why we can use another approach, which is the so-called Workout LGD. In Market LGD prices of defaulted exposures are used to estimate the recovery rate. If a defaulted bond trades at 40 cents a euro, we can say that the market is estimating a 40% recovery rate, and a 60% LGD. This naturally requires the existence of a secondary market. Data about this type of exposures and their recovery rates can clearly also be acquired from rating agencies. For example, we know, that for bond issuers in the public utility sector, the average recovery rate is almost 70%, with a standard deviation of 22%. Market LGD also includes two special subtypes: Emergence LGD and Implicit Market LGD. The first relies on the market value of the new financial instruments that a company may offer to lenders for its defaulted bonds. Something like shares or very long-term bonds. The Implicit Market LGD approach makes use of spreads on performing bonds, often under the strong assumption that LGD and PD are independent. We will see, using Merton’s model, that spreads on corporate bonds necessarily depend on the expected recovery rate. This fact allows us to make the necessary computations. Most banking loans are not traded, hence it is quite hard to find a secondary market. In this case we can rely on the so-called Workout LGD, which requires us to compile a historical database, from which an estimate of the LGD can be derived. This database contains information about past defaulted exposures: recovered amounts, recovery lags, procedures, etc. Data can then be segmented by type of exposure, type of counterparty, etc. On the course platform you can find more details about actual computations. We have seen that different factors may affect recovery rates. Mortgages on residential real estate tend to have a higher recovery rate, close to 90% and more. For unsecured overdrafts, recovery rates are close to 0%. We can say something interesting about the cross-section distribution of recovery rates. In the picture, you see an example of empirical distribution of recovery rates for a medium European bank. Now, this U shape is quite characteristic, and the little statistician in us should suggest that a good candidate to fit these data could be the beta distribution. And this is exactly what many scholars have done in the empirical literature. The density of a beta with parameters a and b, and support on 0-1, is given by the formula on your screen. The parameters a and b can be easily estimated using the method of moments. Let x bar be the sample mean and sigma squared the sample variance of historical recovery rates, we can easily get the estimates of a and b, as you can see on screen in the red formulas. This turns out to be rather useful in practice.</p>
<p>Workout LGD Workout LGD relies on historical data. However, these data should be analyzed according to an economic perspective, and not just statistically or from a simple accounting point of view. This means that every factor that may decrease the final economic value of recovery should be taken into account, in order to guarantee a conservative approach to recovery risk.</p>
<p>Workout LGD can be estimated in many different ways. Here below we start with a first basic approach based on a standard actuarial formula. The actuarial approach is typically used for simple exposures. In the next pages, on the contrary, we introduce a more statistical sound modeling based on GLM and Survival Analysis.</p>
<p>Let’s define the following quantities:</p>
<p>RRate: actual Recovery Rate on the defaulted exposure under consideration. DNVR: the Discounted Net Value of Recovery, i.e. the present value of all recovered amounts minus all costs. EAD: Exposure at Default. FVR: the Face Value of Recovery, as recorded in our accounting data. AC: the Administrative Costs of Recovery. r: simply the discount rate. T: the duration of recovery We can then apply a formula like: Observations</p>
<p>FVR should comprise all fees (including those from late payment) collected from the defaulted counterparty. Administrative costs must include all direct and indirect costs faced by the bank in collecting all payments on the defaulted exposure, legal ones included. The discount rate r, following the recommendations of the BCBS, should appropriately reflect the risk of the recovery process. A possibility could be to use historical values, like the average market rate observed between similar defaults and the end of the recovery process. However, this risks to introduce some historical bias in our estimation, defining a backward-looking measure. Since we are interested in the estimation of LGD for future bad loans, a more rational choice is to use forward rates, as a simple forecast of future spot rates. The right time horizon necessarily depends on T. T, the duration of the recovery process, needs to be assessed in a financial sense, taking into consideration all the possible intermediate flows. Very often, in fact, recovery is not a one-shot but rather a gradual process, involving several intermediate payments. This flow of payments should suggest the use of Macaulay Duration to actually compute T, by weighting the different (re-)payments (the intermediate ones and the final one) by their maturities.</p>
<p>Statistical Approaches for LGD estimation How can we compute LGD using statistical models?</p>
<p>In the literature, many different approaches have been proposed. Most of them fall under the umbrella of generalized linear models.</p>
<p>To summarize, we can identify three main approaches:</p>
<p>Single-stage Models Multi-stage Models Survival Analysis</p>
<p>Single-stage Models</p>
<p>In this class, we can find direct and indirect methods. The first ones include OLS regression, Beta regression and Fractional regression. The second group is related to binary transformations of LGD using weights or probability transforms.</p>
<p>Multi-stage Models</p>
<p>In multi-stage models estimation requires two or more steps to be performed. For example: a two-step ordinal regression with nested linear one, or a three-step tree approach involving logistic and linear regression.</p>
<p>Survival Analysis From a statistical point of view, survival analysis does not form an independent class of models. It is a branch of statistics whose aim is to study the time duration until a given event or a combination of events take place. For us, the event is the default of a counterparty and its recovery. We can have survival approaches in both the single-stage and the multi-stage classes. However, it may be worth to consider survival analysis on its own, because it allows for the exploitation of partial recovery data. Something very useful when dealing with workout LGD and unresolved defaults.</p>
<p>In the next pages, we first consider a short example with the Beta regression, and then we give some hints about the survival analysis approach. Interested in more details? Remember Week 6: ask for it on the course forum.</p>
<p>I also suggest you to read this nice work by Edward I. Altman (the creator of the Z-score): Default Recovery Rates and LGD in Credit Risk Modeling and Practice.</p>
<p>Beta Regression In the video lesson, we have quickly seen that empirical studies support the idea that Recovery Rates are Beta Distributed, as in the picture below.</p>
<p>It is worth noticing that the U-shaped behavior above is rather common among European and Chinese banks, while US banks tend to experience more bell-shaped recovery rates. However, in all these situations, the Beta distribution is a very good candidate, given its flexibility. In fact, while a Beta(0.5,0.5) is clearly U-shaped, a Beta(2,2) looks like a bell. Remember that the support of a Beta distributed random variable is always finite.</p>
<p>Now, an interesting property of a Beta(q,p) distributed random variable is that the quantity follows a Beta(p,q). This is known as the mirror image property of Betas.</p>
<p>But wait a second, if Recovery Rates (RR) are Beta distributed, then is Beta distributed as well.</p>
<p>Ok, this is exactly the starting point of the so-called Beta regression approach, which - from a statistical point of view - is just a special type of Generalized Linear Model, or GLM.</p>
<p>Let us indicate with our Loss Given Default. We know that this quantity is between 0 and 1, but in what follows we assume that 0 and 1 are unlikely to be touched, so that we can consider the open interval (0,1)*.</p>
<p>If is Beta distributed with parameters and , we have that its density function is</p>
<p>From this we can derive and where .</p>
<p>Following Ferrari and Cribari-Neto (2004), we can re-write the density above in terms of and , as this simplifies the estimation.</p>
<p>We thus obtain</p>
<p>for and .</p>
<p>Now, assume we have an i.i.d. sample , where each , for , has mean and a common dispersion parameter (the higher the more dispersed the distribution).</p>
<p>As standard in GLM, we assume that a function of the mean can be approximated with a linear regression with covariates , and their corresponding parameters , with . In formal terms we are thus assuming</p>
<p>Always following Ferrari and Cribari-Neto (2004), we can assume a standard representation for the link function (this is formal name) , that is a logit, or</p>
<p>By equating the two expressions for and inverting we thus obtain</p>
<p>The parameters of the model can then be estimated using the maximum likelihood method. In fact, the log-likelihhod can be obtained explicitly as</p>
<p>and the parameters of interest can be obtained numerically.</p>
<p>*The quantities 0 and 1 are typically considered separately, according to a commonly used two-step procedure, which we can discuss in more details, in Week 6, if you are interested.</p>
<p>Survival Analysis Every time we observe one or more objects that stay in a given state for some time and then move to another state, Survival Analysis provides sound statistical tools for modeling. A nice introductory book here.</p>
<p>Interestingly, survival analysis gives us the possibility of dealing with censored observations, that is observations for which we have information only until a certain time point and nothing after. This is particularly important when considering portfolios of exposures, where partial recoveries and incomplete workout are rather common.</p>
<p>Survival Function, Hazard rate and friends Two key concepts of survival analysis are the survival function and the hazard rate (function).</p>
<p>Let us consider a system which can stay in two states: A and B. We start in A. Let be the random variable representing the time at which this system moves from state A to state B. Once in B we cannot come back.</p>
<p>Naturally is a continuous quantity, and with and we refer to its density function and its distribution function respectively (notice that time cannot be negative, therefore ).</p>
<p>Given , we can define the so-called survival function , which gives us the probability that our system will stay in A at least until time .</p>
<p>The hazard rate is defined as</p>
<p>It can be read as the conditional probability of a move to B in a small time interval , given that no move has occurred by time .</p>
<p>By integrating the hazard rate, we obtain the so-called cumulative hazard function, that is Using standard calculus, we can show that Survival modeling of recovery We are interested in modeling recovery rates (and LGD) using survival analysis. We assume that our counterparty has defaulted (State A) at some time point, which we rescale to 0, so that represents the time at which we observe recovery (State B). Then for us represents the expected recovery rate at time , while is the expected loss rate if the recovery process terminates in . The hazard rate then becomes the so-called incremental recovery rate, that is the speed of recovery on the unrecovered amounts at time . Differently from other estimation methods, like the Beta regression we have seen before, survival models allow us to deal with the problem of partial recovery, that is to say with those defaulted counterparties whose recovery process is still ongoing at the time of estimation. In this framework censoring is easily understood as follows: if before the recovery process is over, the observation is not censored; if the recovery process stops after (tomorrow? in one week? in one year?), the observation is censored.</p>
<p>Modeling Choices Different models can be specified by making parametric and semi-parametric assumptions on , and friends. Explanatory variables can naturally be used to better model the recovery process. Parametric models are simple and easily estimated, but they may require restrictive assumptions on the structure of data. Semi-parametric models allow for more flexibility, but they may be more difficult to handle.</p>
<p>Parametric Models Two common parametric models are the Weibull and the Log-logistic, both mutated from life-insurance mathematics.</p>
<p>In the Weibull model we assume</p>
<p>while in the Log-logistic<br />
In both parametrizations, the parameter is commonly defined as , where is a vector* of covariates without the intercept. These covariates are generally indicators of the factors affecting recovery, as we have seen in the video lesson.</p>
<p>The parameters are then estimated using maximum likelihood, after noticing that the general form is always the same: where is the set of indices of the uncensored observations, while is the set of all the observations.</p>
<p>Semi-parametric Different semi-parametric models can be defined. An interesting one dates back to the 70’s, in a paper by Cox (you can read it online for free). The idea is simple but effective. We assume where is called baseline hazard function, and it is independent from the covariates in . Several assumptions can be made on , but it is common to assume it to be a step function jumping on a discrete set of time points indicating censoring or full recovery.</p>
<p>The new definition of leads to</p>
<p>Estimation can be performed using numerical methods and partial likelihood functions, which go beyond this first introduction.</p>
<p>Only the beginning Apart from the standard models of survival analysis briefly described above, we can make use of more sophisticated approaches, such as the pseudo-survival ones. The aim of these approaches is to improve the quality of the goodness of fit.</p>
<p>Interested in more details? Then ask for it. We can have several video lessons on these topics, which I personally like.</p>
<ul>
<li>Notice that I am not using indices (e.g. and ) to simplify notation.</li>
</ul>
<p>We can easily show how the volatility of recovery rates, that’s Recovery Risk, has an impact on the overall risk of an exposure, hence on the volatility of the Expected Loss, the Unexpected Loss.</p>
<p>Let’s consider a simple loan.</p>
<p>We know that: . To simplify computations we set , without any loss of generality. Therefore, from now on, we have .</p>
<p>Now, you will agree with me that a default is a binary event: it happens or it does not happen. In probability terms, we are dealing with a Bernoulli random variable, whose probability of occurrence is exactly the .</p>
<p>We can experience two situations:</p>
<p>We have a default with probability e we lose . We do not have a default (Hurrah!) with probability and we lose nothing. Let’s make two further assumptions about the (and thus about the Recovery Rates).</p>
<p>LGD is deterministic: No Recovery Risk</p>
<p>In this case, the volatility of losses is entirely given by the Bernoulli nature of defaults. In other words we have that</p>
<p>This comes from the fact that is just a scalar (by our assumption) and that the variance of a Bernoulli with parameter is , so that the standard deviation (our volatility) is just the square root.</p>
<p>LGD is random: Recovery Rates vary and thus we have Recovery Risk</p>
<p>Let’s assume that is a random variable with mean and variance .</p>
<p>From basic results in probability , we know that if and are two independent random variables, then for we have where is the mean of , and its standard deviation (the same for ).</p>
<p>This tells us that Simplifying this last expression, we get Now consider that is usually defined in terms of , and you find one of the famous formulas for unexpected losses, which you very probably know. Later on we will see how to combine all this in a portfolio, with more counterparties.</p>
<p>Little exercise for you: verify that - under realistic assumptions - in case of random the unexpected loss is always larger than/equal to the one of the deterministic case. In 2014, the International Accounting Standards Board (IASB) released the so-called IFRS 9. I am quite sure you know it.</p>
<p>IFRS 9 is an International Financial Reporting Standard whose aim is to address the accounting of financial instruments, focusing on three main aspects: classification and measurement, impairment (of financial assets), and hedge accounting.</p>
<p>Classification and measurement The new standard tells us how financial assets and liabilities must be accounted for in financial statements, also defining how they must be measured on an ongoing basis. This is a relevant novelty. A principle-based approach is introduced to get rid of the inefficient and cumbersome rule-based ones.</p>
<p>Impairment Starting from the consideration that the delayed recognition of (credit) losses on financial instruments was a major weakness of the existing accounting standards during the 2007-2008 world crisis, IASB has introduced new approaches for a more timely recognition of expected losses. This is probably the most interesting part for us.</p>
<p>Hedge accounting IFRS 9 introduces significant novelties for hedge accounting, especially with respect to enhanced disclosures about risk management. The basic idea is that accounting and risk management need to better integrate in order to provide a proper treatment of risk.</p>
<p>The new standard will become effective in 2018, substituting the old ones.</p>
<p>Here we will not cover all the details of IFRS 9, because it is beyond the scope of the course. We will naturally focus our attention on the most interesting aspects for us, those related to credit risk. For example, we will deal with the (relatively) new concepts of lifetime expected losses and lifetime PD, discussing modeling issues and problems.</p>
<p>In order to be consistent, and to give you a correct understanding of the novelties in IFRS 9, we will first introduce the standard results for PD modeling and estimation. In practical terms, we will deal with IFRS novelties from Week 3 on.</p>
<p>Why am I bothering you with all this then? In Week 1?!</p>
<p>Because this is a course for professionals, hence I will assume some familiarity with IFRS 9. If not, some preliminary information can be found here. Hence you have the time to have a look.</p>
<p>As usual, if you have specific questions, I will be more than happy to answer them on the course forum, or in the Casual Friday videos.</p>
<p>Naturally, it goes without saying, the website of IFRS is the best source of information, for those interested in all the little (but actually important) details of the new regulations.</p>
<p>Credit Risk in Basel II and III CR is one of the main risks for banks, together with market risk and operational risk (and liquidity risk). It is part of the so-called First Pillar. The BCBS identifies three main approaches to credit risk: • Standardized: Risk-weighted assets (RWA) are computed as the weighted sum of on-balance and off-balance sheet items, weighted using some specific risk weights provided by the Regulator. • F-IRB: banks are free to compute the PD according to the model they prefer; all other formulas are given. The estimated PD is plugged into some specific risk-weight functions provided by the Regulator, together with the other risk parameters (see below), obtaining an estimate of RWA. • A-IRB: banks can compute RWA using their own internal models and formulas, conditionally on the approval of the Regulator. Risk Parameters The main risk parameters we need to focus on, when dealing with credit risk, are: • Probability of Default (PD): the likelihood of a default over a given time horizon. The quantification of the PD highly depends on the definition of default. ! • Exposure at Default (EAD): a measure of the extent to which a bank is exposed to a counterparty. In general EAD is not a deterministic quantity. • Loss Given Default (LGD): the percentage of loss over the total exposure (EAD), when a counterparty defaults. LGD is strictly related to the concept of recovery rate: LGD=1-R, where R is the recovery rate of the exposure under consideration in decimals. LGD is never completely known, until the recovery process is over. • Maturity (M): in general, the final payment date of a loan or of another financial instrument. However, more sophisticated definitions can be given. Expected and Unexpected Losses The Expected Loss (EL) is what we expect to lose if a counterparty defaults. It is given by</p>
<p>EL=EAD x PD x LGD The Unexpected Loss is the variability of losses around their expected value EL. Unexpected losses can be reduced by diversifying a portfolio of exposures. LGD (recovery risk) and PD (default risk) are the main drivers of unexpected losses. 1 of 2 ACRM - P.Cirillo Credit Risk Credit Risk (CR) is a portmanteau risk combining different types of sub-risks: • Default Risk: our counterparty defaults and its not able to honor its obligations. • Migration Risk: the creditworthiness of our counterparty deteriorates, increasing the probability of default and decreasing the value of the exposure. • Recovery Risk: the actual recovery rate is smaller than what we originally expected, possibly generating higher unexpected losses. • Spread Risk: spreads on the market increase, decreasing the asset value of the relative securities. In the course we mainly focus on these sub-risks, but other sub-risks as the country and the substitution ones can also be taken into consideration. PCS - 1 LGD in more details The estimation of Loss-Given-Default, or of Recovery Rates, is an interesting but tricky topic. First of all LGD is never completely known in advance. We have to wait until the end of the recovery process to have a complete assessment. Moreover, LGD is affected by different factors, which we can collect into 4 groups: characteristics of the exposure, characteristics of the counterparty, bank’s internal factors and external macroeconomic factors. Estimation of LGD If the defaulted exposure has a secondary market, we can use the so-called Market LGD, which relies on the price of the defaulted exposure as an estimate of the recovery rate. Market LGD includes two sub-types: Emergence LGD and Implicit Market LGD. When a secondary market is not available, we can rely on Workout LGD. Workout LGD requires compiling a historical database, containing information about past defaulted exposures. For workout LGD, we can use different estimation methods: • Actuarial Methods: ok for simple exposures. • Single- and Multi-stage Statistical Models: most of these approaches rely on Generalized Linear Models to estimate LGD. A good example is the Beta regression, which starts from the observation that recovery rates (and hence LGD) seem to follow a Beta distribution (as in the picture on the right). • Survival analysis: studying the transition from default to recovery, using tools like the survival function and the hazard rate, we can model LGD, also taking into account censored observations like unresolved defaults. Parametric and semi-parametric models are typically used, and each of them has its points of strength and its points of weakness. IFRS 9 and Credit Risk For the moment, we only need to remember that IFRS 9 requires a more timely recognition of expected credit losses, by introducing new concepts like the lifetime PD. The idea is to avoid the errors of the past, during the 2007-2008 financial crisis. According to IFRS 9, risk management and accounting need to better integrate.</p>
