---
title: ch 3 ex cl
author: ''
date: '2018-06-04'
slug: ch-3-ex-cl
categories: []
tags:
  - credit
---


The aim of this week is to discuss Threshold Models and Mixture Models.
Both classes of models represent Portfolio Approaches, in the sense that we are interested in modeling portfolios containing more than one counterparty.

We are still interested in the PD of each single counterparty, but now we also want to take into account the possibility of joint defaults, trying to model the dependence among different obligors.

We start with Threshold Models, which can be seen as the multivariate generalizations of the structural models of default we have introduced in Week 2.
In particular, we will focus our attention on the portfolio version of CreditMetrics.

We will then move towards Mixture Models, a very general family of models, where the defaults of our counterparties are assumed to be conditionally independent, once we know which are the common factors building the dependence among them.
In this class we will discuss Bernoulli and Poisson mixtures, with a special attention for CreditRisk+, by Credit Suisse.

Mixture models are interesting from different points of view, given their flexility, their computational efficiency, and the possibility of representing every threshold model as a mixture model.

This is a rather theoretical week. 

All the models we discuss together will be used in practice and compared in Week 5, the statistical week of the course.

In this week we also have a novelty: the "Voices from the Field Session".
In this session, experts from Deloitte The Netherlands will share with us their views about the development of the IRB approaches to credit risk.

Threshold Models
Threshold models are for us nothing more than the multivariate generalization of the structural approaches we have seen so far.

Instead of just one counterparty, we now have n counterparties, and instead of only one default threshold, we now consider n default thresholds.

In order to build the dependence among defaults (and counterparties), we can - implicitly or explicitly - make use of copulas.

In this class of models we will mainly focus on the portfolio version of CreditMetrics.

However, given the general construction, we will also briefly - for the moment - consider another interesting construction, the Li's model, a survival model we can see as a sort of implicitly defined threshold model.

An important property of threshold models is the so-called equivalence. We will define conditions for which two apparently different models can be considered as one. A feature that, in Week 5, we will see is rather useful from an estimation and simulation point of view.
In this first video lesson we start dealing with threshold models.
Now, threshold models are just the multivariate extension of the models we have seen last week, in
Week 2.
These models, if you remember, belonged to the class of structural models of default, in which we
have a threshold mechanism that defines the default event.
Now, in threshold models we have a portfolio of counterparties, a portfolio of exposures. Every
exposure will have its own threshold, and we are interested in understanding how each
counterparty may default, what happens if we consider groups of counterparties, and what
happens when we start dealing with the dependence structure, that is to say how the default of a
counterparty may trigger an avalanche effect, may generate other defaults, may affect the
possibility/the probability of default of the other counterparties in our portfolio.
We will start by introducing the so-called one-period models. One period is not meant as one year.
One period means that we define a time window between 0 and T, and we make our evaluations
about defaults in this time window. So, we might be interested in the default of our counterparty in
T, or within T. The point is that we are not interested in what happens after T, that is to say we are
not repeating our evaluations over time. And that’s why we speak of one-period models.
We have a portfolio with m obligors and we fix a time horizon T.
For each counterparty, we consider a categorical variable S_i indicating a state a time T, typically a
rating.
We order these quantities such that S_i equal to 0 means “default”, while all the other values
indicate better rating classes.
Without loss of generality, we assume that none of our counterparties has defaulted in t equal to 0.
We can then introduce a new indicator random variable Y_i for the event default of obligor i.
Clearly the value of Y_i is strictly linked to S_i.
The vector Y is then nothing more than a vector of default indicators.
With p(y) we indicate its joint distribution.
If we are interested in the marginal probabilities, that is to say for example I am interested in
knowing what is the probability of default of the i-th counterparty, I can define the quantity p_i
which is exactly the probability that Y_i is equal to 1.
Now, one of advantages of using multivariate models, of using portfolio models, is the possibility of
modeling dependence, apart from the fact that they are much more realistic from a modeling point
of view.
When we model dependence, we can model dependence in many ways. We start from the
simplest way, that is to say by modeling the default correlation. Sometimes we also say event
correlation, because for us an event is exactly the default.
We can use other ways, we can use other measures of dependence. We start with correlation. At
the end of the course, in week 6, we will see that actually correlation is just a special type of
dependence, is linear dependence. And we can introduced more sophisticated way of measuring
the dependence among counterparties.
Ok, so let’s start from the variance.
What’s the variance of the default indicator of the i-th counterparty?
So…come on…
Ok, you are lazy.
So just look down here: you see the expression.
Now, the indicator is just taking values 0-1, this is a Bernoulli random variable, and the variance is
exactly the variance of a Bernoulli random variable, as you can see in the formula.
Correlation can be easily computed as well. The formula is on the screen.
We can then define M to be the random variable counting the number of defaults. It is a sum of
indicators, each of them is a Bernoulli. But please notice that we are not assuming the Y’s to be
independent.
The overall loss will correspond to the number of defaults times the nominal loss given default for
each defaulted exposure..
For the moment we say nothing about Exposure-at-Default and the Loss-Given-Default. They can
be deterministic, or they can be random. We will specify when we need it.
Let X be an m-dimensional random vector. X can for example contain the asset values of our
counterparties, or another quantity of interest.
We can then define a set of thresholds for each counterparty, and we can collect then into a matrix
D. We do exactly what we did in the univariate case.
For each counterparty i, S_i is equal to j, if and only if X_i is larger than threshold d_ij and lower
than/equal to d_i(j+1).

The couple (X,D) defines what we call a threshold model.
The vector X is the critical vector, while the matrix D contains the critical thresholds.
Given the system of thresholds, the marginal probability of default for our counterparty i is
F_i(d_i1), where F_i is the marginal distribution of X_i.
It is worth noticing that, in general, the asset correlations and the event correlations are not the
same thing. They are for sure related, but they do not coincide.
In particular we can observe how the asset correlation enters into the formula of the default
correlation.
Please notice that when we assume that jointly assets are normally distributed, and we can obtain
this by taking into consideration a multivariate Normal or a Gaussian copula, then we have that the
perfect measure of dependence is correlation, because the only type of dependence we really
have is linear dependence.
In that case the correlation among the assets fully determines the correlation among the events,
that is to say among the defaults.
Now, for the moment we close here. This is the first part of our introduction to threshold models

We start by introducing the concept of equivalence between two different threshold models. And
later in the course, in particular in Week 5, when we’ll deal with all the statistical problems of the
estimation of the different models we are considering, we will see that equivalence is a quite useful
concept, from a practical point of view.
Always in this class, we will consider multivariate extensions of CreditMetrics. We will see that from
a theoretical point of view, we are assuming a multivariate Normal, so that the conceptual difficulty
is not bigger than what we have seen in the univariate case last time. But now we can model
dependence.
And we will see that this is linear dependence. Then, on the course platform, you will find extra
materials, with some extra examples.
So, let’s start.
Let (X,D) and (X*,D*) be two threshold models with state vectors S and S*.
We say they are equivalent if the marginals of S and S* coincide, and if X and X* admit the same
copula C.
Why is this theorem relevant?
As we will see in Week 5, equivalence can be used for calibration purposes.
If I can define two equivalent models, I can play with the more convenient one in terms of
estimation, simulation, and so on, and then be able to make inference on the other.
So we can say that two threshold models are equivalent, if the two state spaces, the two space
vectors, or equivalently - because they are defined starting from the state vectors - the two default
indicator vectors are equal in distribution. So their joint distributions are the same.
In the portfolio version of CreditMetrics, we assume that X has a multivariate normal distribution.
The quantity X_i is the asset value of the i-th obligor at maturity.
The thresholds in D are defined according to the same methodology we have seen for the
univariate case. Exactly the same.
Notice that we could have used a Gaussian Copula and the result would be the same. Why?
Because of the normal marginals.
In other words, it is like we are assuming a Gaussian copula implicitly.
To calibrate the variance-covariance matrix CreditMetrics assumes a factor model for X. And X, I
repeat, is assumed to have standard normal marginals. As in the univariate case.
So, we are assuming a factor model, as the one you see on your screen, in which our vector X is
equal to B times F plus epsilon, where B is a matrix of weights, F is a normally distributed random
vector of factors - and these factors may represent systemic factors, industry factors, economic
factors; so essentially the joint component that the different counterparties share, and that
determine the value of the normalized assets - and we have a vector epsilon, that contains error
terms that are i.i.d. (so identically independently distributed) according to a Normal distribution.
Using the assumptions about the independence of F and epsilon, the variance-covariance matrix of
X is given by the quadratic form in Omega, the variance-covariance matrix of the factors in F, and
Gamma, the diagonal variance-covariance matrix of the error vector epsilon.
Please notice that thanks to the assumption that the marginals are standard normals, then the
covariance matrix is also a correlation matrix.
If we define the vector of weights b_i for counterparty i, we can easily obtain X_i, using the formula
you see on your screen.
Now, remember that, thanks to the standardization we have for the different marginals, the
variance of X_i is equal to 1. Now, this allows us to easily compute the systematic risk and the
idiosyncratic risk.
The systematic risk is the part of the risk that we can explain using the common factors, and in
fact, as you can see in the formula, this depends on the common factors. And then, always from
the construction, we can obtain the idiosyncratic risk, that is to say the counterparty-specific risk
that we cannot explain with the common factors.
As for CreditMetrics, also Moody’s KMV can be generalized to the multivariate case.
We can still use a Gaussian copula to build the dependence among the marginals. However, since
these quantities are unknown for us, we cannot make computations.
It is important to notice that Moody’s KMV and CreditMetrics can be equivalent. How?
Simply if the marginal probabilities of default and the correlation matrix are the same in both
models.
Li’s model is a model that was introduced in 2000.
And, during the last crisis, (it) was even blamed as the formula that blew up Wall Street.
Now, this is quite an exaggeration. On the course platform you find some extra readings that I think
are really interesting. No single model can actually blow up Wall Street.
Models are if you want neutral. It is the way we use them that can generate problems.
That’s why we always need to know the weaknesses of the models we are using.
Now, Li’s model can be considered a threshold model, or a survival model.
In reality, we are covering it here, because it’s a quite famous model, that can be seen as a
threshold model in terms of equivalence.
So, we can show, and we will do that later on in the course, that this model can, under certain
assumptions, correspond to CreditMetrics.
In Li’s model, X_i is no longer a variable representing assets, but rather the random default time of
obligor i.
If X_i is smaller than/equal to T, then we observe the default of obligor i.
Assuming that default rates can be seen as constant over a short time horizon (possibly [0,T] is
short enough), Li assumes that the marginal distribution of X_i is an exponential distribution with
parameter lambda_i.
The marginal probability of default is therefore equal to F_i(T).
In order to build the portfolio dependence, we can then assume a Gaussian copula.
If the marginal default probabilities are the same, and the correlation matrix as well, then Li’s Model
is just another way of parametrizing CreditMetrics. Naturally only for default risk, without taking into
consideration migration risk.
So, I know that for sure you are asking yourself how we can use these models in practice. We will
show that, we will discuss that in Week 5, so I am just asking for your patience.
I prefer to introduce all the models, and later to consider the practical implementations, so that it is
also easier to compare them.
On the course platform, for what concerns CreditMetrics, you will find a simpler example that
should help you in understanding how the dependence structure can be used to make our
evaluations.

CreditMetrics Again: Joint probabilities
Let's consider the simplest portfolio ever: just two exposures.
A BB-rated bond and a A-rated one.

Counterparties are independent
If our counterparties are independent, there is actually no need of a multivariate model. The joint distribution of defaults will be just the product of the marginals. Not really interesting.

For example, if the BB-rated bond has a PD of  and the A-rated bond of , then the joint probability of default is , which we can assume to be negligible (but please remember that it is not really zero!).

The same is naturally true for the migration probabilities. With just multiply cell by cell.

Counterparties are dependent
Much more interesting is the case of dependence.
In particular we assume that the A-rated bond and the BB-rated one are linearly dependent, so that correlation is all we need to know (in Week 6 we will see other measures that I personally prefer, even if, unfortunately, for the moment they are less used).

Let  be the standardized asset value of the company issuing the BB-rated bond under the univariate CreditMetrics model at time  (we ignore the index). Let  be the one of the A-rated issuer.

For the moment, let's assume we already know that the asset correlation between the two counterparties is .

Under the multivariate CreditMetrics model, the joint distribution is assumed to be a multivariate (standard) normal distribution with standard normal marginals. In our case, a bivariate one. As already said, this corresponds to assuming that our counterparties are linked by a Gaussian copula. A Gaussian copula with Gaussian margins is just a Multivariate Normal.

The density of a bivariate normal is
The CDF is
Let's again assume that the marginal probability of default for the issuer of the BB bond is 1.06%, and 0.06% for the other.

Using the univariate CreditMetrics we know that the (standardized) default threshold for X is , while for Y the default threshold is -3.24.

The joint probability of default is therefore
This probability is still small, but definitely larger than in the case of independence.

The procedure is the same for migration risk as well.

Let -1.23 and 1.37 be the thresholds for BB to remain BB.
And -1.51 and 1.98 those for the A-rated bond to stay A.

The joint probability of no migration is therefore 
And so on.

In general, since >0, we expect the probabilities of similar changes (both increasing their rating class, or both worsening their creditworthiness) to be higher than in the case of independence. Conversely, movements in opposite directions will be less likely.

What happens if we have 3 or more counterparties? Nothing special: we will play with larger vectors and  will become the correlation matrix . Then we simply need a computer to speed up computations, assuming you do not want to spend your nights making boring calculations.

In R you can use different functions. Nice ones are contained in the pbivnorm and mvtnorm packages.

CREDITMETRICS AGAIN: CORRELATION
We now remove the assumption of already knowing the asset correlation.

Let's consider the factor model we have seen in the video lesson. 

Once again we focus on the simple case of two counterparties. 
A multivariate example will be discussed in Week 5, using R (if you do not know R, try to familiarize with it), when dealing with estimation issues.

Consider the case in which we have 5 factors, one of them, , is in common.  As said, these factors may represent macroeconomic quantities, or sector-specific covariates.

Assume that  also depends on  and , while  is linked to  and .

The model becomes:
 
If the BB-rated company is a European banking group, and the A-rated company is a Dutch manufacturing company, the common factor  may represent a measure of the business cycle in Europe. The other factors may represent industry-specific components, which these two obligors do not share but that, still, can be somehow correlated.

The correlation between  and  is thus given by (remember that factors and errors are assumed to be uncorrelated):
where, for example,  is the correlation between factor  and . These correlations are typically estimated on historical data, and here we assume them to be known.

It is important to stress that the weights , for  and , are standardized, in order to be compatible with  and , the standardized assets. This means that the sum of their squares needs to be equal to one.

Once we have performed our computations,  will be the value we need for the bivariate normal  we have seen before.


Asset Correlation is not Default Correlation
Let's use the simple bivariate CreditMetrics model to show that asset correlation and default correlation are not the same thing.

We have seen that the joint probability of default of two counterparties A and B, with normalized assets and , is equal to 
Remember that , where  is the probability of default of A. With  we indicate the asset correlation between A and B.

In the video lesson, we have seen that the correlation between the default indicators  and , for the two counterparties A and B, is the correlation of two Bernoulli random variables, and it is equal to 
Once again, we see that default correlation and asset correlation are connected, but they are not the same.

Let's assume the following data (the same we have already used before):

 and .
This tells us that , and 

In this case the default correlation () is therefore one-tenth of the asset correlation ().

Gaussian Copulas and Multivariate Normals
We have said that a Multivariate Normal has Normal margins, but that it is not guaranteed that Normal margins are necessarily jointly Normal.
We have also said that a Gaussian Copula with Normal margins is a Multivariate Normal.

Let  denote the standard Normal CDF, and let  be a bivariate normal CDF with correlation .
Let's apply a bivariate Gaussian copula to the uniform margins  and . 
We obtain

Notation itself tells us that this  is indeed the joint distribution for , when  is a bivariate Normal. 

Let's now build a new bivariate distribution having any marginal distributions, say  and , and a Gaussian copula  building the dependence. 
In practice, in the formula above, we just replace the two 's with  and . Let  and  be the new uniforms. 
We thus get:

What we see above looks very similar to the formula for a bivariate normal distribution, because it is a bivariate normal for the transformed variables .
However, since these transformations are nonlinear, every time  and  are not Normal, the resulting distribution is not a bivariate Normal.

For example, let  be a  and  a .

The copula is still a Gaussian copula by choice, however, the joint distribution of  and  is now 

where  is equal to 

with  the Gamma function,  the regularized Beta function and  the Gauss error function.

Model Issues with Copulas
Most threshold models used in industry are based explicitly or implicitly on the Gaussian copula. This is a fact.

And this is also a risk, because choosing a Gaussian copula is like saying that the only type of dependence we assume among defaults is of the linear type.

And what about the usual criticism about the under-representation of extreme events, typical of Gaussian models?

We can notice that, given the possibility of having very different marginals, not necessarily Normal, a Gaussian copula does not automatically put a small mass on univariate extreme events, as in the case of a Multivariate Normal. However, it does not allow for very extreme joint events.

This is a problem, if with our model we want not only to estimate the PD of a counterparty, but also to estimate measures of tail risk, like VaR and expected shortfall. Underestimating joint tail risk can be a tragic error, especially during a crisis, when extreme dependence is likely to be higher.

Other Copulas
A threshold model is essentially made up of a set of default (and migration) probabilities for individual counterparties, and a more or less explicitly defined copula describing the dependence of certain critical variables. Changing the copula has therefore a very important impact on the performances of a threshold model, as it is like assuming a possibly completely different dependence structure. 

In Week 5 we will consider a practical example, in which we will compare the performances of a CreditMetrics-like model, using first a Gaussian copula and then a t-copula.

If you like spoilers, we will see that changing the copula can be much more extreme than assuming a definitely stronger correlation structure among the counterparties, under the same Gaussian copula.

mixture models
The second class of portfolio models we consider together goes under the name of Mixture Models. 



Mixture models are a special type of reduced form models.

The basic idea is simple: we assume that each exposure in our portfolio is subject to the influence of some common factors, which account for the general dependence among defaults. 
If we are able to isolate these factors, then, conditionally on this information, counterparties can be assumed to be independent.

Mixture models are therefore models of conditional independence.

The great advantage of mixture models are their flexibility, and the natural connection they have with powerful statistical models like GLM, GLMM and GEE.

In this class of models we will discuss Bernoulli and Poisson mixtures.

We will then focus our attention on CreditRisk+, probably the most famous industry model among Poisson mixtures.

In this class we consider Mixture Models, which are a different type of models with respect to the
models we have seen so far. In this class, the most important model that you probably know is
CreditRisk+ by Credit Suisse.
Now, in mixture models we have that the different counterparties, the probabilities of default of the
different counterparties, are jointly dependent on a set of common factors. These common factors
can be macroeconomic factors, sector-specific factors, or every combination, and the idea is that
these are the only components that build the dependence among the different counterparties.
From a probabilistic point of view, we assume that, once we know what the common factors are,
then the defaults of the different counterparties are conditionally independent.
As you can see in the picture here below, we have two counterparties, A and B, they depend on
some common factors, obviously they also have their idiosyncratic factors, and if we can isolate
the common factors, the two counterparty can be considered as independent. So their probabilities
of default would be conditionally independent.
Now, what is the interest in mixture models?
If you remember what we have said so far, about the multivariate extension of CreditMetrics, also
in that case we have some common factors, and actually this is the link to tell you something about
the relationship between mixture models and threshold models.
In particular, every threshold model may have a mixture model representation. And later on in the
course, we will see that this is quite useful, because mixture models are more efficient from a
computational and statistical point of view. It’s easier to estimate the parameters.
Let Psi be a p-dimensional random factor vector. It contains different factors building the
dependence among our counterparties.
The random vector Y is said to follow a Bernoulli mixture model with factor vector Psi, if,
conditionally on Psi, defaults are independent Bernoulli random variables.
This tells us that, conditionally on a realization of Psi, the joint distribution of defaults is just a
product of Bernoullis.
In order to obtain the unconditional distribution of the vector Y, we need to integrate over the
distribution of the vector Psi, which plays the role of a mixing distribution.
The marginal probability of default of each counterparty i can be obtained by computing the
expected value of p_i over Psi.
In order to simplify the exposition, let’s consider 1-factor models. In 1-factor models essentially we
assume that we only have one common factor, we are not specifying what this factor accounts for,
but conditionally on this common factor all counterparties are independent, and so are their
probabilities of default.
As you can see in the slides, the notation simplifies, but from a theoretical point of view there is no
major difference.
It is just because it is easier to grasp some of the details.
Let’s assume that all the probabilities of default are the same for all counterparties.
In this case, the Bernoulli mixture model is called exchangeable, since the random vector Y is
exchangeable. In other words, the joint distribution of Y is immune to permutations of the elements
in Y.
Let Q be the probability of default of whatever counterparty, say the first one. Because we are
assuming that all the PDs are the same, the counterparty we choose does not really matter. Set
G(q) to be the distribution function of this mixing variable.
For the Bayesian among us, the G(q) plays the role of de Finetti measure. The defaults are i.i.d.
conditionally on G(q).
The assumption of exchangeability may seem a strong assumption - and actually it is better to drop
it, but if you want it is like we are considering a homogeneous group of companies, say we are just
considering obligors with a BBB rating, within this class we can really assume that the
counterparties are exchangeable. So the order in which I observe them is not really relevant for the
computation of the joint probability of default.
Conditionally on a realization of Q, the number of defaults M is the sum of m independent
Bernoullis with parameter q. This tells us that M is a Binomial(q,m).
The unconditional distribution of M is obtained by integrating over q. The formula tells us that the
probability of observing k defaults is a combination of a binomial mixed by the mixing distribution
G(q), which determines the behavior of q.
Let pi be the probability of default of each obligor. We can easily show that this probability
corresponds to the expected value of Q.
With pi_k we indicate the joint probability of k defaults. In this case, we see that this probability
corresponds to the k-th moment of Q.
The probability of default of one counterparty is always the same.
Hence the unconditional default probabilities of first and higher order correspond to the moments of
the mixing distribution.
The computation of the covariance, thanks to the conditional independence hypothesis, tells us
that all the joint behavior is given by the mixing variable q. In fact, the covariance between Y_i and
Y_j is just the variance of Q. Which is by definition non-negative.
This tells us that, in an exchangeable Bernoulli mixture model, also the correlation is always nonnegative.
So, what are the economic implications of this?
Since correlation is always non-negative, this model supports the idea of contagion, that is to say
the idea that one default can really trigger an avalanche effect in our homogeneous group of
counterparties.
It is a model that takes into account, that actually fosters pro-cyclicality, that is to say the possibility
that during a bad period, during a crisis, there is this avalanche effect, and this self-reinforcing
behavior, in the number of defaults over time.
It is also a model that, despite its simplicity, and if you want its theoretical framework, is able to
generate very different cases, and we just simply need a few assumptions.
Consider the mixing distribution G(q). If we assume it to be a Bernoulli distribution, with 50%
probability on 0 and 50% probability on 1, then we can show that the correlation between Y_i and
Y_j, that is to say the indicators of two defaults in our portfolio, is 1. And 1 means perfect linear
dependence, positive dependence. Or, in more risk-theoretic words, perfect comonotonicity.
Let’s now assume that the mixing distribution is a Beta.
We know the density of a Beta. We saw it when dealing with recovery rates.
Since we know that the probability of default of k counterparties corresponds to the k-th moment of
Q, we just compute the k-th moment of a Beta. Here it is on your screen.
As for the number of defaults, we can use the mixed binomial, obtaining the famous Beta-binomial
distribution.
Let’s consider an example. A theoretical one for the moment.
Imagine we observe defaults over n periods of time for a homogeneous group of companies.
For every t, let m_t represent the number of observed companies at the beginning of period t, and
let M_t be the number of defaults in the same period.
If defaults follow an exchangeable Bernoulli mixture with Beta-mixing, we know that M_t is
conditionally binomial, and the number of defaults in any period is a Beta-binomial of parameters
(a+M_t) and (b+m_t-M_t).
Writing the likelihood is simple. And the estimation of the parameters can be easily performed with
MLE.
There are situations in which the Beta-mixing model is not at all a bad model, a model that, despite
its simplicity, you can actually use with some good results.
In my research I have developed more advanced versions of this construction, and in Week 5 I will
share with you some insights about the use of this model in practice.


Within this class we focus our attention on CreditRisk+ by Credit Suisse.

CreditRisk+ is often referred to as an actuarial model, given that it relies on some well-known results of actuarial sciences.

We will see that the model is able to generate useful closed-form results, which speed up computations tremendously. The model is also parsimonious in terms of variables and data input.

But, what is the price we have to pay?

Two are the main flaws, as we shall see later in the course:

We assume independent (or at least uncorrelated) common factors, something difficult to have in reality, if we do not filter the data;
The LGD of each counterparty is assumed to be known, deterministic and constant over time. Again, something that - we have already seen - is not really realistic.

In this class we continue speaking about mixture models, but now we move towards Poisson
mixtures.
From a conceptual point of view, the difference is very simple: instead of Bernoullis, we are now
dealing with Poisson random variables.
The practical implication of this change is that in principle a company may now default more than
once. How can we justify this?
If you think, there is a simple example. And it’s the case in which a company defaults, the
shareholders bring new capital, in order to save the company, but this new capital is not sufficient
to avoid a new default in a very short period.
With Y tilde, we indicate the variable counting the number of defaults for company i.
Given the random factor vector Psi, we assume that the number of defaults for each company is a
Poisson random variable with rate lambda_i(Psi).
We can then define a new variable M tilde that corresponds to the sum of all the Y’s tilde. It is
interesting to notice that, when the lambda_i’s, that is to say the Poisson rates, are small enough,
then M tilde is a very good approximation of the M we have seen before in Bernoulli mixtures.
In probabilistic terms, M tilde is the sum of conditionally independent Poisson variables. On screen
you can see what I mean.
If we define the random variables Z_i, indicating the events “at least one default for company i”,
then we are back to the old framework we have seen, and p_i is equal to 1-exp(lambda_i(Psi)).
CreditRisk+ was introduced by Credit Suisse Financial Products in 1997.
It is a Poisson mixture model, in which the common factors are p independently distributed Gamma
random variables.
And this assumption essentially allows for the derivation of closed-form analytical results, and, from
a computational point of view, it allows for the use of very strong tools coming from actuarial
mathematics, like the so-called Panjer recursion.
Now, I have to admit that, within the class of credit risk models, CreditRisk+ is actually one of my
favorite ones.
In the model, we assume the intensities lambda_i(Psi) to have a linear representation of the form
k_i w_i Psi, where k_i is a non-negative constant (an important one, we will see) and w_i is a
vector of convex weights.
We assume the vector Psi to contain independent Gamma-distributed factors, with parameters
alpha_j and beta_j, j from 1 to p, equal to the inverse of the factor variance sigma_j^2. We assume
this quantity to be observable from data.
The choice of Gammas is due to two different reasons. First, their flexibility in terms of shape, and,
second, the possibility of obtaining closed-form analytical results, as we will see in a minute.
Thanks to this parametrization, we have that the expected value of each factor psi_i is exactly
equal to 1, while the variance is the sigma_j^2 we were assuming to be observable.
Moreover, if we consider the expected value of every lambda_i, this expected value is exactly k_i.
And this is very interesting from a computational point of view, as we will see in Week 5.
What is interesting is that the quantity k_i can be essentially computed on the basis of historical
data.
The default probability of counterparty i corresponds to the probability of observing at least a
default for i.
This quantity can be easily and satisfactorily approximated, if we assume the values k_i to be small
enough.
Now, since we know that the Poisson distribution is closed under convolution, the conditional
distribution of M tilde is still a Poisson, whose intensity is the sum of the intensities of the
counterparties.
For the derivation of the unconditional distribution of the approximate number of defaults, we can
use the properties of Poisson-Gamma mixtures.
This is a popular result from actuarial sciences, which tells us that if a random variable N has a
Poisson distribution with intensity lambda, and this intensity is Gamma distributed with parameters
alpha and beta, then N is a negative binomial.
The proof is simple, and you find it here, if you are interested.
If we assume to have only 1 common factor, then the negative binomial result can be immediately
applied.
In the general case with p factors, M tilde will be a sum of p independent negative binomials.
This follows from the fact that we have assumed independent gamma distributed factors.
The computation of M tilde can be performed using a very interesting tool known as Panjer
recursion, which allows for an efficient iterative procedure.
For the moment it makes no sense to enter into the details, but we will be back in Week 5 with a
working example.
We will see that CreditRisk+ is fast and requires only a few input data, typically easily observable.

Threshold Models
Strengths

Intuitive construction, relying on a multivariate threshold mechanism.
Possibility of dealing with default dependence in a convenient way.
Possibility of modeling migration risk.

Weaknesses

Strongly dependent on the copula assumptions. 
Most industry models assume a Gaussian copula, more or less explicitly. This can lead to an underestimation of (joint) tail risk.
Risk of serious statistical problems if the models is not well designed. For example,  with a large number of common factors, there is a non-trivial risk of collinearity.
Computationally inefficient for large portfolios.
They typically require a non-trivial amount of input data (asset values, transition matrices, common factors, etc.).

Mixture Models
Strengths

Very flexible models, based on the intuitive idea of conditional independence.
Connection with threshold models.
Higher computational efficiency with respect to threshold models
Natural connection with GLM, GLMM and GEE.

Weaknesses

Some models are more academic divertissements than actually usable models.
Almost impossible analytical tractability, apart from the cases with one (max two) common factor(s).
No modeling of migration risk.

CReditrisk+
Strengths

Possibility of deriving the PD (and other related quantities like the loss distribution, if we are interested in capital requirements and tail measures like the VaR) analytically.
Very quick computation of all the desired quantities. Much much quicker than via Monte Carlo.
Relatively small amount of input required. Moreover these inputs, as the historical probabilities of default (), are typically available.

Weaknesses

Since the common factors are assumed to be independent, sometimes it is difficult to identify variables with an immediate economic meaning (most economic quantities are strictly interconnected). 
The common factors need to be somehow "filtered", for example using PCA and similar techniques. 
No modeling of migration risk.
Some unrealistic assumptions about recovery rates and LGD, which we will consider in more details later on in the working example.
For example LGD is assumed to be known, deterministic and constant over time, not to lose the analytical tractability. 

Alternative Approaches
The list of models for the estimation of the PD could continue.

For example we could deal with discriminant and factors models like Altman's Z-score, or regression models like the logit and the probit. Generalized linear models are then another natural step.
But this is a course for professionals, therefore I assume you already know the details of these approaches. Given your feedback via mail and on the course forum, these are probably the models you already use.

Similarly, I know you are already familiar with rating systems. That's why I was giving transition matrices for granted.
However, if you would like to see something more about rating quantification and validation, just let me know.

Regarding portfolio models we could cover PortfolioManager by KMV (before the acquisition by Moody's) or CreditPortfolioView by McKinsey & Company.

Among mixture models, we could consider EBMM with probit and logit factors.

Given the large number of emails I already received, starting from Week 4 I will introduce a mini-course (in the course) on survival analysis. We will deal with LGD, but also with PD. With a particular attention on some of the novelties introduced by IFRS 9.

Threshold Models
Multivariate extensions of the structural models of default. Each counterparty has its own
threshold, but the probabilities of default are not independent, given that they share some
common structure, usually modeled via copulas. The most used copula is the Gaussian one.
Mixture Models
Defaults are assumed to be independent, conditionally on a
set of common factors.
Conditional PDs can be Bernoulli or Poisson.
Mixture models are flexible and very convenient from an
estimation point of view.
Equivalence of Threshold Models
Two threshold models, whose vectors of critical values admit the same copula, and whose
marginal distributions on the space vectors are the same, are said equivalent.
Equivalence is very useful from a practical point of view, when we are interested in estimation
issues.
CreditMetrics (Portfolio version)
We consider a portfolio of exposures. For each single counterparty we take standardized assets as
the critical variable, and we define (standard normal quantile) thresholds as in the univariate case.
We then assume the different counterparties to be linearly dependent and, imposing a Gaussian
copula, we build a Multivariate Normal to model the joint behavior of the counterparties.
All the information about dependence is contained in the correlation matrix of the Gaussian copula.
This correlation matrix is calibrated by assuming a factor model for the standardized assets.
In particular, we assume that the standardized asset vector X is such that
 ,
where F is a vector of normally distributed factors, B is a matrix of weights and epsilon is a vector
of errors, uncorrelated with the factors.
On the course platform you can find a bivariate example with all computations.
Li’s Model
Li’s model is a survival model, in which is the random time of default of counterparty i.
Default is thus observed if
Li assumes an exponential distribution for the marginal default times and a Gaussian copula for
their joint behavior. In formulas:
with . This implies an interesting threshold model representation.
Model Issues
If we change the Gaussian copula assumption, the results we obtain for the joint probabilities of
default (and the number of defaults) may change substantially.
1 of 2 ACRM - P.Cirillo
Threshold Models
Multivariate extensions of the structural models of default. Each counterparty has its own
threshold, but the probabilities of default are not independent, given that they share some
common structure, usually modeled via copulas. The most used copula is the Gaussian one.
Mixture Models
Defaults are assumed to be independent, conditionally on a
set of common factors.
Conditional PDs can be Bernoulli or Poisson.
Mixture models are flexible and very convenient from an
estimation point of view.


Bernoulli Mixtures
In a Bernoulli mixture, the conditional probabilities of default of the different counterparties are
simple Bernoullis, given the common factor vector ?.
If Yi is the indicator of the default for the i-th counterparty, we assume
Making distributional assumptions on ?, we can obtain many different models.
An interesting class of Bernoulli models is the one we get by assuming that we only have one
common factor.
In this class we have considered Exchangeable Mixtures.
If we assume the mixing factor to be Beta distributed, the number of defaults follows a Betabinomial
distribution.
Poisson Mixtures
The setting is very similar to the one we have discussed under Bernoulli mixtures, however we now
assume conditional PDs to be Poisson.
This family of models allows the same counterparty to default more than once in a short time
horizon.
The most famous Poisson mixture model is CreditRisk+.
CreditRisk+
Introduced in 1997 by Credit Suisse Financial Products.
It is a Poisson mixture model where the common factor vector ? is assumed to contain
independent Gamma distributed random factors.
The choice of the flexible Gamma distribution is due to:
• The desire of obtaining closed form analytical results.
• The interest in computationally efficient algorithms for the number of defaults.
The parameters of each Gamma are chosen in a way that guarantees the expected value of each
factor to be equal to 1, and its variance to correspond to the variance we can compute from data.
Relying on well-known results of actuarial mathematics, CreditRisk+ shows that:
• In case of only one common factor, the number of defaults follows a negative binomial
distribution.
• In case of more factors, the number of defaults is the sum of independent negative binomials.
Using the so-called Panjer recursion, all the quantities involved in CR+ are easily obtainable using
iterative procedures.
A nice feature of CreditRisk+ is the fact that only a few input data are necessary, and they are
typically easily observable.









